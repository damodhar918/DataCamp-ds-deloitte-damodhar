## HQL (Hive Query Language) Terminology

Hive is a data warehouse infrastructure that provides data summarization, query, and analysis of large datasets in Hadoop. Here are some common HQL terminology that every Hive developer should know:

1. **Database**: A container for tables and views. Each database can contain multiple tables and views.

2. **Table**: A collection of rows and columns used for storing data. Each table is defined by its schema, with a set of columns and their data types.

3. **Partition**: A way of dividing a table into smaller, more manageable pieces based on some criteria, such as date or region.

4. **Bucket**: A way of organizing data within partitions to improve query performance. Buckets are created based on the values of one or more columns.

5. **View**: A virtual table that provides a customized view of data stored in one or more tables.

6. **Query**: A request for data from one or more tables or views. Queries are expressed in HQL (Hive Query Language) and are used to extract, transform, and analyze data.

7. **Query Execution Plan**: A blueprint that describes how a query will be executed by Hive. The plan is generated by the query optimizer and includes information about the order in which tables will be scanned, which indices will be used, and which join algorithms will be employed, etc.

8. **UDF (User-Defined Function)**: A custom-built function that extends HQL to perform complex data analysis or processing.

9. **SerDe (Serializer/Deserializer)**: A set of code for serializing and deserializing data. Hive uses SerDes to read data from and write data to tables.

10. **Join**: A SQL operation that combines rows from two or more tables based on a common column (or set of columns).

11. **UDAF (User-Defined Aggregation Function)**: A custom-built function that extends HQL to perform complex data aggregation or summarization.

12. **SerDe Properties**: Properties that control how data is serialized and deserialized by SerDes. They are set at the table or partition level.

13. **Dynamic Partitioning**: A feature that allows Hive to automatically create partitions based on the data being loaded.

14. **Bucketing Properties**: Properties that control how data is bucketed within partitions. They are set at the table or partition level.

15. **UDTF (User-Defined Table-Generating Function)**: A custom-built function that extends HQL to generate tables as output.

16. **ACID (Atomicity, Consistency, Isolation, Durability)**: A set of properties that ensure data integrity and consistency in Hive transactions.

17. **Tez**: A more efficient execution engine for Hive that uses directed acyclic graph (DAG) based processing.

18. **LLAP (Live Long And Process)**: A caching layer that provides faster query response times for Hive queries by caching frequently accessed data.

19. **Metastore Upgrade**: The process of upgrading the Hive metastore database to a new version of Hive.

20. **HiveServer2**: A server daemon that provides a JDBC/ODBC interface to Hive.
21. **Subquery**: A query nested inside another query. It can be used to filter data or perform calculations before joining tables.

22. **GroupBy**: A clause used to group the result set by one or more columns.

23. **OrderBy**: A clause used to sort the result set by one or more columns.

24. **Limit**: A clause used to limit the number of rows returned by a query.

25. **Union**: A SQL operation that combines the result sets of two or more SELECT statements.

26. **CTE (Common Table Expression)**: A temporary result set that is defined within the execution of a single SQL statement.

27. **Window Function**: A function that performs calculations across a set of rows that are related to the current row.

28. **Hadoop Distributed File System (HDFS)**: A distributed file system used to store large datasets across multiple nodes in a Hadoop cluster.

29. **MapReduce**: A distributed processing framework used to process large datasets in Hadoop. Hive queries are translated into MapReduce jobs for execution.

30. **Metastore**: The central repository for Hive metadata, such as table schemas, partitions, and views. It is implemented as a relational database.

Sure, here are even more HQL terms:

31. **HiveQL**: The query language used in Hive, which is based on SQL but includes extensions for Hadoop data processing.

32. **Hive Operator**: A command or function used in HiveQL to perform operations such as filtering, aggregation, or data transformation.

33. **InputFormat**: A class that defines how Hadoop reads data from a file or other source.

34. **OutputFormat**: A class that defines how Hadoop writes data to a file or other destination.

35. **SerDe (Serializer/Deserializer)**: A library used by Hive to convert data between its internal format and external formats such as CSV, JSON, or Avro.

36. **UDF (User-Defined Function)**: A custom-built function that extends HQL to perform complex data processing or transformation.

37. **Partition**: A logical division of data in Hive based on one or more columns. Partitions are used to improve query performance and manage large datasets.

38. **Table**: A named collection of data in Hive, similar to a table in a relational database.

39. **View**: A virtual table in Hive that is created by a query and does not store data. Views can be used to simplify complex queries or provide a layer of abstraction over data.

40. **Join**: A SQL operation that combines rows from two or more tables based on a common column.

### Examples

1. Using HiveQL:

   ```
   SELECT name, salary FROM employee WHERE department = 'Marketing';
   ```

2. Using Hive Operator:

   ```
   SELECT AVG(salary) FROM employee;
   ```

3. Using InputFormat:

   ```
   CREATE EXTERNAL TABLE employee (
     id INT,
     name STRING,
     address STRING,
     salary FLOAT
   ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
   STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' 
   LOCATION '/path/to/employee';
   ```

4. Using OutputFormat:

   ```
   INSERT OVERWRITE TABLE employee PARTITION (year=2021, month=8)
   SELECT id, name, address, salary FROM employee_raw;
   SET hive.exec.output.format=org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat;
   ```

5. Using SerDe:

   ```
   CREATE TABLE employee (
     id INT,
     name STRING,
     address STRING,
     salary FLOAT
   ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
   STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
   OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
   TBLPROPERTIES ('avro.schema.url'='/path/to/schema.avsc');
   ```

6. Using UDF:

   ```
   ADD JAR /path/to/myudf.jar;
   CREATE TEMPORARY FUNCTION my_udf AS 'com.example.udf.MyUdf';
   SELECT my_udf(name) FROM employee;
   ```

7. Using Partition:

   ```
   CREATE TABLE employee (
     id INT,
     name STRING,
     address STRING,
     salary FLOAT
   ) PARTITIONED BY (year INT, month INT);
   ALTER TABLE employee ADD PARTITION (year=2021, month=8) LOCATION '/path/to/employee/2021/8';
   ```

8. Using Table:

   ```
   CREATE TABLE employee (
     id INT,
     name STRING,
     address STRING,
     salary FLOAT
   ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
   STORED AS TEXTFILE;
   ```

9. Using View:

   ```
   CREATE VIEW marketing_employee AS
   SELECT name, salary FROM employee WHERE department = 'Marketing';
   SELECT * FROM marketing_employee;
   ```

10. Using Join:

    ```
    SELECT employee.name, department.manager
    FROM employee
    JOIN department ON employee.department_id = department.id;
    ```

1. Creating a table in Hive:

   ```
   CREATE TABLE employee (
     id INT,
     name STRING,
     address STRING,
     salary FLOAT
   ) ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ','
   LINES TERMINATED BY '\n'
   STORED AS TEXTFILE;
   ```

2. Loading data into a table:

   ```
   LOAD DATA LOCAL INPATH '/path/to/employee.csv' INTO TABLE employee;
   ```

3. Querying data from a table:

   ```
   SELECT name, salary FROM employee WHERE salary > 50000;
   ```

4. Creating a view in Hive:

   ```
   CREATE VIEW high_salary_employees AS
   SELECT name, address FROM employee WHERE salary > 50000;
   ```

5. Joining two tables in Hive:

   ```
   SELECT employee.name, department.department_name
   FROM employee JOIN department
   ON employee.department_id = department.id;
   ```

6. Using UDAF:

   ```
   CREATE TEMPORARY FUNCTION my_avg AS 'com.example.udaf.MyAvg';
   SELECT my_avg(salary) FROM employee;
   ```

7. Using SerDe Properties:

   ```
   CREATE TABLE employee (
     id INT,
     name STRING,
     address STRING,
     salary FLOAT
   ) PARTITIONED BY (year INT, month INT)
   ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
   WITH SERDEPROPERTIES ('avro.schema.url'='/path/to/schema.avsc');
   ```

8. Using Dynamic Partitioning:

   ```
   SET hive.exec.dynamic.partition=true;
   SET hive.exec.dynamic.partition.mode=nonstrict;
   INSERT INTO TABLE employee PARTITION (year, month) SELECT * FROM employee_raw;
   ```

9. Using Bucketing Properties:

   ```
   CREATE TABLE employee (
     id INT,
     name STRING,
     address STRING,
     salary FLOAT
   ) PARTITIONED BY (year INT, month INT)
   CLUSTERED BY (id) INTO 4 BUCKETS;
   ```

10. Using UDTF:

```
CREATE TEMPORARY FUNCTION my_udtf AS 'com.example.udtf.MyUdtf' USING JAR '/path/to/udtf.jar';
SELECT * FROM my_udtf(salary, department) AS (employee_name STRING, department_name STRING);
```

11. Using ACID:

```
SET hive.support.concurrency=true;
SET hive.enforce.bucketing=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT INTO TABLE employee PARTITION (year, month) VALUES (1, 'John', '123 Main St', 50000, 2021, 8)
SET acid=true;
```

12. Using Tez:

```
SET hive.execution.engine=tez;
SELECT name, salary FROM employee WHERE department = 'Marketing';
```

13. Using LLAP:

```
SET hive.llap.execution.mode=all;
SELECT name, salary FROM employee WHERE department = 'Marketing';
```

14. Metastore Upgrade:

```
schematool -dbType mysql -upgradeSchemaFrom 2.1.1 -upgradeSchemaTo 2.3.0
```

15. Using HiveServer2:

    ```
    beeline -u jdbc:hive2://localhost:10000/default -n admin -p admin
    SELECT name, salary FROM employee WHERE department = 'Marketing';
    ```

16. Using a subquery:

```
SELECT name FROM employee WHERE salary > (SELECT AVG(salary) FROM employee);
```

17. Using GroupBy:

```
SELECT department, AVG(salary) FROM employee GROUP BY department;
```

18. Using OrderBy:

```
SELECT name, salary FROM employee ORDER BY salary DESC;
```

19. Using Limit:

```
SELECT name, salary FROM employee LIMIT 10;
```

20. Using Union:

```
SELECT name FROM employee WHERE salary > 50000
UNION
SELECT name FROM employee WHERE department = 'Sales';
```

21. Using CTE:

```
WITH temp_table AS (
  SELECT name, salary FROM employee WHERE department = 'Marketing'
)
SELECT * FROM temp_table WHERE salary > 50000;
```

22. Using Window Function:

```
SELECT name, salary, AVG(salary) OVER (PARTITION BY department) AS avg_salary
FROM employee;
```

23. Using HDFS:

```
LOAD DATA INPATH '/path/to/employee.csv' INTO TABLE employee;
```

24. Using MapReduce:

```
SET hive.execution.engine=mr;
SELECT name, salary FROM employee WHERE department = 'Marketing';
```

25. Using Metastore:

    Hive metastore can be configured to use various databases like MySQL, Postgres, and Oracle to store metadata. Here is an example of using MySQL as a metastore:

    ```
    hive.metastore.uris=thrift://localhost:9083
    javax.jdo.option.ConnectionURL=jdbc:mysql://localhost/metastore
    javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver
    javax.jdo.option.ConnectionUserName=hiveuser
    javax.jdo.option.ConnectionPassword=hivepassword
    ```
### Create Table query 
in Hive that includes a range of possible and rare combinations:

```
CREATE TABLE my_table (
  id INT,
  name STRING,
  age INT,
  address STRUCT<street:STRING, city:STRING, state:STRING>,
  income ARRAY<FLOAT>,
  score MAP<STRING, INT>,
  created_date TIMESTAMP,
  is_active BOOLEAN,
  gender CHAR(1),
  email STRING COMMENT 'email address of the user',
  PRIMARY KEY (id)
)
PARTITIONED BY (year INT, month INT)
CLUSTERED BY (age) INTO 5 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '|'
MAP KEYS TERMINATED BY ':'
STORED AS ORC
TBLPROPERTIES ('creator'='hive', 'created_on'='2021-09-30');
```

This create table query includes the following:

- `id`: An integer column that serves as the primary key of the table.
- `name`: A string column that stores the name of a person.
- `age`: An integer column that stores the age of a person.
- `address`: A complex data type that includes a `street` string, `city` string, and `state` string.
- `income`: An array of floating-point numbers that stores a person's income.
- `score`: A map that associates a score with a string key.
- `created_date`: A timestamp column that stores the date and time when a record was created.
- `is_active`: A boolean column that indicates whether a person is currently active.
- `gender`: A character column that stores a single character to represent a person's gender.
- `email`: A string column that stores a person's email address. This column includes a comment to describe its purpose.
- `year`: A partitioning column that stores the year value.
- `month`: A partitioning column that stores the month value.
- `age`: A column used for bucketing to improve query performance.
- `ROW FORMAT DELIMITED`: Specifies that the data in this table is delimited.
- `FIELDS TERMINATED BY ','`: The delimiter used for columns is a comma.
- `COLLECTION ITEMS TERMINATED BY '|'`: The delimiter used for array elements is a pipe character.
- `MAP KEYS TERMINATED BY ':'`: The delimiter used for keys in the map is a colon.
- `STORED AS ORC`: Specifies that the table is stored in the ORC format.
- `TBLPROPERTIES`: Additional table properties, including the creator and creation date.

### `INSERT INTO` queries


 to insert data into a Hive table:

1. Insert a single row of data into the table:

```
INSERT INTO my_table (id, name, age, address, income, score, created_date, is_active, gender, email, year, month)
VALUES (1, 'John Smith', 35, named_struct('street', '123 Main St', 'city', 'Anytown', 'state', 'CA'), array(50000.00, 60000.00), map('English', 80, 'Math', 90, 'Science', 85), '2021-09-30 10:00:00', true, 'M', 'john.smith@example.com', 2021, 9);
```

2. Insert multiple rows of data into the table:

```
INSERT INTO my_table (id, name, age, address, income, score, created_date, is_active, gender, email, year, month)
VALUES (2, 'Jane Doe', 28, named_struct('street', '456 Elm St', 'city', 'Otherville', 'state', 'NY'), array(70000.00, 80000.00), map('English', 90, 'Math', 95, 'Science', 92), '2021-09-30 11:00:00', true, 'F', 'jane.doe@example.com', 2021, 9),
       (3, 'Bob Johnson', 42, named_struct('street', '789 Oak St', 'city', 'Somecity', 'state', 'TX'), array(40000.00, 45000.00), map('English', 70, 'Math', 75, 'Science', 80), '2021-09-30 12:00:00', false, 'M', 'bob.johnson@example.com', 2021, 9),
       (4, 'Sarah Lee', 25, named_struct('street', '321 Pine St', 'city', 'Othercity', 'state', 'CA'), array(55000.00, 60000.00), map('English', 85, 'Math', 90, 'Science', 88), '2021-09-30 13:00:00', true, 'F', 'sarah.lee@example.com', 2021, 9);
```

3. Insert data into partitioned table:

```
INSERT INTO my_table (id, name, age, address, income, score, created_date, is_active, gender, email)
VALUES (5, 'Tom Smith', 30, named_struct('street', '456 Elm St', 'city', 'Otherville', 'state', 'NY'), array(78000.00, 85000.00), map('English', 90, 'Math', 95, 'Science', 92), '2021-09-30 14:00:00', true, 'M', 'tom.smith@example.com')
PARTITION (year=2021, month=10);
```

4. Insert data from a select query:

```
INSERT INTO my_table (id, name, age, address, income, score, created_date, is_active, gender, email, year, month)
SELECT id, name, age, address, income, score, created_date, is_active, gender, email, year, month
FROM other_table
WHERE year = 2021 AND month = 9;
```


5. `INSERT INTO` query in Hive that demonstrates how to insert data into a table using a query:

```
INSERT INTO my_table (id, name, age, address, income, score, created_date, is_active, gender, email, year, month)
SELECT 
  id, 
  name, 
  age, 
  named_struct('street', street, 'city', city, 'state', state) AS address, 
  array(income) AS income, 
  map('English', english_score, 'Math', math_score, 'Science', science_score) AS score, 
  created_date, 
  CASE WHEN is_active = 'Y' THEN true ELSE false END AS is_active, 
  gender, 
  email, 
  year, 
  month
FROM raw_table;
```

In this example, we assume that we have an existing table called `raw_table` with columns `id`, `name`, `age`, `street`, `city`, `state`, `income`, `english_score`, `math_score`, `science_score`, `created_date`, `is_active`, `gender`, `email`, `year`, and `month`. We want to insert this data into a new table called `my_table`, which has a slightly different schema.

The `SELECT` statement in this query fetches data from the `raw_table` and maps it to the columns in `my_table`. We use functions such as `named_struct`, `array`, and `map` to transform the data as needed.

Note that we also use a `CASE` statement to convert the `is_active` column from a string value ('Y' or 'N') to a boolean value (true or false) as expected by the `my_table` schema.

Once we have the data transformed and mapped correctly, we use the `INSERT INTO` statement to insert it into `my_table`.

This is just one example of how to use an `INSERT INTO` query with a `SELECT` statement in Hive. The specifics of the query and the transformation functions used will depend on your specific use case.

### Transformation functions that can be used in an `INSERT INTO` query in Hive:

1. `CONCAT` function to concatenate strings:

```
INSERT INTO my_table (id, full_name, address)
SELECT 
  id, 
  CONCAT(first_name, ' ', last_name) AS full_name, 
  CONCAT(street, ', ', city, ', ', state) AS address
FROM raw_table;
```

2. `CAST` function to convert data types:

```
INSERT INTO my_table (id, name, age, income)
SELECT 
  id, 
  name, 
  CAST(age AS INT) AS age, 
  CAST(income AS ARRAY<DOUBLE>) AS income
FROM raw_table;
```

3. `REGEXP_REPLACE` function to replace patterns in strings:

```
INSERT INTO my_table (id, name, phone_number)
SELECT 
  id, 
  name, 
  REGEXP_REPLACE(phone_number, '\\D', '') AS phone_number
FROM raw_table;
```

4. `CASE` statement to conditionally transform data:

```
INSERT INTO my_table (id, name, age_category)
SELECT 
  id, 
  name, 
  CASE 
    WHEN age < 18 THEN 'Under 18' 
    WHEN age BETWEEN 18 AND 65 THEN '18-65' 
    ELSE 'Over 65' 
  END AS age_category
FROM raw_table;
```

5. `COALESCE` function to replace null values:

```
INSERT INTO my_table (id, name, address, phone_number)
SELECT 
  id, 
  name, 
  COALESCE(address, named_struct('street', '', 'city', '', 'state', '')) AS address, 
  COALESCE(phone_number, '') AS phone_number
FROM raw_table;
```

These are just a few examples of transformation functions that can be used in an `INSERT INTO` query in Hive. The specific functions and syntax used will depend on your specific use case.