# Q&A questions / answers
## From session on 2020-06-17

- So functions from F are applied to each value in the column individually, right? Or is it the case only in comdbination with `withColumn()`?
  - Functions from F (ie, `from pyspark.sql import functions as F`). These functions can be used in various places depending on exactly what you're trying to do, including the `df.withColumn()` method.

- pyspark has a different syntax than normal python code. I'm I right
  - PySpark is a Python API and uses Python syntax. You may be thrown off with the multiple chaining though, or the one letter function (F). In a nutshell it looks a bit different but it uses Python syntax.

- so with pyspark everything is chain statements? can't you have python states such as loops, etc?
  - Pyspark is really a Python API on top of the Scala / Java underpinnings of Spark. Dataframes are most performant when you use the built in API calls to avoid serialization performance penalties. That said, you can use loops and such in your code, especially in UDFs (User defined functions), but it is not always recommended. 

- can you explain again why the tab seperator didn't work and why the curly brackt?
  - Because { is not used anywhere in the dataset content, so there's no risk to accidentally separate a cell in two.

- So sorry... is "/tmp/netblex_titles_cleaned.csv" a directory here? (with one piece of data from "partition" under it "part-00000-5f404d8b-...."?)
  - Yes, exactly. The Spark dataframewriter class writes data out in this fashion, and typically you will have between a few and several thousand files present depending on the size of the dataframe.

- is Spark just a python library? or can it be used outside of Python?
  - Spark is a unified analytics engine for big data processing built in Scala, working on top of Java. PySpark is the Python API that allows you to interact iwth Spark in Python. You can also use Spark directly with Scala and Java or via the R and SQL libraries.

- what's the advantage of using pyspark over pandas?
  - Primarily that you can handle significantly larger datasets on a Spark cluster than you could on any given machine running Pandas. Beyond that, it is often easier to implement Spark jobs into a production workflow rather than pandas, especially if you can take advantage of the Spark machine learning and other analysis tools. SparkSQL is also a bit more mature than using SQL style syntax with pandas. That said, both are extremely useful tools and have considerable overlap conceptually. I'd recommend becoming familiar with both and using whatever makes sense for your use case.

- can't we use the pandas write csv command to create a single csv file automatically?
  - You could, using the `dataframe.toPandas()` method. You must make sure that the driver, or primary, node of the Spark cluster has enough memory to handle the combination of all the data at a single location. 

- Sorry, i know that u explained this, but whats the main different or benefits of using DF versus RDD?
  - An RDD is a Resilient Distributed Dataset, and is the underlying data structure for most of Spark. Dataframes are technically built on top of RDDs, but provide added functionality and capability. Especially when using Pyspark, dataframes are recommended for performance reasons, but also for functionality such as SparkSQL, and so forth. If you ever came across a specific need for an RDD you could always use the `dataframe.rdd` call. For example, it's possible to implement a similar cleaning methodology as this course using RDDs - there are performance implications but depending on your background it might reduce the cognitive load when writing the code.

- Does Spark use  index values for each record like in Pandas?
  - Not as such. Spark treats rows or records in a dataframe as a defined group of columns. The columns are then stored / accessed according to their underlying data structure at the time (ie, CSV source, Parquet source files, etc). You can use something like `.filter/.where` to specifically limit the rows you're accessing. You could also use the `pyspark.sql.functions.rank` function to assign a value and filter from there. Beyond that, you can use the `pyspark.sql.functions.monotonically_increasing_id()` method to generate an ID column and access rows via said ID (again, with the `.filter/.where` methods on the dataframe. Note that there are some gotchas to using this function - please refer to the documentation or the full Cleaning Data with Pyspark course for more information.

- Beside loops, what else is not advisable to use in spark?
  - Really anything that acts like a Singleton object, or anything that you couldn't split in some fashion. Spark works best when each partition of data can be processed independently. There are some ways around things like this (ie, using the `broadcast()` method) but usually you'd want to have a splittable job to get the most out of Spark. *NOTE* You may still want to test the functionality when using things like loops / etc. It might not be the fastest method *in* Spark, but it still might be worth the time to implement it overall.

- When you are creating new dataframes from existing ones, is this occupying more space in memory?
  - It is, but it is a very small amount of memory, especially before the dataframe is instantiated with a Spark action (ie, `.count()`, `.write()`, etc). Remember that transformations in Spark are *lazy*, meaning they don't take effect until data is required to provide an answer. A dataframe itself is more like a recipe of steps rather than the actual data. We use actions to instantiate the dataframe. If Spark runs out of working memory, it will try to flush out unused information and reprocess the partition.

- Could we get a statistics about rows with different columns number in DataFrame without using every time `count()` method?
  - There are certainly other methods you can use to gain information about a dataframe. The `.describe()` method will provide assorted information about the data that may be useful. 

- What does the '`truncate=False`' parameter indicates inside the '`.show()`' method?
  - It means the column is shown completely, so if the content of the cell is really long, you will see it all, instead of it being truncated with three dots: https://stackoverflow.com/questions/33742895/how-to-show-full-column-content-in-a-spark-dataframe

- Can CSV reader read the CSV file that is compressed (gz) but does not end with .gz?
  - The CSV parser can handle compressed files, including bzip2, gzip, lz4, snappy and deflate *if* they are labelled as such. Note that gzip files are unsplittable, so having multiple files is recommended vs one large file. If you need one large file, try something like snappy instead. In regard to the specific question, I had not tried it before, but it does not look like the parser understands how to handle a compressed file that is not labelled specifically as such (ie, .bz2, .gz, etc). There unfortunately is not an argument available (as of Spark 2.4.6 / 3.0.0b2) that allows you to specify the type of compression used in files.

- You used single quote' and double quote " in the codes.  Is there any difference between the two types?
  - No specific difference beyond if you need to mix the two (ie, an inline filter clause `'name == "John"'`) or if you need an interpreted character (use `"` in that case).

- What are the pros and cons of PySpark, PyArrow, and FastParquet?
  - These aren't specifically analogous to one another, but PySpark is capable of reading / writing parquet data and is a recommended format for high performance. PyArrow is a library to parse parquet style data in any application that implements the library requirements. Note that it is not necessarily distributed, so you may need to adjust the sizing capabilities of your system vs the amount of data. I have not specifically used FastParquet but it is a native Python implementation to parse parquet data. Both PyArrow and FastParquet appear to be under active development so I'd use whatever works for your use case assuming your data fits on a given system.

- How does `F.col()` knows which dataframe to use column from?
  - It assumes that the column is a portion of the dataframe you've defined it with (ie, `df.withColumn('test', F.col('orig'))` would look for `df.orig` as the column to access. If you're performing a join, F.col will try to determine from the available columns, otherwise it will throw an error regarding the selection being ambigious. In that case, you can specify the column via one of the other available options (`df['colname']`, `df.colname`, etc).

- what the difference between 'filter' and 'where' functions?
  - There is no difference here, they are alias of each other. You can use whichever one makes more sense to you (ie, if you come from a SQL background, `.where` might make more sense while reading it.

- As long as withColumn creates new DataFrame does this mean that I need a lot more RAM than size of initial DataFrame to effectively wrangling data?
  - Refer to question previously, but generally no, you don't need a lot of extra RAM for dataframe storage as Spark intelligently manages it. The area with the greatest RAM utilization would be joining a lot of large dataframes together as they must be at least partially instantiated.

- `.cast(IntegerType())` and `.cast('Integer')` are the same?
  - Yes, either style can be used.

- What happened if you don't run `.coalesce(1)` before writing csv file? It will write several files?
  - Correct, it will write as many files as there are internal partitions of the dataframe. 

- What if the data itself contains the curly brace "{"?
  - Then you risk separating a cell in two and treating the separated content as two separate columns, so you would need to find another character to separate with.

- Sorry, maybe basic question, this is the first time for me learning spark. Thanks.
  - If you're looking for more information on Spark, please try the Intro to PySpark course available on DataCamp for more information. Once you've taken that, the content here and in the Cleaning Data with PySpark course will make more sense.

- What is difference between PySpark and Pandas?
  - Spark is a framework built to work with Big Data, using parallel processing among other things. It's built in Java. PySpark is the Python tool to interact with Spark. pandas is a Python library used for data manipulation. Both can do similar operations but Spark is better suited for Big Data.

- Spark is memory based(data is loaded into memory). So how does it work when the data size is much bigger than the memory can handle? during that time, is it better to use a different engine e.g. mapreduce for efficiency?
  - Spark partitions data into multiple chunks and then operates on the chunks, typically reducing the amount of memory required to process a dataset. More memory typically means faster performance as more information can be processed at once. There is a lower limit depending on the amount of data you have and the type of operations you're running but typically you wouldn't hit these on normally spec'd clusters. Regarding the use of mapreduce, Spark does keep data in memory vs Hadoop mapreduce but it's not required to fit it all at once. If data is needed later it is either reloaded from source / reprocessed or loaded from cache. Another option is to write intermediary data out before beginning a large processing step as this often helps the optimizer access needed data without reprocessing everything. The final note though, is to test your workloads - in almost all circumstances I've used in production, Spark is faster than classic mapreduce, especially for any SQL style jobs.

- So for highly scalable data its recommended to use pyspark? 
  - Definitely.

- can pyspark be used for unstructured data- i.e documents? csv files already have a structure
  - You can, but you may need to preprocess the data, or perform steps similar to what we are here. If you know the formatting of say a binary file or via a set of regex's, you could perform a similar split / clean operation. It depends entirely on what you're trying to accomplish.

- `titles_single_new_df = titles_single_df.filter(F.col('_c0').startsWith('#'))` gives me an error : 'Column' object is not callable
  - The method you want to use is `.startswith('#')` instead of `.startsWith('#')`. Note the lowercase vs uppercase `W`. 

- Do you have any tips on how to install Spark on my local machine in PyCharm for example? I've been finding it very difficult to install it locally.
  - You can try following this tutorial: https://medium.com/@gongster/how-to-use-pyspark-in-pycharm-ide-2fd8997b1cdd. Also note that installing Spark locally can be troublesome. I'd highly advise using something like Anaconda, a Docker container or one of the services available (Databricks, Amazon EMR, etc) to run Spark for you elsewhere.

- In this code we've written fieldcount as lower case and camel case - `titles_single_df.select('fieldcount', '_c0').where('fieldCount > 12').show(truncate=False)`. Why is that? Are these different columns or does it not matter to Pyspark?
  - Great observation! In this circumstance, it does not matter, but I would recommend keeping it all the same in your code. I will fix the discrepency in the future.

- printSchema is a Spark command, correct?
  - Yes, it is a method on the Spark dataframe class.

- ~ = not, correct?
  - Yes.

- when filter & select is being used -- is this part of SparkSQL?
  - Yes, but note that there is some terminology clarification here. The PySpark dataframe library is part of pyspark.sql. SparkSQL itself is the ability to run SQL commands directly within Spark (ie, entirely without another language). PySpark and Scala can also run SQL commands internally, but also use the extra functionality in Python / Scala as desired.

- what are positives/negatives of caching?
  - Typically, caching is almost always good. The only case where I'd say it's not is if you don't plan to do anything with the dataframe shortly after caching the data, as you'll have spent the time for the data to be stored (often on disk) with little to no gain. If you need to cache a lot of objects, I'd recommend writing at least some of your result sets out to parquet files and reload a dataframe from there. This provides more options for Spark to optimize your later processing steps.

- When we specify column, do we all need to add `F.col()`? or only `col()` sufficient?
  - Assuming you plan to use the `col()` function at all (ie, you can use the `df['name']` or `df.name` option as well) the answer depends on how you load the `pyspark.sql.functions` library. In this case, we've aliased it to `F`, so you'd reference it as `F.col()`. If you instead do something like `from pyspark.sql.functions import *`, you can use it without the `F.`. It depends on your style / requirements.

- (additional question) Some engineer said to me that production running would be preferred with Scala or Java, but with latest Spark, is there so much difference using PySpark? (could I use it in production system)?
  - In certain operations there is a speedup using Scala, especially if you need to use UDFs. That said, Myself and many others run tasks in PySpark in production with no issue. You can also use a mix of the two, putting performance sensitive code in Scala and referencing it from Python as needed. 

- What would be the advantage/disadvantage for PySpark v Scala/Java?
  - It depends on your familiarity, but Python is often easier to write (though Scala really isn't bad at all. Java depends on your experience with static languages). You can also use any of the python libraries as needed (with some potential performance implications vs a Scala implementation.) Often, people will use both, or start with Python and convert to Scala as it makes sense. 

- you said there are many ways to find rows starting with '#' one of which we just used. Can you briefly touch upon the other ones?
  - The primary method for dealing with comment rows is using the `comment=` attribute on the `spark.read.csv()` method to specify which comment to handle. Assuming you have one type of comment row, and it starts with a single character, this works well. The method we worked with handles any comments that start with a character or multiple characters / words / etc, assuming they start at the beginning of the line. If I also wanted to remove comments later in the row (ie, a code comment or some such) I could also use the Spark regex functions. You could also do the same behavior in a SQL statement, though it gets a bit hairy.

- What is an ArrayType column?
  - It is an aggregate Spark column type, that is analogous to a Python list, stored in a column. These can work well if you have a variable amount of data that you wish to store within a dataframe under a single column header.

- whats is the difference between Spark and  Dask library python?
  - Spark supports Python, R and Java Virtual Machine code. Dask only supports Python. Both are clustered options and have varying levels of flexibility depending on your needs. 

- where are the curly braces as separators?
  - They are used to separate upon, to create the columns, so they will not be rendered in the dataset (like columns or tabs won't render)

- Could we try this in a script in visual code or anywhere else?
  - Yes. You could use a notebook to test your pipeline, and then turn it into an executable scripit using any IDE. Note that you will need to define your Spark session object based on your cluster setup (refer to docs / ask your cluster admin for more details)

- have I to use spark before tensorflow to process the big data?
  - Spark or any other tool. Tensorflow is not a data manipulation tool, it's used for machine learning operations. Just like you would use pandas to clean data and scikit learn for predictions. You can use Tensorflow and Keras within Spark if desired.

- What is the best process, use a df 10*10 or a df 1*100? in memory sense
I have to process 1 billions rows but I could transpose some part and get 10millions rows by 100 columns, so I wonder what is the best method, keep 1billion rows and 1 column or 10millions rows and 100 columns?
  - Best answer is to test a portion of both and see how it behaves. Typically more rows mean that you can partition your data a bit better, but this varies a bit based on use case. That said, a 1 column dataframe is not always that useful (beyond working through the example here) so store the data in the method that makes the most sense overall.

- with this code
```
# Run this code as is to install Spark in Colab
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
!tar xf spark-2.4.5-bin-hadoop2.7.tgz
!pip install -q findspark
```
  - These are specific commands to install dependencies in Google collab. You need to install these dependencies on your system using the appropriate commands for it (WIndows, MacOS or Linux): https://openjdk.java.net/install/ to start. Otherwise, you can use an online Spark provider to test with (Databricks, AWS, etc)

- Where is the best location to store our csv file, in C:/ or in another hard drive? 
  - There is no better location. The best location is where you will easily be able to retrieve it. Then, if you plan to do additional analysis, you can move it to the folder in which you will create your scripts. If you plan to use Spark on a cluster to access any data, it needs to be accessible to the Spark nodes in something like NFS, HDFS, S3, or other network accessible storage.

- In a local environment, Do we need to work in a tempory folder (I see '/temp')? I am sorry but I don't understand if the csv file is stored in our hard drive ou in our ram when we open it trough a sparksession
  - The CSV file is an actual file. What gets written in memory is the different manipulations that you do on dataframes as you manipulate the data. When you downlowd the CSV, you will have an actual file, with lines of text separated by tabs. /temp is a folder on the Google Collab environment to store stuffs that you odn't want to see persist over sessions.
