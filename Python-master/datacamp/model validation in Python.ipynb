{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Introduction to model validation**\n",
    "___\n",
    "- What is model validation?\n",
    "    - Model validation consists of:\n",
    "        - ensuring your model performs as expected on new data\n",
    "        - testing model performance on holdout datasets\n",
    "        - selecting the best model, parameters, and accuracy metrics\n",
    "        - achieving the best accuracy for the data given\n",
    "- scikit-learn modeling review\n",
    "    - Basic modeling steps\n",
    "        - model = RandomForestRegressor(n_estimators=500, random_state=1111_\n",
    "        - model.fit(X=X_train, y=y_train)\n",
    "        - predictions = model.predict(X_test)\n",
    "        - print(\"{0:.2f}\".format(mae(y_true=y_test, y_pred=predictions)))\n",
    "            - e.g., \"10.84\"\n",
    "            - Mean Absolute Error\n",
    "                - (sum |y true - y pred|) / n\n",
    "- this course uses 538's ultimate Holloween Candy Power ranking dataset\n",
    "- Seen vs. unseen data\n",
    "    - training data = seen data\n",
    "    - testing data = unseen data\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Seen vs. unseen data\n",
    "\n",
    "#Model's tend to have higher accuracy on observations they have seen\n",
    "#before. In the candy dataset, predicting the popularity of Skittles\n",
    "#will likely have higher accuracy than predicting the popularity of\n",
    "#Andes Mints; Skittles is in the dataset, and Andes Mints is not.\n",
    "\n",
    "#You've built a model based on 50 candies using the dataset X_train\n",
    "#and need to report how accurate the model is at predicting the\n",
    "#popularity of the 50 candies the model was built on, and the 35\n",
    "#candies (X_test) it has never seen. You will use the mean absolute\n",
    "#error, mae(), as the accuracy metric.\n",
    "\n",
    "# The model is fit using X_train and y_train\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Create vectors of predictions\n",
    "#train_predictions = model.predict(X_train)\n",
    "#test_predictions = model.predict(X_test)\n",
    "\n",
    "# Train/Test Errors\n",
    "#train_error = mae(y_true=y_train, y_pred=train_predictions)\n",
    "#test_error = mae(y_true=y_test, y_pred=test_predictions)\n",
    "\n",
    "# Print the accuracy for seen and unseen data\n",
    "#print(\"Model error on seen data: {0:.2f}.\".format(train_error))\n",
    "#print(\"Model error on unseen data: {0:.2f}.\".format(test_error))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Model error on seen data: 3.28.\n",
    "#    Model error on unseen data: 11.07.\n",
    "#################################################\n",
    "#When models perform differently on training and testing data, you\n",
    "#should look to model validation to ensure you have the best performing\n",
    "#model. In the next lesson, you will start building models to validate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Regression models**\n",
    "___\n",
    "- Random forests in scikit-learn\n",
    "    - decision trees\n",
    "    - mean prediction of decision trees = final value for observation\n",
    "    - parameters\n",
    "        - n_estimators: the number of trees in the forest\n",
    "        - max_depth: tha maximum depth of the trees\n",
    "        - random_state: random seed for reproducibility\n",
    "    - feature importance\n",
    "        - .feature_ importances_\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Set parameters and fit a model\n",
    "\n",
    "#Predictive tasks fall into one of two categories: regression or\n",
    "#classification. In the candy dataset, the outcome is a continuous\n",
    "#variable describing how often the candy was chosen over another\n",
    "#candy in a series of 1-on-1 match-ups. To predict this value (the\n",
    "#win-percentage), you will use a regression model.\n",
    "\n",
    "#In this exercise, you will specify a few parameters using a random\n",
    "#forest regression model rfr.\n",
    "\n",
    "# Set the number of trees\n",
    "#rfr.n_estimators = 100\n",
    "\n",
    "# Add a maximum depth\n",
    "#rfr.max_depth = 6\n",
    "\n",
    "# Set the random state\n",
    "#rfr.random_state = 1111\n",
    "\n",
    "# Fit the model\n",
    "#rfr.fit(X_train, y_train)\n",
    "\n",
    "#################################################\n",
    "#You have updated parameters after the model was initialized. This\n",
    "#approach is helpful when you need to update parameters. Before\n",
    "#making predictions, let's see which candy characteristics were most\n",
    "#important to the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Feature importances\n",
    "\n",
    "#Although some candy attributes, such as chocolate, may be extremely\n",
    "#popular, it doesn't mean they will be important to model prediction.\n",
    "#After a random forest model has been fit, you can review the model's\n",
    "#attribute, .feature_importances_, to see which variables had the\n",
    "#biggest impact. You can check how important each variable was in the\n",
    "#model by looping over the feature importance array using enumerate().\n",
    "\n",
    "#If you are unfamiliar with Python's enumerate() function, it can loop\n",
    "#over a list while also creating an automatic counter.\n",
    "\n",
    "# Fit the model using X and y\n",
    "#rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print how important each column is to the model\n",
    "#for i, item in enumerate(rfr.feature_importances_):\n",
    "    # Use i and item to print out the feature importance of each column\n",
    "#    print(\"{0:s}: {1:.2f}\".format(X_train.columns[i], item))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    chocolate: 0.44\n",
    "#    fruity: 0.03\n",
    "#    caramel: 0.02\n",
    "#    peanutyalmondy: 0.05\n",
    "#    nougat: 0.01\n",
    "#    crispedricewafer: 0.03\n",
    "#    hard: 0.01\n",
    "#    bar: 0.02\n",
    "#    pluribus: 0.02\n",
    "#    sugarpercent: 0.17\n",
    "#    pricepercent: 0.19\n",
    "#################################################\n",
    "#No surprise here - chocolate is the most important variable.\n",
    "#.feature_importances_ is a great way to see which variables were\n",
    "#important to your random forest model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Classification models**\n",
    "___\n",
    "- Categorical Responses\n",
    "- Tic-Tac-Toe dataset\n",
    "- .predict()\n",
    "    - sparse array\n",
    "- .predict_proba()\n",
    "- .get_params()\n",
    "- .score(X_test, y_test)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Classification predictions\n",
    "\n",
    "#In model validation, it is often important to know more about the\n",
    "#predictions than just the final classification. When predicting\n",
    "#who will win a game, most people are also interested in how likely\n",
    "#it is a team will win.\n",
    "\n",
    "#Probability\tPrediction\tMeaning\n",
    "#0 < .50\t        0\t    Team Loses\n",
    "#.50 +\t            1\t    Team Wins\n",
    "\n",
    "#In this exercise, you look at the methods, .predict() and\n",
    "#.predict_proba() using the tic_tac_toe dataset. The first method\n",
    "#will give a prediction of whether Player One will win the game, and\n",
    "#the second method will provide the probability of Player One winning.\n",
    "#Use rfc as the random forest classification model.\n",
    "\n",
    "# Fit the rfc model.\n",
    "#rfc.fit(X_train, y_train)\n",
    "\n",
    "# Create arrays of predictions\n",
    "#classification_predictions = rfc.predict(X_test)\n",
    "#probability_predictions = rfc.predict_proba(X_test)\n",
    "\n",
    "# Print out count of binary predictions\n",
    "#print(pd.Series(classification_predictions).value_counts())\n",
    "\n",
    "# Print the first value from probability_predictions\n",
    "#print('The first predicted probabilities are: {}'.format(probability_predictions[0]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    1    563\n",
    "#    0    204\n",
    "#    dtype: int64\n",
    "#    The first predicted probabilities are: [0.26524423 0.73475577]\n",
    "#################################################\n",
    "#ou can see there were 563 observations where Player One was\n",
    "#predicted to win the Tic-Tac-Toe game. Also, note that the\n",
    "#predicted_probabilities array contains lists with only two values\n",
    "#because you only have two possible responses (win or lose). Remember\n",
    "#these two methods, as you will use them a lot throughout this course."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Reusing model parameters\n",
    "#Replicating model performance is vital in model validation. Replication\n",
    "#is also important when sharing models with co-workers, reusing models\n",
    "#on new data or asking questions on a website such as Stack Overflow.\n",
    "#You might use such a site to ask other coders about model errors,\n",
    "#output, or performance. The best way to do this is to replicate your\n",
    "#work by reusing model parameters.\n",
    "\n",
    "#In this exercise, you use various methods to recall which parameters\n",
    "#were used in a model.\n",
    "\n",
    "#rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)\n",
    "\n",
    "# Print the classification model\n",
    "#print(rfc)\n",
    "\n",
    "# Print the classification model's random state parameter\n",
    "#print('The random state is: {}'.format(rfc.random_state))\n",
    "\n",
    "# Print all parameters\n",
    "#print('Printing the parameters dictionary: {}'.format(rfc.get_params()))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#                max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
    "#                min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                min_samples_leaf=1, min_samples_split=2,\n",
    "#                min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
    "#                oob_score=False, random_state=1111, verbose=0,\n",
    "#                warm_start=False)\n",
    "#    The random state is: 1111\n",
    "#    Printing the parameters dictionary: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 1111, 'verbose': 0, 'warm_start': False}\n",
    "#################################################\n",
    "#Recalling which parameters were used will be helpful going forward.\n",
    "#Model validation and performance rely heavily on which parameters\n",
    "#were used, and there is no way to replicate a model without keeping\n",
    "#track of the parameters used!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Random forest classifier\n",
    "\n",
    "#This exercise reviews the four modeling steps discussed throughout\n",
    "#this chapter using a random forest classification model. You will:\n",
    "\n",
    "#Create a random forest classification model.\n",
    "#Fit the model using the tic_tac_toe dataset.\n",
    "#Make predictions on whether Player One will win (1) or lose (0) the current game.\n",
    "#Finally, you will evaluate the overall accuracy of the model.\n",
    "\n",
    "#Let's get started!\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "#rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)\n",
    "\n",
    "# Fit rfc using X_train and y_train\n",
    "#rfc.fit(X_train, y_train)\n",
    "\n",
    "# Create predictions on X_test\n",
    "#predictions = rfc.predict(X_test)\n",
    "#print(predictions[0:5])\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [1 1 1 1 1]\n",
    "#################################################\n",
    "\n",
    "# Print model accuracy using score() and the testing data\n",
    "#print(rfc.score(X_test, y_test))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.817470664928292\n",
    "#################################################\n",
    "#Notice the first five predictions were all 1, indicating that\n",
    "#Player One is predicted to win all five of those games. You also\n",
    "#see the model accuracy was only 82%.\n",
    "\n",
    "#Let's move on to Chapter 2 and increase our model validation toolbox\n",
    "#by learning about splitting datasets, standard accuracy metrics, and\n",
    "#the bias-variance tradeoff."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating train, test, and validation datasets**\n",
    "___\n",
    "- Ratio Examples\n",
    "    - 80:20\n",
    "    - 90:10\n",
    "        - used when we have little data\n",
    "    - 70:30\n",
    "        - used when model is computationally expensive\n",
    "- test_size\n",
    "- train_size\n",
    "- random_state\n",
    "- validation samples are used when testing different parameters\n",
    "    - it is a holdout sample taken from the training sample\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create one holdout set\n",
    "#Your boss has asked you to create a simple random forest model on the\n",
    "#tic_tac_toe dataset. She doesn't want you to spend much time selecting\n",
    "#parameters; rather she wants to know how well the model will perform\n",
    "#on future data. For future Tic-Tac-Toe games, it would be nice to know\n",
    "#if your model can predict which player will win.\n",
    "\n",
    "#The dataset tic_tac_toe has been loaded for your use.\n",
    "\n",
    "#Note that in Python, =\\ indicates the code was too long for one line\n",
    "#and has been split across two lines.\n",
    "\n",
    "# Create dummy variables using pandas\n",
    "#X = pd.get_dummies(tic_tac_toe.iloc[:, 0:9])\n",
    "#y = tic_tac_toe.iloc[:, 9]\n",
    "\n",
    "# Create training and testing datasets. Use 10% for the test set\n",
    "#X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.10, random_state=1111)\n",
    "\n",
    "#################################################\n",
    "#Remember, without the holdout set, you cannot truly validate a model.\n",
    "#Let's move on to creating two holdout sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create two holdout sets\n",
    "\n",
    "#You recently created a simple random forest model to predict Tic-Tac-Toe\n",
    "#game wins for your boss, and at her request, you did not do any\n",
    "#parameter tuning. Unfortunately, the overall model accuracy was too\n",
    "#low for her standards. This time around, she has asked you to focus\n",
    "#on model performance.\n",
    "\n",
    "#Before you start testing different models and parameter sets, you\n",
    "#will need to split the data into training, validation, and testing\n",
    "#datasets. Remember that after splitting the data into training and\n",
    "#testing datasets, the validation dataset is created by splitting the\n",
    "#training dataset.\n",
    "\n",
    "#The datasets X and y have been loaded for your use.\n",
    "\n",
    "# Create temporary training and final testing datasets\n",
    "#X_temp, X_test, y_temp, y_test  =\\\n",
    "#    train_test_split(X, y, test_size=0.20, random_state=1111)\n",
    "\n",
    "# Create the final training and validation datasets\n",
    "#X_train, X_val, y_train, y_val  =\\\n",
    "#    train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)\n",
    "\n",
    "#################################################\n",
    "#You now have training, validation, and testing datasets, but do you\n",
    "#know when you need both validation and testing datasets?\n",
    "#When testing parameters, tuning hyper-parameters, or anytime you are\n",
    "#frequently evaluating model performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Accuracy metrics: regression models**\n",
    "___\n",
    "- Mean Absolute Error (MAE)\n",
    "    - most intuitive\n",
    "- Mean Squared Error (MSE)\n",
    "    - same as MAE except the difference term is squared\n",
    "    - allows outlier errors to contribute more to the overall error\n",
    "    - most widely used\n",
    "- MAE vs MSE\n",
    "    - accuracy metrics are always application specific\n",
    "    - MAE and MSE error terms are in different units and should not be compared\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Mean absolute error\n",
    "\n",
    "#Communicating modeling results can be difficult. However, most clients\n",
    "#understand that on average, a predictive model was off by some number.\n",
    "#This makes explaining the mean absolute error easy. For example, when\n",
    "#predicting the number of wins for a basketball team, if you predict 42,\n",
    "#and they end up with 40, you can easily explain that the error was\n",
    "#two wins.\n",
    "\n",
    "#In this exercise, you are interviewing for a new position and are\n",
    "#provided with two arrays. y_test, the true number of wins for all\n",
    "#30 NBA teams in 2017 and predictions, which contains a prediction\n",
    "#for each team. To test your understanding, you are asked to both\n",
    "#manually calculate the MAE and use sklearn.\n",
    "\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Manually calculate the MAE\n",
    "#n = len(predictions)\n",
    "#mae_one = sum(abs(y_test - predictions)) / n\n",
    "#print('With a manual calculation, the error is {}'.format(mae_one))\n",
    "\n",
    "# Use scikit-learn to calculate the MAE\n",
    "#mae_two = mean_absolute_error(y_test, predictions)\n",
    "#print('Using scikit-lean, the error is {}'.format(mae_two))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    With a manual calculation, the error is 5.9\n",
    "#    Using scikit-lean, the error is 5.9\n",
    "#################################################\n",
    "#These predictions were about six wins off on average. This isn't\n",
    "#too bad considering NBA teams play 82 games a year. Let's see how\n",
    "#these errors would look if you used the mean squared error instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Mean squared error\n",
    "\n",
    "#Let's focus on the 2017 NBA predictions again. Every year, there are\n",
    "#at least a couple of NBA teams that win way more games than expected.\n",
    "#If you use the MAE, this accuracy metric does not reflect the bad\n",
    "#predictions as much as if you use the MSE. Squaring the large errors\n",
    "#from bad predictions will make the accuracy look worse.\n",
    "\n",
    "#In this example, NBA executives want to better predict team wins.\n",
    "#You will use the mean squared error to calculate the prediction\n",
    "#error. The actual wins are loaded as y_test and the predictions as\n",
    "#predictions.\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#n = len(predictions)\n",
    "# Finish the manual calculation of the MSE\n",
    "#mse_one = sum((y_test - predictions)**2) / n\n",
    "#print('With a manual calculation, the error is {}'.format(mse_one))\n",
    "\n",
    "# Use the scikit-learn function to calculate MSE\n",
    "#mse_two = mean_squared_error(y_test, predictions)\n",
    "#print('Using scikit-learn, the error is {}'.format(mse_two))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    With a manual calculation, the error is 49.1\n",
    "#    Using scikit-learn, the error is 49.1\n",
    "#################################################\n",
    "# If you run any additional models, you will try to beat an MSE of 49.1,\n",
    "#which is the average squared error of using your model. Although the\n",
    "#MSE is not as interpretable as the MAE, it will help us select a\n",
    "#model that has fewer 'large' errors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Performance on data subsets\n",
    "\n",
    "#In professional basketball, there are two conferences, the East and\n",
    "#the West. Coaches and fans often only care about how teams in their\n",
    "#own conference will do this year.\n",
    "\n",
    "#You have been working on an NBA prediction model and would like to\n",
    "#determine if the predictions were better for the East or West\n",
    "#conference. You added a third array to your data called labels,\n",
    "#which contains an \"E\" for the East teams, and a \"W\" for the West.\n",
    "#y_test and predictions have again been loaded for your use.\n",
    "\n",
    "# Find the East conference teams\n",
    "#east_teams = labels == \"E\"\n",
    "\n",
    "# Create arrays for the true and predicted values\n",
    "#true_east = y_test[east_teams]\n",
    "#preds_east = predictions[east_teams]\n",
    "\n",
    "# Print the accuracy metrics\n",
    "#print('The MAE for East teams is {}'.format(\n",
    "#    mae(true_east, preds_east)))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The MAE for East teams is 6.733333333333333\n",
    "#################################################\n",
    "\n",
    "# Print the West accuracy\n",
    "#print('The MAE for West conference is {}'.format(west_error))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    <script.py> output:\n",
    "#    The MAE for West conference is 5.01\n",
    "#################################################\n",
    "#It looks like the Western conference predictions were about two\n",
    "#games better on average. Over the past few seasons, the Western teams\n",
    "#have generally won the same number of games as the experts have\n",
    "#predicted. Teams in the East are just not as predictable as those\n",
    "#in the West."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Classification metrics**\n",
    "___\n",
    "- Examples\n",
    "    - **precision**\n",
    "    - **recall**/sensitivity\n",
    "    - **accuracy**\n",
    "    - specificity\n",
    "    - f-1 score and its variations\n",
    "    - etc...\n",
    "\n",
    "- Confusion Matrix\n",
    "    - confusion_matrix\n",
    "![_images/17.1.PNG](_images/17.1.PNG)\n",
    "\n",
    "- Accuracy\n",
    "    - overall ability of a model to predict correct classification\n",
    "![_images/17.2.PNG](_images/17.2.PNG)\n",
    "\n",
    "- Precision\n",
    "    - when we don't want to overpredict positive values\n",
    "    - e.g., the number of invited interviewees who accept a position\n",
    "![_images/17.3.PNG](_images/17.3.PNG)\n",
    "\n",
    "- Recall\n",
    "    - when we can't afford to miss any positive values\n",
    "    - e.g., even if a patient has a small chance of having cancer\n",
    "![_images/17.4.PNG](_images/17.4.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Confusion matrices\n",
    "\n",
    "#Confusion matrices are a great way to start exploring your model's\n",
    "#accuracy. They provide the values needed to calculate a wide range\n",
    "#of metrics, including sensitivity, specificity, and the F1-score.\n",
    "\n",
    "#You have built a classification model to predict if a person has a\n",
    "#broken arm based on an X-ray image. On the testing set, you have the\n",
    "#following confusion matrix:\n",
    "\n",
    "#\t         Prediction: 0\tPrediction: 1\n",
    "#Actual: 0\t 324 (TN)\t    15 (FP)\n",
    "#Actual: 1\t 123 (FN)\t    491 (TP)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "#accuracy = (491 + 324) / (953)\n",
    "#print(\"The overall accuracy is {0: 0.2f}\".format(accuracy))\n",
    "\n",
    "# Calculate and print the precision\n",
    "#precision = (491) / (491 + 15)\n",
    "#print(\"The precision is {0: 0.2f}\".format(precision))\n",
    "\n",
    "# Calculate and print the recall\n",
    "#recall = (491) / (491 + 123)\n",
    "#print(\"The recall is {0: 0.2f}\".format(recall))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The overall accuracy is  0.86\n",
    "#    The precision is  0.97\n",
    "#    The recall is  0.80\n",
    "#################################################\n",
    "#In this case, a true positive is a picture of an actual broken arm\n",
    "#that was also predicted to be broken. Doctors are okay with a few\n",
    "#additional false positives (predicted broken, not actually broken),\n",
    "#as long as you don't miss anyone who needs immediate medical attention."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Confusion matrices, again\n",
    "\n",
    "#Creating a confusion matrix in Python is simple. The biggest challenge\n",
    "#will be making sure you understand the orientation of the matrix. This\n",
    "#exercise makes sure you understand the sklearn implementation of\n",
    "#confusion matrices. Here, you have created a random forest model\n",
    "#using the tic_tac_toe dataset rfc to predict outcomes of 0 (loss) or\n",
    "#1 (a win) for Player One.\n",
    "\n",
    "#Note: If you read about confusion matrices on another website or for\n",
    "#another programming language, the values might be reversed.\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create predictions\n",
    "#test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "#cm = confusion_matrix(y_test, test_predictions)\n",
    "#print(cm)\n",
    "\n",
    "# Print the true positives (actual 1s that were predicted 1s)\n",
    "#print(\"The number of true positives is: {}\".format(cm[1, 1]))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [[177 123]\n",
    "#     [ 92 471]]\n",
    "#    The number of true positives is: 471\n",
    "#################################################\n",
    "#Row 1, column 1 represents the number of actual 1s that were predicted\n",
    "#1s (the true positives). Always make sure you understand the orientation\n",
    "#of the confusion matrix before you start using it!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Precision vs. recall\n",
    "\n",
    "#The accuracy metrics you use to evaluate your model should always be\n",
    "#based on the specific application. For this example, let's assume\n",
    "#you are a really sore loser when it comes to playing Tic-Tac-Toe,\n",
    "#but only when you are certain that you are going to win.\n",
    "\n",
    "#Choose the most appropriate accuracy metric, either precision or\n",
    "#recall, to complete this example. But remember, if you think you\n",
    "#are going to win, you better win!\n",
    "\n",
    "#Use rfc, which is a random forest classification model built on the\n",
    "#tic_tac_toe dataset.\n",
    "\n",
    "#from sklearn.metrics import precision_score\n",
    "\n",
    "#test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create precision or recall score based on the metric you imported\n",
    "#score = precision_score(y_test, test_predictions)\n",
    "\n",
    "# Print the final result\n",
    "#print(\"The precision value is {0:.2f}\".format(score))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The precision value is 0.79\n",
    "#################################################\n",
    "#Precision is the correct metric here. Sore-losers can't stand losing\n",
    "#when they are certain they will win! For that reason, our model needs\n",
    "#to be as precise as possible. With a precision of only 79%, you may\n",
    "#need to try some other modeling techniques to improve this score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The bias-variance tradeoff**\n",
    "___\n",
    "- Variance\n",
    "    - following the training data too closely\n",
    "    - fails to generalize to test data\n",
    "    - low training error but high testing error\n",
    "    - occurs when models are overfit and have high complexity\n",
    "![_images/17.5.PNG](_images/17.5.PNG)\n",
    "\n",
    "- Bias\n",
    "    - failing to find the relationship between the data and the response\n",
    "    - high training and testing error\n",
    "    - occurs when models are underfit\n",
    "    - e.g., not enough trees, trees not deep enough\n",
    "![_images/17.6.PNG](_images/17.6.PNG)\n",
    "\n",
    "- Optimal performance\n",
    "![_images/17.7.PNG](_images/17.7.PNG)\n",
    "\n",
    "- Parameters causing over/underfitting\n",
    "    - max_depth\n",
    "    - max_features\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Error due to under/over-fitting\n",
    "\n",
    "#The candy dataset is prime for overfitting. With only 85 observations,\n",
    "#if you use 20% for the testing dataset, you are losing a lot of vital\n",
    "#data that could be used for modeling. Imagine the scenario where most\n",
    "#of the chocolate candies ended up in the training data and very few\n",
    "#in the holdout sample. Our model might only see that chocolate is a\n",
    "#vital factor, but fail to find that other attributes are also\n",
    "#important. In this exercise, you'll explore how using too many features\n",
    "#(columns) in a random forest model can lead to overfitting.\n",
    "\n",
    "#A feature represents which columns of the data are used in a\n",
    "#decision tree. The parameter max_features limits the number of\n",
    "#features available.\n",
    "\n",
    "# Update the rfr model\n",
    "#rfr = RandomForestRegressor(n_estimators=25,\n",
    "#                            random_state=1111,\n",
    "#                            max_features=2)\n",
    "#rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies\n",
    "#print('The training error is {0:.2f}'.format(\n",
    "#  mae(y_train, rfr.predict(X_train))))\n",
    "#print('The testing error is {0:.2f}'.format(\n",
    "#  mae(y_test, rfr.predict(X_test))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The training error is 3.88\n",
    "#    The testing error is 9.15\n",
    "#################################################\n",
    "\n",
    "# Update the rfr model\n",
    "#rfr = RandomForestRegressor(n_estimators=25,\n",
    "#                            random_state=1111,\n",
    "#                            max_features=11)\n",
    "#rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies\n",
    "#print('The training error is {0:.2f}'.format(\n",
    "#  mae(y_train, rfr.predict(X_train))))\n",
    "#print('The testing error is {0:.2f}'.format(\n",
    "#  mae(y_test, rfr.predict(X_test))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The training error is 3.57\n",
    "#    The testing error is 10.05\n",
    "#################################################\n",
    "\n",
    "# Update the rfr model\n",
    "#rfr = RandomForestRegressor(n_estimators=25,\n",
    "#                            random_state=1111,\n",
    "#                            max_features=4)\n",
    "#rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies\n",
    "#print('The training error is {0:.2f}'.format(\n",
    "#  mae(y_train, rfr.predict(X_train))))\n",
    "#print('The testing error is {0:.2f}'.format(\n",
    "#  mae(y_test, rfr.predict(X_test))))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The training error is 3.60\n",
    "#    The testing error is 8.79\n",
    "#################################################\n",
    "#The chart below shows the performance at various max feature values.\n",
    "#Sometimes, setting parameter values can make a huge difference in\n",
    "#model performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/17.8.png](_images/17.8.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Am I underfitting?\n",
    "\n",
    "#You are creating a random forest model to predict if you will win a\n",
    "#future game of Tic-Tac-Toe. Using the tic_tac_toe dataset, you have\n",
    "#created training and testing datasets, X_train, X_test, y_train, and\n",
    "#y_test.\n",
    "\n",
    "#You have decided to create a bunch of random forest models with\n",
    "#varying amounts of trees (1, 2, 3, 4, 5, 10, 20, and 50). The more\n",
    "#trees you use, the longer your random forest model will take to run.\n",
    "#However, if you don't use enough trees, you risk underfitting. You\n",
    "#have created a for loop to test your model at the different number\n",
    "#of trees.\n",
    "\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "#test_scores, train_scores = [], []\n",
    "#for i in [1, 2, 3, 4, 5, 10, 20, 50]:\n",
    "#    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)\n",
    "#    rfc.fit(X_train, y_train)\n",
    "    # Create predictions for the X_train and X_test datasets.\n",
    "#    train_predictions = rfc.predict(X_train)\n",
    "#    test_predictions = rfc.predict(X_test)\n",
    "    # Append the accuracy score for the test and train predictions.\n",
    "#    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))\n",
    "#    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))\n",
    "# Print the train and test scores.\n",
    "#print(\"The training scores were: {}\".format(train_scores))\n",
    "#print(\"The testing scores were: {}\".format(test_scores))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The training scores were: [0.94, 0.93, 0.98, 0.97, 0.99, 1.0, 1.0, 1.0]\n",
    "#    The testing scores were: [0.83, 0.79, 0.89, 0.91, 0.91, 0.93, 0.97, 0.98]\n",
    "#################################################\n",
    "#Notice that with only one tree, both the train and test scores are\n",
    "#low. As you add more trees, both errors improve. Even at 50 trees,\n",
    "#this still might not be enough. Every time you use more trees, you\n",
    "#achieve higher accuracy. At some point though, more trees increase\n",
    "#training time, but do not decrease testing error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The problems with holdout sets**\n",
    "___\n",
    "- the split matters\n",
    "    - sampling error for train/test samples\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Two samples\n",
    "\n",
    "#After building several classification models based on the tic_tac_toe\n",
    "#dataset, you realize that some models do not generalize as well as\n",
    "#others. You have created training and testing splits just as you\n",
    "#have been taught, so you are curious why your validation process is\n",
    "#not working.\n",
    "\n",
    "#After trying a different training, test split, you noticed differing\n",
    "#accuracies for your machine learning model. Before getting too frustrated\n",
    "#with the varying results, you have decided to see what else could be\n",
    "#going on.\n",
    "\n",
    "# Create two different samples of 200 observations\n",
    "#sample1 = tic_tac_toe.sample(200, random_state=1111)\n",
    "#sample2 = tic_tac_toe.sample(200, random_state=1171)\n",
    "\n",
    "# Print the number of common observations\n",
    "#print(len([index for index in sample1.index if index in sample2.index]))\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    40\n",
    "#################################################\n",
    "\n",
    "# Print the number of observations in the Class column for both samples\n",
    "#print(sample1['Class'].value_counts())\n",
    "#print(sample2['Class'].value_counts())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    positive    134\n",
    "#    negative     66\n",
    "#    Name: Class, dtype: int64\n",
    "#    positive    123\n",
    "#    negative     77\n",
    "#    Name: Class, dtype: int64\n",
    "#################################################\n",
    "#Notice that there are a varying number of positive observations for\n",
    "#both sample test sets. Sometimes creating a single test holdout\n",
    "#sample is not enough to achieve the high levels of model validation\n",
    "#you want. You need to use something more robust.\n",
    "#\n",
    "# If our models are not generalizing well or if we have limited data,\n",
    "#we should be careful using a single training/validation split. You\n",
    "#should use the next lesson's topic: cross-validation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cross-validation**\n",
    "___\n",
    "![_images/17.9.PNG](_images/17.9.PNG)\n",
    "![_images/17.10.PNG](_images/17.10.PNG)\n",
    "- KFold in sklearn.model_selection\n",
    "    - n_splits: number of cross-validation splits\n",
    "    - shuffle: boolean indicating to shuffle data before splitting\n",
    "    - random_state: random seed\n",
    "    - .split(X): indices of splits\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#scikit-learn's KFold()\n",
    "\n",
    "#You just finished running a colleagues code that creates a random\n",
    "#forest model and calculates an out-of-sample accuracy. You noticed\n",
    "#that your colleague's code did not have a random state, and the\n",
    "#errors you found were completely different than the errors your\n",
    "#colleague reported.\n",
    "\n",
    "#To get a better estimate for how accurate this random forest model\n",
    "#will be on new data, you have decided to generate some indices to\n",
    "#use for KFold cross-validation.\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "# Use KFold\n",
    "#kf = KFold(n_splits=5, shuffle=True, random_state=1111)\n",
    "\n",
    "# Create splits\n",
    "#splits = kf.split(X)\n",
    "\n",
    "# Print the number of indices\n",
    "#for train_index, val_index in splits:\n",
    "#    print(\"Number of training indices: %s\" % len(train_index))\n",
    "#    print(\"Number of validation indices: %s\" % len(val_index))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Number of training indices: 68\n",
    "#    Number of validation indices: 17\n",
    "#    Number of training indices: 68\n",
    "#    Number of validation indices: 17\n",
    "#    Number of training indices: 68\n",
    "#    Number of validation indices: 17\n",
    "#    Number of training indices: 68\n",
    "#    Number of validation indices: 17\n",
    "#    Number of training indices: 68\n",
    "#    Number of validation indices: 17\n",
    "#################################################\n",
    "#This dataset has 85 rows. You have created five splits - each\n",
    "#containing 68 training and 17 validation indices. You can use these\n",
    "#indices to complete 5-fold cross-validation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using KFold indices\n",
    "\n",
    "#You have already created splits, which contains indices for the\n",
    "#candy-data dataset to complete 5-fold cross-validation. To get a\n",
    "#better estimate for how well a colleague's random forest model will\n",
    "#perform on a new data, you want to run this model on the five\n",
    "#different training and validation indices you just created.\n",
    "\n",
    "#In this exercise, you will use these indices to check the accuracy\n",
    "#of this model using the five different splits. A for loop has been\n",
    "#provided to assist with this process.\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n",
    "\n",
    "# Access the training and validation indices of splits\n",
    "#for train_index, val_index in splits:\n",
    "    # Setup the training and validation data\n",
    "#    X_train, y_train = X[train_index], y[train_index]\n",
    "#    X_val, y_val = X[val_index], y[val_index]\n",
    "    # Fit the random forest model\n",
    "#    rfc.fit(X_train, y_train)\n",
    "    # Make predictions, and print the accuracy\n",
    "#    predictions = rfc.predict(X_val)\n",
    "#    print(\"Split accuracy: \" + str(mean_squared_error(y_val, predictions)))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Split accuracy: 178.75586448813047\n",
    "#    Split accuracy: 98.29560208158634\n",
    "#    Split accuracy: 86.2673010849621\n",
    "#    Split accuracy: 217.4185114496197\n",
    "#    Split accuracy: 140.5437661156536\n",
    "#################################################\n",
    "#KFold() is a great method for accessing individual indices when\n",
    "#completing cross-validation. One drawback is needing a for loop to\n",
    "#work through the indices though. In the next lesson, you will look\n",
    "#at an automated method for cross-validation using sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**sklearn's cross_val_score()**\n",
    "___\n",
    "- cross_val_score()\n",
    "    - estimator: the model to use\n",
    "    - X, y: predictor and response arrays\n",
    "    - cv: the number of cross-validation splits\n",
    "    - scoring: see make_scorer below\n",
    "- make_scorer from sklearn.metrics to create a scorer for cross_val_score()\n",
    "- mean of errors is usually reported with std to indicate variance of errors\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#scikit-learn's methods\n",
    "\n",
    "#You have decided to build a regression model to predict the number\n",
    "#of new employees your company will successfully hire next month. You\n",
    "#open up a new Python script to get started, but you quickly realize\n",
    "#that sklearn has a lot of different modules. Let's make sure you\n",
    "#understand the names of the modules, the methods, and which module\n",
    "#contains which method.\n",
    "\n",
    "#Follow the instructions below to load in all of the necessary methods\n",
    "#for completing cross-validation using sklearn. You will use modules:\n",
    "\n",
    "#metrics\n",
    "#model_selection\n",
    "#ensemble\n",
    "\n",
    "# Instruction 1: Load the cross-validation method\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instruction 2: Load the random forest regression model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instruction 3: Load the mean squared error method\n",
    "# Instruction 4: Load the function for creating a scorer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "#################################################\n",
    "#It is easy to see how all of the methods can get mixed up, but it\n",
    "#is important to know the names of the methods you need. You can\n",
    "#always review the scikit-learn documentation should you need any help\n",
    "#https://scikit-learn.org/stable/documentation.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Implement cross_val_score()\n",
    "\n",
    "#Your company has created several new candies to sell, but they are\n",
    "#not sure if they should release all five of them. To predict the\n",
    "#popularity of these new candies, you have been asked to build a\n",
    "#regression model using the candy dataset. Remember that the response\n",
    "#value is a head-to-head win-percentage against other candies.\n",
    "\n",
    "#Before you begin trying different regression models, you have\n",
    "#decided to run cross-validation on a simple random forest model to\n",
    "#get a baseline error to compare with any future results.\n",
    "\n",
    "#rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n",
    "#mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Set up cross_val_score\n",
    "#cv = cross_val_score(estimator=rfc,\n",
    "#                     X=X_train,\n",
    "#                     y=y_train,\n",
    "#                     cv=10,\n",
    "#                     scoring=make_scorer(mean_squared_error))\n",
    "\n",
    "# Print the mean error\n",
    "#print(cv.mean())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    155.55845080026586\n",
    "#################################################\n",
    "#You now have a baseline score to build on. If you decide to build\n",
    "#additional models or try new techniques, you should try to get an\n",
    "#error lower than 155.56. Lower errors indicate that your popularity\n",
    "#predictions are improving"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Leave-one-out-cross-validation (LOOCV)**\n",
    "___\n",
    "- n models for n observations\n",
    "![_images/17.11.PNG](_images/17.11.PNG)\n",
    "- when to use LOOCV\n",
    "    - the amount of training data is limited\n",
    "    - you want the absolute best error estimate for new data\n",
    "    - this method is very computationally expensive\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Leave-one-out-cross-validation\n",
    "\n",
    "#Let's assume your favorite candy is not in the candy dataset, and\n",
    "#that you are interested in the popularity of this candy. Using 5-fold\n",
    "#cross-validation will train on only 80% of the data at a time. The\n",
    "#candy dataset only has 85 rows though, and leaving out 20% of the\n",
    "#data could hinder our model. However, using leave-one-out-cross-validation\n",
    "#allows us to make the most out of our limited dataset and will give\n",
    "#you the best estimate for your favorite candy's popularity!\n",
    "\n",
    "#In this exercise, you will use cross_val_score() to perform LOOCV.\n",
    "\n",
    "#from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "# Create scorer\n",
    "#mae_scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "#rfr = RandomForestRegressor(n_estimators=15, random_state=1111)\n",
    "\n",
    "# Implement LOOCV\n",
    "#scores = cross_val_score(rfr, X=X, y=y, cv=y.shape[0], scoring=mae_scorer)\n",
    "\n",
    "# Print the mean and standard deviation\n",
    "#print(\"The mean of the errors is: %s.\" % np.mean(scores))\n",
    "#print(\"The standard deviation of the errors is: %s.\" % np.std(scores))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The mean of the errors is: 9.464989603398694.\n",
    "#    The standard deviation of the errors is: 7.265762094853885.\n",
    "#################################################\n",
    "#You have come along way with model validation techniques. The final\n",
    "#chapter will wrap up model validation by discussing how to select\n",
    "#the best model and give an introduction to parameter tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to hyperparameter tuning**\n",
    "___\n",
    "- model parameters are:\n",
    "    - learned or estimated from the data\n",
    "    - the result of fitting a model\n",
    "    - used when making future predictions\n",
    "    - not manually set\n",
    "- linear regression parameters\n",
    "    - coefficients and intercepts\n",
    "        - .coef_, .intercept_\n",
    "- model hyperparameters\n",
    "    - manually set *before* the training occurs\n",
    "    - specify how the training is supposed to happen\n",
    "- random forest hyperparameters\n",
    "    - n_estimators\n",
    "    - max_depth\n",
    "    - max_features\n",
    "    - min_samples_split\n",
    "        - the minimum number of samples required to make a split\n",
    "- hyperparameter tuning\n",
    "    - select hyperparameters\n",
    "    - run a single model type at different value sets\n",
    "    - create ranges of possible values to select from\n",
    "    - specify a single accuracy metric\n",
    "- specifying ranges\n",
    "    - .get_params_\n",
    "    - start with the basics\n",
    "    - read through the documentation\n",
    "    - test practical ranges\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating Hyperparameters\n",
    "\n",
    "#For a school assignment, your professor has asked your class to\n",
    "#create a random forest model to predict the average test score for\n",
    "#the final exam.\n",
    "\n",
    "#After developing an initial random forest model, you are unsatisfied\n",
    "#with the overall accuracy. You realize that there are too many\n",
    "#hyperparameters to choose from, and each one has a lot of possible\n",
    "#values. You have decided to make a list of possible ranges for the\n",
    "#hyperparameters you might use in your next model.\n",
    "\n",
    "#Your professor has provided de-identified data for the last ten\n",
    "#quizzes to act as the training data. There are 30 students in your\n",
    "#class.\n",
    "\n",
    "# Review the parameters of rfr\n",
    "#print(rfr.get_params())\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    {'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': 1111, 'verbose': 0, 'warm_start': False}\n",
    "#################################################\n",
    "\n",
    "# Maximum Depth\n",
    "#max_depth = [4, 8, 12]\n",
    "\n",
    "# Minimum samples for a split\n",
    "#min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Max features\n",
    "#max_features = [4, 6, 8, 10]\n",
    "\n",
    "#################################################\n",
    "#Hyperparameter tuning requires selecting parameters to tune, as\n",
    "#well the possible values these parameters can be set to."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Running a model using ranges\n",
    "\n",
    "#You have just finished creating a list of hyperparameters and ranges\n",
    "#to use when tuning a predictive model for an assignment. You have\n",
    "#used max_depth, min_samples_split, and max_features as your range\n",
    "#variable names.\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Fill in rfr using your variables\n",
    "#rfr = RandomForestRegressor(\n",
    "#    n_estimators=100,\n",
    "#    max_depth=random.choice(max_depth),\n",
    "#    min_samples_split=random.choice(min_samples_split),\n",
    "#    max_features=random.choice(max_features))\n",
    "\n",
    "# Print out the parameters\n",
    "#print(rfr.get_params())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    {'bootstrap': True, 'criterion': 'mse', 'max_depth': 4, 'max_features': 10, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
    "#################################################\n",
    "#Notice that min_samples_split was randomly set to 2. Since you\n",
    "#specified a random state, min_samples_split will always be set to\n",
    "#2 if you only run this model one time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**RandomizedSearchCV**\n",
    "___\n",
    "- Grid Searching hyperparameters\n",
    "![_images/17.12.PNG](_images/17.12.PNG)\n",
    "- tests every possible combination\n",
    "- each additional hyperparameter added increases computation time exponentially\n",
    "- better methods\n",
    "    - random searching using RandomizedSearch CV in sklearn.model_selection\n",
    "    - bayesian optimization\n",
    "- random search parameters\n",
    "    - estimator: the model to use\n",
    "    - param_distributions: dictionary containing hyperparameters and possible values\n",
    "    - n_iter: number of iterations\n",
    "    - scoring: scoring method to use\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Preparing for RandomizedSearch\n",
    "\n",
    "#Last semester your professor challenged your class to build a\n",
    "#predictive model to predict final exam test scores. You tried\n",
    "#running a few different models by randomly selecting hyperparameters.\n",
    "#However, running each model required you to code it individually.\n",
    "\n",
    "#After learning about RandomizedSearchCV(), you're revisiting your\n",
    "#professors challenge to build the best model. In this exercise, you\n",
    "#will prepare the three necessary inputs for completing a random\n",
    "#search.\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Finish the dictionary by adding the max_depth parameter\n",
    "#param_dist = {\"max_depth\": [2, 4, 6, 8],\n",
    "#              \"max_features\": [2, 4, 6, 8, 10],\n",
    "#              \"min_samples_split\": [2, 4, 8, 16]}\n",
    "\n",
    "# Create a random forest regression model\n",
    "#rfr = RandomForestRegressor(n_estimators=10, random_state=1111)\n",
    "\n",
    "# Create a scorer to use (use the mean squared error)\n",
    "#scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "#################################################\n",
    "#To use RandomizedSearchCV(), you need a distribution dictionary,\n",
    "#an estimator, and a scorer—once you've got these, you can run a\n",
    "#random search to find the best parameters for your model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Implementing RandomizedSearchCV\n",
    "\n",
    "#You are hoping that using a random search algorithm will help you\n",
    "#improve predictions for a class assignment. You professor has\n",
    "#challenged your class to predict the overall final exam average\n",
    "#score.\n",
    "\n",
    "#In preparation for completing a random search, you have created:\n",
    "\n",
    "#param_dist: the hyperparameter distributions\n",
    "#rfr: a random forest regression model\n",
    "#scorer: a scoring method to use\n",
    "\n",
    "# Import the method for random search\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Build a random search using param_dist, rfr, and scorer\n",
    "#random_search =\\\n",
    "#    RandomizedSearchCV(\n",
    "#        estimator=rfr,\n",
    "#        param_distributions=param_dist,\n",
    "#        n_iter=10,\n",
    "#        cv=5,\n",
    "#        scoring=scorer)\n",
    "\n",
    "#################################################\n",
    "#Although it takes a lot of steps, hyperparameter tuning with\n",
    "#random search is well worth it and can improve the accuracy of\n",
    "#your models. Plus, you are already using cross-validation to\n",
    "#validate your best model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Selecting your final model**\n",
    "___\n",
    "- .best_score_\n",
    "- .best_params_\n",
    "- .best_estimator_\n",
    "    - best model\n",
    "- other attributes\n",
    "    - .cv_results_\n",
    "        - ['mean_test_score']\n",
    "        - ['params']\n",
    "        - ['max_depth']\n",
    "- save model for use later\n",
    "    - from sklearn.externals import joblib\n",
    "    - joblib.dump(model, 'model_best_<date>.pkl')\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Selecting the best precision model\n",
    "\n",
    "#Your boss has offered to pay for you to see three sports games this\n",
    "#year. Of the 41 home games your favorite team plays, you want to\n",
    "#ensure you go to three home games that they will definitely win.\n",
    "#You build a model to decide which games your team will win.\n",
    "\n",
    "#To do this, you will build a random search algorithm and focus on\n",
    "#model precision (to ensure your team wins). You also want to keep\n",
    "#track of your best model and best parameters, so that you can use\n",
    "#them again next year (if the model does well, of course). You have\n",
    "#already decided on using the random forest classification model rfc\n",
    "#and generated a parameter distribution param_dist.\n",
    "\n",
    "#from sklearn.metrics import precision_score, make_scorer\n",
    "\n",
    "# Create a precision scorer\n",
    "#precision = make_scorer(precision_score)\n",
    "# Finalize the random search\n",
    "#rs = RandomizedSearchCV(\n",
    "#  estimator=rfc, param_distributions=param_dist,\n",
    "#  scoring = precision,\n",
    "#  cv=5, n_iter=10, random_state=1111)\n",
    "#rs.fit(X, y)\n",
    "\n",
    "# print the mean test scores:\n",
    "#print('The precision for each run was: {}.'.format(rs.cv_results_['mean_test_score']))\n",
    "# print the best model score:\n",
    "#print('The best precision for a single model was: {}'.format(rs.best_score_))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    The precision for each run was: [0.86446668 0.75302055 0.67570816 0.88459939 0.88381178 0.86917588\n",
    "#     0.68014695 0.81721906 0.87895856 0.92917474].\n",
    "#    The best precision for a single model was: 0.9291747446879924\n",
    "#################################################\n",
    "#Your model's precision was 93%! The best model accurately predicts\n",
    "#a winning game 93% of the time. If you look at the mean test scores,\n",
    "#you can tell some of the other parameter sets did really poorly.\n",
    "#Also, since you used cross-validation, you can be confident in your\n",
    "#predictions. Well done!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Course completed!**\n",
    "___\n",
    "- Some topics covered:\n",
    "    - accuracy/evaluation metrics\n",
    "    - splitting data into train, validation, and test sets\n",
    "    - cross-validation and LOOCV\n",
    "    - hyperparameter tuning\n",
    "- Next steps\n",
    "    - check out kaggle\n",
    "    - DataCamp courses\n",
    "        - hyperparameter tuning in Python\n",
    "        - deep learning in Python\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}