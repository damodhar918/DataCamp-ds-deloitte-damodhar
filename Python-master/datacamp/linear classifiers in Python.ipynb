{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**scikit-learn refresher**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#KNN classification\n",
    "\n",
    "#In this exercise you'll explore a subset of the Large Movie Review Dataset.\n",
    "#'The variables X_train, X_test, y_train, and y_test are already loaded\n",
    "#into the environment. The X variables contain features based on the words\n",
    "#in the movie reviews, and the y variables contain labels for whether the\n",
    "#review sentiment is positive (+1) or negative (-1).\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model\n",
    "knn = KNeighborsClassifier()\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "#pred = knn.predict(X_test)[0]\n",
    "#print(\"Prediction for test example 0:\", pred)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Prediction for test example 0: 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Comparing models\n",
    "#create two instances of KNeighborsClassifier with n_neighbors=1, 5\n",
    "#fit training data to both instances\n",
    "\n",
    "#test for accuracy\n",
    "#knn1.score(X_test, y_test)\n",
    "#knn5.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Review:\n",
    "- **Underfitting**: model is too simple, low training accuracy\n",
    "- **Overfitting**: model is too complex, low test accuracy\n",
    "\n",
    "an example of **overfitting**:\n",
    "- Training accuracy 95%, testing accuracy 50%.\n",
    "\n",
    "*overfitting refers to doing better on the training set than the test set.*\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Applying logistic regression and Support Vector Machines/Classifier (SVM/SVC)**\n",
    "___\n",
    "- SVC - non-linear SVM by default"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p1n3d\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9688888888888889\n",
      "0.9955456570155902\n",
      "0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "#Running LogisticRegression and SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
      "Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n"
     ]
    }
   ],
   "source": [
    "#Sentiment analysis for movie reviews\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "#review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "#print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "#review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "#print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
    "#    Probability of positive review: 0.8079007873616059\n",
    "#    Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n",
    "#    Probability of positive review: 0.5855117402793947"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear classifiers**\n",
    "___\n",
    "Definitions\n",
    "- **classification**: learning to predict categories\n",
    "- **decision boundary**: the surface separating different predicted classes\n",
    "- **linear classifier**: a classifier that learns linear decision boundaries\n",
    "    - e.g., logistic regression, linear SVM\n",
    "- **linearly separable**: a data set can be perfectly explained by a linear classifier\n",
    "___\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p1n3d\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\p1n3d\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Visualizing decision boundaries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "X=wine.data\n",
    "y=wine.target\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(),\n",
    "               SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers - plot is a series of previously defined functions\n",
    "#plot_4_classifiers(X, y, classifiers)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/9.1.svg](_images/9.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear classifiers: the coefficients**\n",
    "___\n",
    "- Dot products\n",
    "    - multiply each element in arrays\n",
    "    - sum elements in remaining array\n",
    "    - x@y\n",
    "- Linear classifier prediction\n",
    "    - raw model output = coefficients[.coef_] @ features + intercept[.intercept_]\n",
    "    - if positive predict one class; if negative predict the other class\n",
    "- This is the same for logistic regression and linear SVM\n",
    "    - i.e. 'fit' is different but 'predict' is the same\n",
    "- **intercept** changes boundary location but not orientation\n",
    "- **coefficients** change orientation of boundary line\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Changing the model coefficients\n",
    "\n",
    "#When you call fit with scikit-learn, the logistic regression coefficients\n",
    "#are automatically learned from your dataset. In this exercise you will\n",
    "#explore how the decision boundary is represented by the coefficients. To\n",
    "#do so, you will change the coefficients manually (instead of with fit),\n",
    "#and visualize the resulting classifiers.\n",
    "\n",
    "#A 2D dataset is already loaded into the environment as X and y,\n",
    "#along with a linear classifier object model\n",
    "\n",
    "# Set the coefficients\n",
    "#model.coef_ = np.array([[0,1]])\n",
    "#model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary using preset function\n",
    "#plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "#num_err = np.sum(y != model.predict(X))\n",
    "#print(\"Number of errors:\", num_err)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Number of errors: 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/9.2.svg](_images/9.2.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is a loss function?**\n",
    "___\n",
    "- Least squares: the squared loss (linear regression)\n",
    "    - minimizes square of the error made on training set\n",
    "    - $$\\sum_{i=1}^{n}(\\text{true ith target value - predicted ith target value})^{2}$$\n",
    "- **loss function**\n",
    "    - penalty score that tells us how well/poorly model is doing on training data\n",
    "- **fit function**\n",
    "    - minimizes loss\n",
    "- *squared loss/error is not appropriate for classification problems*\n",
    "    - 0-1 loss\n",
    "    - 0 for correct prediction, 1 for incorrect prediction\n",
    "        - a natural loss for classification problem is the number of errors\n",
    "- minimizing loss using Python\n",
    "    - from **scipy.optimize** import **minimize**\n",
    "    - inputs are values of model coefficents\n",
    "    - what values of the model coefficients make my squared error as small as possible?\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Minimizing a loss function\n",
    "\n",
    "#In this exercise you'll implement linear regression \"from scratch\" using\n",
    "#scipy.optimize.minimize.\n",
    "\n",
    "#We'll train a model on the Boston housing price data set, which is\n",
    "#already loaded into the variables X and y. For simplicity, we won't\n",
    "#include an intercept in our regression model.\n",
    "\n",
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_true - y_i_pred)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "#w_fit = minimize(my_loss, X[0]).x\n",
    "#print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "#lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "#print(lr.coef_)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [-9.16299112e-02  4.86754828e-02 -3.77698794e-03  2.85635998e+00\n",
    "#     -2.88057050e+00  5.92521269e+00 -7.22470732e-03 -9.67992974e-01\n",
    "#      1.70448714e-01 -9.38971600e-03 -3.92421893e-01  1.49830571e-02\n",
    "#     -4.16973012e-01]\n",
    "#    [-9.16297843e-02  4.86751203e-02 -3.77930006e-03  2.85636751e+00\n",
    "#     -2.88077933e+00  5.92521432e+00 -7.22447929e-03 -9.67995240e-01\n",
    "#      1.70443393e-01 -9.38925373e-03 -3.92425680e-01  1.49832102e-02\n",
    "#     -4.16972624e-01]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loss function diagrams**\n",
    "___\n",
    "- incorrect predictions get a loss penalty of 1\n",
    "- **linear regression** (OLS)\n",
    "    - loss is higher the further away from true target value we are, positive or negative (parabola)\n",
    "- **logistic regression**\n",
    "    - loss is a smooth curve, higher for incorrect predictions\n",
    "- **hinge loss** (SVM)\n",
    "    - consistent with logistic loss curve, but not smooth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUVfrA8e+bRoCQHggtJFRp0kIAqUqRpigICIh0xO7ququ7rq6uu3Zdsa4CKoIUEQWVjnRpoffeQighQCgJkHJ+f9yBXwxJCCGTO8m8n+eZJzNzz9x55xLmzbn3nPeIMQallFLuy8PuAJRSStlLE4FSSrk5TQRKKeXmNBEopZSb00SglFJuzsvuAG5WaGioiYyMtDsMpZQqUtatW3fKGBOW3bYilwgiIyOJjY21OwyllCpSRORQTtv01JBSSrk5TQRKKeXmNBEopZSbK3LXCJRSKjU1lbi4OC5dumR3KC7H19eXSpUq4e3tnefXaCJQShU5cXFxlClThsjISETE7nBchjGGxMRE4uLiiIqKyvPrnHZqSER8RWSNiGwSkW0i8mo2bURERovIXhHZLCKNnRWPUqr4uHTpEiEhIZoEshARQkJCbrqn5MwewWXgLmPMBRHxBpaLyGxjzKpMbboANRy3ZsBnjp9KKZUrTQLZy89xcVqPwFguOB56O25Za173AMY72q4CAkWkvFMCupgIs1+A1BSn7F4ppYoqp44aEhFPEdkInATmG2NWZ2lSETiS6XGc47ms+xkpIrEiEpuQkJC/YA4shtWfwzf3wIV87kMppRz8/Pzy/drhw4ezffv2HLd//fXXxMfH57n9rXJqIjDGpBtjGgKVgBgRqZelSXZ9mOtWyjHGfGGMiTbGRIeFZTtD+sbq9YI+4+H4FhjbAU7tyd9+lFLqFo0ZM4Y6derkuD1rIrhR+1tVKPMIjDFngcVA5yyb4oDKmR5XAuJxljr3wuBf4fIFGNMBDq5w2lsppdyDMYbnn3+eevXqUb9+faZMmQJARkYGjz32GHXr1qV79+507dqVadOmAdCuXTtiY2NJT09n8ODB1177wQcfMG3aNGJjYxkwYAANGzYkJSXlWnuAOXPm0LhxYxo0aED79u0L5DM47WKxiIQBqcaYsyJSEugAvJWl2UzgCRGZjHWROMkYc8xZMQFQKRqGL4CJveHb+6DHp3B7b6e+pVLKeV79eRvb488V6D7rVPDnlXvq5qnt9OnT2bhxI5s2beLUqVM0bdqUNm3asGLFCg4ePMiWLVs4efIktWvXZujQoX947caNGzl69Chbt24F4OzZswQGBvLxxx/z7rvvEh0d/Yf2CQkJjBgxgqVLlxIVFcXp06cL5PM6s0dQHlgkIpuBtVjXCH4RkVEiMsrRZhawH9gLfAk85sR4/l9wFAybB5ViYPpwWPIO6NrNSql8WL58Of369cPT05Ny5crRtm1b1q5dy/Lly+nduzceHh6Eh4dz5513XvfaqlWrsn//fp588knmzJmDv79/ru+1atUq2rRpc22OQHBwcIF8Bqf1CIwxm4FG2Tz/eab7BnjcWTHkqlQwDJwOM5+ERa/D2YPQ/b/gmffZeEop++X1L3dnMTn8EZnT85kFBQWxadMm5s6dyyeffMLUqVMZN25cru/ljGGz7l1ryKsE3P8/aPtX2DABJj4Al5LsjkopVYS0adOGKVOmkJ6eTkJCAkuXLiUmJoZWrVrxww8/kJGRwYkTJ1i8ePF1rz116hQZGRn06tWLf/3rX6xfvx6AMmXKcP78+evat2jRgiVLlnDgwAGAAjs1pCUmRODOv0FgFfj5KRh7NwyYCoERdkemlCoC7r//flauXEmDBg0QEd5++23Cw8Pp1asXCxcupF69etSsWZNmzZoREBDwh9cePXqUIUOGkJGRAcAbb7wBwODBgxk1ahQlS5Zk5cqV19qHhYXxxRdf0LNnTzIyMihbtizz58+/5c8geem+uJLo6GjjtIVp9i+GKQ+Dty/0nwIVrjuzpZRyATt27KB27dp2h3FDFy5cwM/Pj8TERGJiYlixYgXh4eFOf9/sjo+IrDPGRGfX3r1PDWVVtR0MmwuePvBVV9g12+6IlFJFWPfu3WnYsCGtW7fmH//4R6EkgfzQU0NZla0NwxfCpL4wuT90fguajbQ7KqVUEZTddQFXpD2C7JQpZ008q9kZZj8Pc/4GGel2R6WUUk6hiSAnPqWh7wRoNgpWfQJTH4YryXZHpZRSBU4TQW48PKHLW9D5Tdj5K3zTHS6ctDsqpZQqUJoI8qL5o/DgRDixHca0h4RddkeklFIFRhNBXt3WDYb8aq1nMLYjHFhmd0RKKRsdPHiQevWyFlSGl19+mQULFtgQUf5pIrgZFZtYI4r8wuHb+2HTZLsjUkq5mNdee40OHTrYHcZN0URws4KqWAXrIprDj4/A4re0YJ1Sbio9PZ0RI0ZQt25dOnXqREpKCoMHD75WbjoyMpJXXnmFxo0bU79+fXbu3AlYVUQ7duxI48aNeeSRR6hSpQqnTp0CYMKECcTExNCwYUMeeeQR0tOdP2JR5xHkR8lAeGi6VZJi8X/gzEG450Pw8rE7MqXcz+wXrAWnClJ4fejy5g2b7dmzh0mTJvHll1/Sp08ffvjhh+vahIaGsn79ej799FPeffddxowZw6uvvspdd93Fiy++yJw5c/jiiy8Aa0bwlClTWLFiBd7e3jz22GNMnDiRhx9+uGA/XxaaCPLLywfu+wyCoqxkkHTEGm5aMtDuyJRShSQqKoqGDRsC0KRJEw4ePHhdm549e17bPn36dMAqXf3jjz8C0LlzZ4KCggBYuHAh69ato2nTpgCkpKRQtmxZZ38MTQS3RATa/dUqUDfzSRh3N/Sfap0+UkoVjjz85e4sJUqUuHbf09OTlJSUHNt4enqSlpYG5F66etCgQdeKzxUWvUZQEBr2g4E/wvlj1hKYR9fbHZFSyoW1atWKqVOnAjBv3jzOnDkDQPv27Zk2bRonT1rzlU6fPs2hQ4ecHo8mgoIS1RqGzbcql37dzZqAppRS2XjllVeYN28ejRs3Zvbs2ZQvX54yZcpQp04dXn/9dTp16sTtt99Ox44dOXbMuav3gpahLngXTsJ3fSF+A3R+w5qMppQqUEWlDHVOLl++jKenJ15eXqxcuZJHH32UjRs3Ftj+b7YMtV4jKGh+Za2CddNHwJwXrBFFd//HKlehlFLA4cOH6dOnDxkZGfj4+PDll1/aGo8mAmfwKQV9xsO8f1gF684egV5fWoXslFJur0aNGmzYsMHuMK7RawTO4uEJnf8DXd6B3bOt6wbnT9gdlVLFRlE7rV1Y8nNcNBE4W7OR8OB3VqG6MR3g5A67I1KqyPP19SUxMVGTQRbGGBITE/H19b2p1+nF4sISv8G6iJx6Cfp+C1Xb2h2RUkVWamoqcXFxXLp0ye5QXI6vry+VKlXC29v7D8/ndrFYE0FhOnsYJvaBxD1w70fQsL/dESml3IQti9eLSGURWSQiO0Rkm4g8nU2bdiKSJCIbHbeXnRWPSwiMgGFzoUpL+OlRWPQfLVinlLKdM0cNpQHPGWPWi0gZYJ2IzDfGbM/SbpkxprsT43AtvgEwYBr88idY8hacOQT3jgavEjd+rVJKOYHTEoEx5hhwzHH/vIjsACoCWROB+/HygR4fQ3Ak/PY6nDtqXTcoGWR3ZEopN1Qoo4ZEJBJoBKzOZnMLEdkkIrNFpG4Orx8pIrEiEpuQkODESAuRCLR5Hnp+CUdWw9hO1uQzpZQqZE5PBCLiB/wAPGOMOZdl83qgijGmAfAR8FN2+zDGfGGMiTbGRIeFhTk34MJ2ex+rYN2Fk9bw0rgieiFcKVVkOTURiIg3VhKYaIyZnnW7MeacMeaC4/4swFtEQp0Zk0uKbAXDF1gzj7/uBjt+tjsipZQbceaoIQHGAjuMMe/n0Cbc0Q4RiXHEk+ismFxaaA0YtgDK1YMpA2HlJzqiSClVKJw5aqglMBDYIiJXy+r9DYgAMMZ8DjwAPCoiaUAK8KApahMbCpJfGAz+BaaPhLl/g9MHoPOb4KkloZRSzuPMUUPLAblBm4+Bj50VQ5HkXRJ6fwMLXobfP7KWwOw1Fkr42R2ZUqqY0lpDrsjDAzq9Dl3fhT3z4OuucP643VEppYopTQSuLGYE9JsMp/bCl+3hhE7BUEoVPE0Erq7m3TB0NmSkwbi7Yd8iuyNSShUzmgiKgvINYMRCCKgMEx+A9d/aHZFSqhjRRFBUBFSCoXMgqg3MfAIW/kuHlyqlCoQmgqLE1x/6T4XGD8Oyd611kdMu2x2VUqqI0wHqRY2nN9wzGoIiYeFrkHQUHpwIpYLtjkwpVURpj6AoEoHWz1nzC47GwtiOcHq/3VEppYooTQRFWf0H4OGZkJxoFaw7ssbuiJRSRZAmgqKuSgurRlEJf/jmHtg+w+6IlFJFjCaC4iC0ulW9NPx2mDoIVozWEUVKqTzTRFBclA6FQTOhTg+Y/w/49TlIT7M7KqVUEaCjhooT75LwwFewMBJW/NcqWPfAV1qwTimVK+0RFDceHtDxVej+AexdCF91hnPxdkellHJhmgiKq+ih0H+KtabBmA5wfKvdESmlXJQmguKsRkerLIUxMK4z7F1gd0RKKRekiaC4C69vjSgKqgIT+8C6r+2OSCnlYtwqESRecNO6PAEVYchsqHYn/Pw0LHgVMjLsjkop5SLcJhHM2XqcNm8vYsbGo3aHYg9ff+g3BZoMhuXvww/DIPWS3VEppVyA2ySChpUDqVshgKcnb+TF6Zu5lJpud0iFz9MLuv8XOrwK26bD+B5wMdHuqJRSNnObRBAe4Mt3I5rxaLtqTFpzhPs//Z39CRfsDqvwiUCrZ6z5BfEbrIJ1ifvsjkopZSO3SQQAXp4e/LXzbXw1pCnHk1K456Pl/LzJTcfY1+sJg36GlDPW8NLDq+2OSCllE7dKBFfdWassvz7VmtvK+/PkpA289NMW9zxVFNHMGlFUMsgqWLd1ut0RKaVs4JaJAKBCYEkmj2zOI22qMmHVYXp99juHEi/aHVbhC6lmJYOKjWHaEFj+gRasU8rNOC0RiEhlEVkkIjtEZJuIPJ1NGxGR0SKyV0Q2i0hjZ8WTHW9PD17sWpsxD0cTdyaF7qOXM2vLscIMwTWUCoaBP0G9XrDgn/DLM1qwTik34sweQRrwnDGmNtAceFxE6mRp0wWo4biNBD5zYjw56lCnHL8+1YpqZf14bOJ6XpmxlctpbnaqyNsXeo6BVs9ak84m9YXL5+2OSilVCJyWCIwxx4wx6x33zwM7gIpZmvUAxhvLKiBQRMo7K6bcVAoqxdRHWjCsVRTfrDxE789Xcjgx2Y5Q7OPhAR1esdZE3rcIxnWx1kRWShVrhXKNQEQigUZA1qEpFYEjmR7HcX2yQERGikisiMQmJCQ4K0x8vDz4R/c6/G9gEw6cuki3j5YxZ+txp72fy2oyCAZ8D2cOwpj2cGyz3REppZzI6YlARPyAH4BnjDHnsm7O5iXXXak0xnxhjIk2xkSHhYU5I8w/uLtuOLOeak1UaGlGTVjHqz9v40qam5VkqN7eKlgnHvBVF9gz3+6IlFJO4tREICLeWElgojEmu7GJcUDlTI8rAS4xsL9ycCm+H9WCwXdE8tWKg/T+nxueKgqvB8MXQnAUfNcXYsfZHZFSygmcOWpIgLHADmPM+zk0mwk87Bg91BxIMsa4zLCdEl6e/PPeunw2oDH7Ey7QbfQyftnsEnmq8PiXtwrWVW8Pv/wJ5r+sBeuUKmac2SNoCQwE7hKRjY5bVxEZJSKjHG1mAfuBvcCXwGNOjCffutQvz6ynWlOtrB9PfLeBF6dvJuWKG40qKlEGHpwE0cNgxYfWfIPUFLujUkoVEDFFbPJQdHS0iY2NteW9U9MzeH/+bj5bvI8aZf34uH9jaoWXsSUWWxgDKz+GeS9BpRjoNwlKh9odlVIqD0RknTEmOrttbjuzOD+8HbWKxg+N4UzyFe79eDnfrT5MUUum+SYCdzwJvb+B45utGkWn9todlVLqFmkiyIc2NcOY9XRrYqKC+duPW3jiuw0kpaTaHVbhqXsfDPrFmnA2tgMcWml3REqpW6CJIJ/KlvHlmyExvNDlNuZuO0630ctYf/iM3WEVnspNYfh8KBUC4++FLdPsjkgplU+aCG6Bh4cwqm01po5qAUCfz1fy2eJ9ZGS4yami4KowbD5UjLZWPFv2nhasU6oI0kRQABpHBPHrU625u244b83ZyaCv1pBw3k3WRy4VDA//BPV7w8LXYOaTkO5Gp8mUKgY0ERSQgJLefNy/Ef+5vz5rDpymy4fLWLbHeeUwXIpXCej5JbR5HjZ8CxN7w6Uku6NSSuWRJoICJCL0bxbBzCdaEVzam4Fj1/Dm7J3uUZ5CBO56Ce79GA4ucxSsi7M7KqVUHmgicIJa4WWY8Xgr+sVE8PmSffT67Hf2ucv6yI0HwoBpkHQEvmwP8RvtjkgpdQOaCJykpI8nb/Ssz+cPNeHImWS6j17OpDVuMueg2p0wdC54eMFXXWH3XLsjUkrlQhOBk3WuF86cp9vQuEogL07fwqgJ6zhz8YrdYTlfuTowYiGEVodJD8KaL+2OSCmVA00EhSA8wJdvhzbj711r89vOk3T+cCkr9p6yOyznKxMOg2dBjU4w688w9+9asE4pF6SJoJB4eAgj2lTlx8da4lfCiwFjVvOfWTuK/5KYJfzgwe8gZqRVp+j7QVqwTikXo4mgkNWrGMAvT7ZmQLMIvli6n56f/s7ek8X8QrKHJ3R5G+5+A3b8DN/cAxfcZGitUkWAJgIblPTx5N/31+eLgU2IP5tC94+WMWHVoeJ9IVkEWjwGfb+F41utJTATdtsdlVIKTQS26lQ3nLnPtKFpZDAv/bSVEePXkXihmM9Irn0PDP4VUpNhbEc4uMLuiJRye5oIbFbW3ype91K32izdnUDnD5exZHcxP21SqQkMXwB+ZeHb+2DzVLsjUsqtaSJwAR4ewvDWVfnp8ZYElvRm0Lg1vDxja/FeBS0oEobNg8rNYPoIWPKOFqxTyiaaCFxInQr+/PxkK4a0jGT8ykN0G72MjUfO2h2W85QMgoemw+0PwqLXYcYTWrBOKRtoInAxvt6evHJPXSYOb0ZKajq9Pvud9+fvJjW9mI6/9/KB+z+Hti/AxgkwoRekFOPkp5QL0kTgolpWD2XOM224t0EFRi/c4xhmet7usJxDBO58Ee77DA6tgHGd4exhu6NSym1oInBhASW9+aBvQz4d0Ji4M8l0G72cccsPFN+Fbxr2t04VnYu31kOO32B3REq5hTwlAhF5WkT8xTJWRNaLSCdnB6csXeuXZ+4zbbijWgiv/bKdgeNWE3+2mM7OrdrWuojsWcIqWLdrtt0RKVXs5bVHMNQYcw7oBIQBQ4A3nRaVuk5Zf1/GDW7KGz3rs+HwWe7+71J+3BBXPCehlb3NGl4aVgsm94fVX9gdkVLFWl4TgTh+dgW+MsZsyvScKiQiQr+YCGY/3Zqa5crwpymbePy79ZwujtVMy5SzJp7V7AKzn4c5L0JGMR5Oq5SN8poI1onIPKxEMFdEygC5DmMRkXEiclJEtuawvZ2IJInIRsft5ZsL3X1VCSnN1Eda8JfOtZi//QR3/3cp87efsDusgudT2ipJ0exRWPUpTH0YriTbHZVSxU5eE8Ew4AWgqTEmGfDGOj2Um6+Bzjdos8wY09Bxey2PsSjA00N4rF11fnq8JSGlfRgxPpY/TdnI2eRi1jvw8IQub0Lnt2Dnr/B1N7hw0u6olCpW8poIWgC7jDFnReQh4CUg19XJjTFLgdO3GJ+6gboVApj5RCueal+DnzfF0/GDpSwojr2D5qPgwYlwcoejYN0uuyNSqtjIayL4DEgWkQbAX4BDwPgCeP8WIrJJRGaLSN2cGonISBGJFZHYhIRiXocnH3y8PHi2Y81rvYPh42N5dspGkpKL2Szd27rBkF8h9ZJVsO7AUrsjUqpYyGsiSDPW8JQewIfGmA+BMrf43uuBKsaYBsBHwE85NTTGfGGMiTbGRIeFhd3i2xZf9So6egd3VWfGpng6frCEhTuKWe+goqNgXZny8G1P2DTZ7oiUKvLymgjOi8iLwEDgVxHxxLpOkG/GmHPGmAuO+7MAbxEJvZV9KkfvoFMtZjzekuDSPgz7JpZnpxaz3kFQFRg6F6q0gB8fgcVvasE6pW5BXhNBX+Ay1nyC40BF4J1beWMRCRcRcdyPccSSeCv7VP/vau/gybuqM2NjPJ3+u4Tfdhaj3kHJQBjwAzToD4vfgJ8ehbRidqFcqUIieZ2QJCLlgKaOh2uMMbkO3RCRSUA7IBQ4AbyCoxdhjPlcRJ4AHgXSgBTgWWPM7zeKIzo62sTGxuYpZmXZEpfEn7/fxK4T5+nVuBIv31OHgJK31KFzHcbA0ndg0b8hsjX0nWAlCaXUH4jIOmNMdLbb8pIIRKQPVg9gMdZEstbA88aYaQUYZ55oIsify2npfPzbXj5dvI+Q0j681qMeneuF2x1Wwdk0BWY8DsFVYcD31ukjpdQ1BZEINgEdr/YCRCQMWOC40FuoNBHcmq1Hk/jLtM1sP3aOLvXCebVHXcqW8bU7rIJxYBlMGQCePtB/inVhWSkF5J4I8nqNwCPLqaDEm3itciH1KgYw44mW/KVzLRbuPEmH95YwZe3h4lGzKKo1DJsP3iXhq27WBDSl1A3l9ct8jojMFZHBIjIY+BWY5bywlDN5e3rwWLvqzHm6NbeV9+evP2xhwJjVHEq8aHdoty6sFgxfCOXqwOQBsOozuyNSyuXdzMXiXkBLrGsES40xPzozsJzoqaGClZFhmLT2MG/O2klqRgbPdqzJ0JZReHkW8Q7flWRrLeSdv0CzUXD3f6xyFUq5qVu+RuBKNBE4x/GkS7z001YW7DhB/YoBvNmrPnUrBNgd1q3JSIf5L8PKj6FWV+g1xipkp5Qbyvc1AhE5LyLnsrmdF5FzzglX2SE8wJcvH27CJ/0bcywphXs/XsHbc3ZyKbUIl3728IS7/w1d34Xdc6yFbs4Xo7kUShUQ7RGo65xNvsLrv+5g2ro4okJL8+/76nFH9SI+6XvXHJg2BEqFwoCpULa23REpVagKYtSQciOBpXx4t3cDvh0WQ4Yx9B+zmj9N2cipC5ftDi3/anWGIbMg/TKMvRv2L7Y7IqVchiYClaPWNcKY+0wbnryrOr9sjueudxfz3erDZGQUrV7kNRUaWSOK/CvAhF6wYaLdESnlEjQRqFz5envyXKdazH66DXUq+PO3H7fwwOe/s+NYEb1EFFgZhs2FyFYw4zH47d9asE65PU0EKk+ql/Vj0ojmvNe7AQcTk+n+0XLemLWD5Ctpdod283wDYMA0aPQQLH3bqmCaVoRPeyl1izQRqDwTEXo1qcTCZ9vSu0kl/rd0Px3fL6Ironl6w70fw10vweYp1toGKWfsjkopW2giUDctqLQPb/a6nWmjWuBXwovh42MZOT6W+LMpdod2c0SgzfPQcwzErYExHeH0AbujUqrQaSJQ+RYdGcwvT7XihS63sXRPAu3fW8Ini/ZyOa2IzT24vTcM/AkuJsCYDhCnw5OVe9FEoG6Jt6cHo9pWY8GzbWlbM4x35u7i7g+WsmhXrstVuJ7IltYSmCX84OtusH2m3REpVWg0EagCUSmoFJ8PbML4oTF4eAhDvlrL8G9iOZyYbHdoeRdawxpeGl4fpj4Mv3+sI4qUW9BEoApUm5phzHm6DS92uY3f952iwwdLeH/+blKuFJHTRaVDYdDPUPsemPd3mPU8pBfBkVFK3QRNBKrA+Xh58Ejbavz2XDs61w1n9MI9dHh/CXO2Hi8a6x54l4Te38AdT8HaL63Fbi5fsDsqpZxGE4FymvAAX0b3a8Tkkc3xK+HFqAnreHjcGvYlFIEvVQ8P6PQv6PYe7JkHX3WBc8fsjkopp9BEoJyuedUQfn2qFa/cU4eNh89y9wdLee3n7SQlp9od2o01HQ79pkDiPmtE0YltdkekVIHTRKAKhZenB0NaRvHbn9vRO7oSX/1+gLbvLuKb3w+Smp5hd3i5q9kJhs4Gkw7jOsO+3+yOSKkCpYlAFaqwMiV4o+ft/Ppka2qH+/PKzG10+XCZ6w83Ld/AGl4aUBkm9ob139odkVIFRhOBskWdCv58N6IZXwxsQlp6BkO+WsugcWvYc+K83aHlLKASDJ0DUW1h5hOw8F86vFQVC7owjbLdlbQMxq88yIcL95B8JZ0BzSJ4pkNNgkv72B1a9tJT4ddnYf14qPcA3PcpeJWwOyqlcmXLwjQiMk5ETorI1hy2i4iMFpG9IrJZRBo7Kxbl2ny8PBjeuipLnr+T/jERTFx9mLbvLGLMsv2uWa7C0xvuGQ3tX4Gt02D8fZB82u6olMo3Z54a+hronMv2LkANx20k8JkTY1FFQHBpH/51Xz1mP92aRhFBvP7rDtq/t4SfNhx1vcVwRKD1s/DAODi6DsZ2hNP77Y5KqXxxWiIwxiwFcvszqQcw3lhWAYEiUt5Z8aiio2a5MowfGsP4oTH4+3rzzJSN3PPxcpbtSbA7tOvV6wUPz4DkRGt46ZE1dkek1E2z82JxReBIpsdxjueuIyIjRSRWRGITElzwy0A5RZuaYfzyZCs+6NuAs8mpDBy7hoFjV7P1aJLdof1RlRZWjSLfAPjmHtj2k90RKXVT7EwEks1z2fb/jTFfGGOijTHRYWFhTg5LuRIPD+H+RpVY+FxbXupWmy1Hk+j+0XKembyBI6ddqKBdSDUYtsAaZvr9IFjxoY4oUkWGnYkgDqic6XElIN6mWJSL8/X2vHZBeVTbaszeepz27y3hX79s58zFK3aHZykdAg/PhLr3w/yXrZFFWrBOFQF2JoKZwMOO0UPNgSRjjBZzUbkKKOnNC11uY9Gf29GjYQW+WnGANm8v4r8LdnP+kguUrPD2hV7joOUzEDsOJveDyy48N0IpnDiPQEQmAe2AUOAE8ArgDWCM+VxEBPgYa2RRMjDEGHPDCQI6j0Bltuv4ed6fv4u5204QWMqbR9pUY9AdVSjl42V3aBD7Ffz6HJSrA/2ngn8FuyNSbghUwq0AABc4SURBVCy3eQQ6oUwVC1viknhv/i4W70og1K8Ej99ZjX4xEfh6e9ob2J4F1jWDEv4w4HsIr2dvPMptaSJQbiP24GnenbeLVftPUz7AlyfvqkHv6Ep4e9p4FvT4FpjYxzpF1OdrqN7BvliU27JlZrFSdoiODGbyyBZ8N7wZ4QG+/O3HLbR/bwk/rIsj3a5JaeH1YcRCCIq0EsK6r+2JQ6kcaI9AFVvGGBbvSuDdebvYFn+OqqGlefzO6vRoWAEvO3oIl8/D94Nh7wJo9Se462VrARylCoGeGlJuLSPDMG/7cT5cuJcdx84REVyKx9pVo2fjSvh4FfIXcXoazH7eGlFUtyfc95k10kgpJ9NEoBRWD2HhjpN89NseNsUlUTGwJKPaVaNPdCVKeBXiRWVj4PfR1lyDys3hwe+sOQhKOZEmAqUyMcawZHcCoxfuYf3hs4T7+/JI26qFP8po248w/REIqAgDplmzk5VyEk0ESmXDGMPv+xIZvXAPqw+cJtSvBCPbRNG/WRX8ShTSPITDq61JZ8ZAv0kQ0bxw3le5HU0ESt3A6v2JfPTbXpbvPUUZXy8GNq/CkJZRhJUphAVnEvdZy18mxcH9n1kVTZUqYJoIlMqjTUfO8r+l+5i99Tjenh70alyJkW2qEhVa2rlvnHwaJveHwyuhwz+tEhWSXV1GpfJHE4FSN+nAqYt8uWw/09bFkZqeQee64YxqW40GlQOd96apl2DGY7D1B2gyGLq+B54uUCpDFQuaCJTKp5PnL/HN7wf5duUhzl1Ko0XVEB5pW5W2NcMQZ/zFnpEBi16HZe9BtfbQ+2vw9S/491FuRxOBUrfowuU0Jq85zJhlBzh+7hI1y/kxpGUU9zeq6JyRRuvHw8/PQNnaVsG6gGzXbFIqzzQRKFVArqRl8POmeMYuP8D2Y+cIKuVN/2YRDGweSXhAAU8M27sQpg6CEn5WMih/e8HuX7kVTQRKFTBjDGsOnGbcigPM234CTxG63V6eoS2jCvY6woltVn2iS2et00Q1OhbcvpVb0USglBMdTkzmm5UHmbr2COcvp9GkShBDWkbSuW54wdQ0OncMvutjJYWu70DTYbe+T+V2NBEoVQguXE5jWuwRvvr9IIcSkynnX4J+MRE82DTi1k8bXb4A04bCnrlwx1PQ4VUtWKduiiYCpQpReoZh0c6TTFh9iCW7E/AQoWPtcjzUvAp3VAvBwyOfo43S02DOX2HtGKhzH9z/OXiXLNjgVbGVWyLQQcpKFTBPD6FDnXJ0qFOOw4nJTFxziO9j45iz7ThRoaUZ0CyCB5pUIrCUz03u2Au6vgtBUTDvJTgXb5WlKB3qnA+i3Ib2CJQqBJdS05m99RgTVh1m3aEzlPDyoPvtFejfLILGEYE3Pydh+wyYPhLKlLcK1oVWd07gqtjQU0NKuZAdx84xYdUhftpwlItX0qle1o++0ZW5v3FFQv1uorbRkbUw6UEw6VYp6yp3OC9oVeRpIlDKBV24nMYvm+KZGnuE9YfP4uUhtK9dlr5NK9OmRljeRhydPmAVrDt7yFrkpv4Dzg9cFUmaCJRycXtOnGdq7BGmrz9K4sUrlPMvQa/GlegTXZnIGxW8Sz4NUx6CQyvgrn9A6+e0YJ26jiYCpYqIK2kZ/LbzJFNjj7B410kyDMREBnNfo4p0q1+egFLe2b8w7TLMeAK2TIVGA6H7B+CZQ1vlljQRKFUEnTh3iWnr4pi+Po59CRfx8fTgrtvKcl+jitx5W9j1y2saA4v+A0vfhqp3Qp9vwDfAnuCVy7EtEYhIZ+BDwBMYY4x5M8v2dsAM4IDjqenGmNdy26cmAuVujDFsPXqOHzccZeameE5duIy/rxfdbq/A/Y0qEl0l6I9zEzZMgJ+fhtCaVo2iwMr2Ba9chi2JQEQ8gd1ARyAOWAv0M8Zsz9SmHfBnY0z3vO5XE4FyZ2npGazYl8hPG44yZ+txUlLTqRhYkh4NK9Dt9vLUKe9vDUXdvximDATvUtB/ClRoaHfoymZ2JYIWwD+NMXc7Hr8IYIx5I1ObdmgiUCpfLl5OY/72E0zfcJQVe0+RnmGICi1N1/rhdKtfgdqeR5Dv+loXkx8YB7U62x2yspFdieABoLMxZrjj8UCgmTHmiUxt2gE/YPUY4rGSwrZs9jUSGAkQERHR5NChQ06JWamiKvHCZeZuO8GsLcdYuT/xWlLoU8ubwYf+im/iNqTL2xAzwu5QlU3sSgS9gbuzJIIYY8yTmdr4AxnGmAsi0hX40BhTI7f9ao9AqdxdTQq/boln5b5ESphLjC39GXekr+VU/ZGE3P8m4uGExXSUS7Or1lAckPkqVSWsv/qvMcacy3R/loh8KiKhxphTToxLqWItxK8E/ZtF0L9ZBKcuXGbutuN8uvlVdh/6kMFbvmDRts2sqP9v2tWLpFnVYLwLolS2KtKc2SPwwrpY3B44inWxuH/mUz8iEg6cMMYYEYkBpgFVTC5BaY9AqfxJvHCZw7Peo8H2t9lsqjHs8nNc8Q3hzlpl6VinHO1qhVHGV+ceFFe29AiMMWki8gQwF2v46DhjzDYRGeXY/jnwAPCoiKQBKcCDuSUBpVT+hfiVIKTP32DH7TT4YTjL/P7NJxXeYPLeU8zcFI+3p9C8aoiVFGqWJSKklN0hq0KiE8qUckdx62BSX0hPJb3PBNZ71GX+9hPM336CA6cuAlA1tDRta4XRrlZZmkUF4+ut1xWKMp1ZrJS63pmD1nrIp/dDj0+gQV8ADpy6yOJdJ1m8K4FV+xO5nJaBr7cHLaqG0K5WWdrVCqNKyA3qHymXo4lAKZW9lDPWxLODy+DOv0Ob5/9QsO5Sajor9yeyZFcCi3ed5GBiMgBRoaVpWT2EltVCaVEt5OYX2VGFThOBUipnaVdg5pOweTI0HADd/wte2X+xH3T0FpbsTmDNgdNcvJKOCNSt4E/LaqHcUT2UppFBlPLRxQ9djSYCpVTujIElb8HiNyCqLfQZDyUDc31JanoGm+POsmJvIiv2nmLD4bNcSc/A21NoFBHEHdVCaFk9lNsrBVxfIE8VOk0ESqm82fgdzHwKQqrDgKkQGJHnl6ZcSWftwdOs2HeK3/cmsjU+CWPAx8uDhpUCaRoVRNPIYJpUCdJhqjbQRKCUyrsDS2HyQ+DtC/0mQ8XG+dpNUnIqqw8ksvbgadYcPMPWo0mkZxg8BGqX96dpZDAxUcE0jQwmrMxNLNGp8kUTgVLq5pzcaS2BmXzKUbCuyy3vMvlKGhsOn2X1gdOsPXCaDUfOcCk1A4DIkFI0igiiUUQgDSsHclu4Pz5eOuO5IGkiUErdvPMnYNKDcGwjdH4Tmj1SoLu/kpbB1vgk1h44TeyhM2w8cpaE85cB63RSvQr+NIoIomFlKzlUCippldhW+aKJQCmVP1eSYfoI2PkLNH8MOr0OTipYZ4zh6NkUNh45y8bDZ9l45CxbjiZxOc3qNYT6laBh5UBurxRAvYr+1K0QQNkyJTQ55JEmAqVU/mWkw7yXYNWncFt36Pkl+BRO+YnU9Ax2HjvPxiNn2HDESg4HTl3k6tdWqF8JR1Lwp16FAOpVDNCeQw40ESilbt3q/8GcF6B8Q2vVM7+ytoRx4XIaO46dY+vRJLbFWz/3nLxAeob1Xebv60XdCgHUreBPrfAy3BbuT/WyfpT0ce8hrJoIlFIFY+cs+GEYlA6FAdMgrJbdEQHWDOhdx89biSE+iW1Hk9hx/DxXHKeVRKBKcClqhZehVrky1Ar3p1a4H5EhpfFykzLcmgiUUgXn6Hr4ri+kX4a+EyCqjd0RZSstPYNDp5PZffw8O4+fZ/eJ8+w6cZ6Dpy7i6Dzg4+lBtbJ+1CrnR7UwP6qG+VGtbGkiQ0oXuyJ7mgiUUgXr7GGrYF3iXrj3I2jYz+6I8uxSajp7T15gV6bksPv4eeKTLl1rIwKVgkpSNfRqgihN1bDSVA/zI6yIXqC2a4UypVRxFRgBQ+fA1IHw0yg4ewja/vUPBetcla+3J/UqWheWM0u+ksaBUxfZl3CR/QkXrv1cc+A0Kanp19qVKeFFldBSVAkuTURIKSKCS1EluBSVg0tRIbAknh6ufwyy0h6BUir/0q7AL8/AxonQoB/cMzrHgnVFVUaG4fi5S+xPuMi+hAvsS7jAocRkDp9OJu5MMqnp//8d6u0pVAqykkKVYCtJRISUonJQKSoGlsS/pJdtvQntESilnMPLx1rLICgKFr0OSXHQ91soGWR3ZAXGw0OoEFiSCoElaVUj9A/b0jMMx5JSOOxIDIdOJ1+7v/HwGc5dSvtDe78SXlQI9L22v4qBJa3HAdbj8ABfW9aQ1h6BUqpgbJoCMx6H4Kow4HsIqmJ3RLZLSk7l0OmLHDmdwrGkFI6eTeHomRTik1KIP3uJ0xev/KG9h0A5f99rSSHc33ELsG6RIaXzXZdJLxYrpQrHweUweQB4ekO/KVCpid0RubSUK+mOpGDdjp5J4ejZSxw9m8yJc5c5nnTpD9cnHmlTlRe71s7Xe+mpIaVU4YhsBcPmw8QH4Otu0GsM1O5ud1Quq6SPJ9XCrJFJ2THGcC4ljePnLnH83CXKB/g6JQ73mEmhlCo8YTVh+EIoVxemPASrPrM7oiJLRAgo5U2t8DK0rRlGzXJlnPI+mgiUUgXPLwwG/Wz1Bua8ALP+YtUsUi5JE4FSyjl8SkHv8dDiCVjzP+vawZWLdkelsqGJQCnlPB4ecPe/oeu7sGcufNXVWudAuRSnJgIR6Swiu0Rkr4i8kM12EZHRju2bRSR/a+IppVxbzAh4cBKc2gNj2sPJHXZHpDJxWiIQEU/gE6ALUAfoJyJ1sjTrAtRw3EYCelVJqeKqVmcYMgvSU2FsJ9i/2O6IlIMzh4/GAHuNMfsBRGQy0APYnqlND2C8sSYzrBKRQBEpb4w55sS4lFJ2qdAQhi+A7/rA+B4QWhNEz1DnWaOBcMcTBb5bZyaCisCRTI/jgGZ5aFMR+EMiEJGRWD0GIiIiCjxQpVQhCqxsFaxb+o5VxVTlnZMWA3JmIsiuslLWacx5aYMx5gvgC7BmFt96aEopW/kGWOsfK5fgzD5ZHFA50+NKQHw+2iillHIiZyaCtUANEYkSER/gQWBmljYzgYcdo4eaA0l6fUAppQqX004NGWPSROQJYC7gCYwzxmwTkVGO7Z8Ds4CuwF4gGRjirHiUUkplz6lF54wxs7C+7DM/93mm+wZ43JkxKKWUyp2O21JKKTeniUAppdycJgKllHJzmgiUUsrNFbmlKkUkATiUz5eHAqcKMJyC4qpxgevGpnHdHI3r5hTHuKoYY8Ky21DkEsGtEJHYnNbstJOrxgWuG5vGdXM0rpvjbnHpqSGllHJzmgiUUsrNuVsi+MLuAHLgqnGB68amcd0cjevmuFVcbnWNQCml1PXcrUeglFIqC00ESinl5op1IhCRd0Rkp4hsFpEfRSQwh3adRWSXiOwVkRcKIa7eIrJNRDJEJMehYCJyUES2iMhGEYl1obgK+3gFi8h8Ednj+BmUQ7tCOV43+vyOsuqjHds3i0hjZ8Vyk3G1E5Ekx/HZKCIvF1Jc40TkpIhszWG7XcfrRnHZdbwqi8giEdnh+P/4dDZtCvaYGWOK7Q3oBHg57r8FvJVNG09gH1AV8AE2AXWcHFdtoBawGIjOpd1BILQQj9cN47LpeL0NvOC4/0J2/46Fdbzy8vmxSqvPxlqBrzmwuhD+7fISVzvgl8L6fcr0vm2AxsDWHLYX+vHKY1x2Ha/yQGPH/TLAbmf/jhXrHoExZp4xJs3xcBXWCmhZxQB7jTH7jTFXgMlADyfHtcMYs8uZ75EfeYyr0I+XY//fOO5/A9zn5PfLTV4+fw9gvLGsAgJFpLwLxGULY8xS4HQuTew4XnmJyxbGmGPGmPWO++eBHVhruWdWoMesWCeCLIZiZdCsKgJHMj2O4/qDbhcDzBORdSIy0u5gHOw4XuWMY+U6x8+cVvAujOOVl89vxzHK63u2EJFNIjJbROo6Oaa8cuX/g7YeLxGJBBoBq7NsKtBj5tSFaQqDiCwAwrPZ9HdjzAxHm78DacDE7HaRzXO3PKY2L3HlQUtjTLyIlAXmi8hOx18xdsZV6MfrJnZT4McrG3n5/E45RjeQl/dcj1Vv5oKIdAV+Amo4Oa68sON45YWtx0tE/IAfgGeMMeeybs7mJfk+ZkU+ERhjOuS2XUQGAd2B9sZxci2LOKBypseVgHhnx5XHfcQ7fp4UkR+xuv+39MVWAHEV+vESkRMiUt4Yc8zR/T2Zwz4K/HhlIy+f3ynH6FbjyvxlYoyZJSKfikioMcbu4mp2HK8bsvN4iYg3VhKYaIyZnk2TAj1mxfrUkIh0Bv4K3GuMSc6h2VqghohEiYgP8CAws7BizImIlBaRMlfvY134znZ0QyGz43jNBAY57g8Cruu5FOLxysvnnwk87BjZ0RxIunpqy4luGJeIhIuIOO7HYP3/T3RyXHlhx/G6IbuOl+M9xwI7jDHv59CsYI9ZYV8RL8wbsBfrPNpGx+1zx/MVgFmZ2nXFujK/D+sUibPjuh8ro18GTgBzs8aFNfpjk+O2zVXisul4hQALgT2On8F2Hq/sPj8wChjluC/AJ47tW8hlZFghx/WE49hswho8cUchxTUJOAakOn6/hrnI8bpRXHYdr1ZYp3k2Z/ru6urMY6YlJpRSys0V61NDSimlbkwTgVJKuTlNBEop5eY0ESillJvTRKCUUm5OE4FSt0BEBovIx7faJo/vFSgij93iPu4TkTq3GosqXjQRqCLBMXHG3X9fA4FbSgRYBfs0Eag/cPf/WMqFiUikoyb7p1h1XyqLyGciEuuo0/6qo12MiEx33O8hIiki4iMiviKyP5v9fu3YzyIR2S8ibcWqTb9DRL7O1K6fWOsbbBWRtzI9P0REdovIEqBlpufDROQHEVnruLUkF2Kts/CTWPXkV4nI7Y7n/ykif87Ubquj+NibQDWxauO/I1a9/KVirbWxXUQ+v5osReRCptc/4PjMdwD3Au849lEt7/8aqjgr8rWGVLFXCxhijHkMrAKCxpjTIuIJLHR8ea7HqtAI0BqrtERTrN/vrFUbrwoC7sL6YvwZ6wt9OLBWRBpi1TN6C2gCnMGqanqfY3+vOp5PAhYBGxz7/BD4wBizXEQigLlYazzk5FVggzHmPhG5CxgPNMyl/QtAPWNMQ8exaIdVT6kOcAiYA/QEpmX3YmPM7yIyE6vGfrZtlHvSRKBc3SFj1Vu/qo9YJaa9sBbwqGOM2SzWSk21sb4Y38dadMQTWJbDfn82xhgR2QKcMMZsARCRbUAkUAVYbIxJcDw/0bFPsjw/BajpeL4DUMdRngbA/2r9oxy0AnoBGGN+E5EQEQm48SH5gzXGmP2OWCY59qlf8uqmaCJQru7i1TsiEgX8GWhqjDnjOI3j69i8DOiCVTdmAfA1ViL4M9m77PiZken+1cdeWGXLc5JTXRYPoIUxJiXzk5kSQ1Y5lRJO44+nbX2zaZdTLCab53N7vVJ6jUAVKf5YiSFJRMphffFftRR4Bljp+Gs9BLgNq2hYfqwG2opIqOM0VD9gieP5do6/3r2B3pleMw+rUBkAjlNMuVkKDHC0bQecMlbp44NYSygi1lq0UY7257GWLswsxlFx1APoCyx3PH9CRGo7nr8/U/vs9qHcnCYCVWQYYzZhnY/fBowDVmTavBoox/+vP7AZ2GzyWVXRWCV9X8S6BrAJWG+MmeF4/p/ASqyex/pML3sKiHZc/N2OVS0yN/+82h7rQvDVUts/AMEishF4FKuiKMaYRGCF4+LxO462Kx2v3QocAH50PP8C8AvwG1aFzasmA8+LyAa9WKyu0uqjShVRjl7En40x3e2ORRVt2iNQSik3pz0CpZRyc9ojUEopN6eJQCml3JwmAqWUcnOaCJRSys1pIlBKKTf3fy+a8H1H1YIaAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Comparing the logistic and hinge losses\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.xlabel(\"raw model output\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Implementing logistic regression\n",
    "\n",
    "#This is very similar to the earlier exercise where you implemented linear\n",
    "#regression \"from scratch\" using scipy.optimize.minimize. However, this\n",
    "#time we'll minimize the logistic loss and compare with scikit-learn's\n",
    "#LogisticRegression (we've set C to a large value to disable regularization;\n",
    "#more on this in Chapter 3!).\n",
    "\n",
    "#The log_loss() function from the previous exercise is already defined in\n",
    "#your environment, and the sklearn breast cancer prediction dataset (first\n",
    "#10 features, standardized) is loaded into the variables X and y.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Mathematical function for logistic loss\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "\n",
    "# The logistic loss, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        raw_model_output = w@X[i]\n",
    "        s = s + log_loss(raw_model_output * y[i])\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "#w_fit = minimize(my_loss, X[0]).x\n",
    "#print(w_fit)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [ 1.03592182 -1.65378492  4.08331342 -9.40923002 -1.06786489  0.07892114\n",
    "#     -0.85110344 -2.44103305 -0.45285671  0.43353448]\n",
    "#    [[ 1.03731085 -1.65339037  4.08143924 -9.40788356 -1.06757746  0.07895582\n",
    "#      -0.85072003 -2.44079089 -0.45271     0.43334997]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Logistic regression and regularization**\n",
    "___\n",
    "- **Regularization** combats overfitting by making model coefficients smaller\n",
    "    - hyperparameter \"C\" is inverse to regularization\n",
    "        - higher \"C\" means less regularization\n",
    "        - smaller \"C\" means more regularization\n",
    "- How does regularization affect training accuracy?\n",
    "    - regularized loss = original loss + large coefficient penalty\n",
    "    - regularization is an extra term added to original loss function\n",
    "    - more regularization: *lower training accuracy*\n",
    "- How does regularization affect test accuracy?\n",
    "    - regularized loss = original loss + large coefficient penalty\n",
    "    - regularization assists in keeping features that overfit the test data\n",
    "    - more regularization: almost always *higher test accuracy*\n",
    "- Lasso\n",
    "    - L1 Regularization\n",
    "    - also performs feature selection\n",
    "- Ridge\n",
    "    - L2 Regularization\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Regularized logistic regression\n",
    "\n",
    "#In Chapter 1, you used logistic regression on the handwritten digits\n",
    "#data set. Here, we'll explore the effect of L2 regularization.\n",
    "\n",
    "#As you can see, too much regularization (small C) doesn't work well -\n",
    "#due to underfitting - and too little regularization (large C) doesn't\n",
    "#work well either - due to overfitting.\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(digits.data, digits.target, random_state=42)\n",
    "\n",
    "# Train and validaton errors initialized as empty list\n",
    "train_errs = list()\n",
    "valid_errs = list()\n",
    "C_values=[0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Loop over values of C_value\n",
    "for C_value in C_values:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C=C_value, max_iter=2750)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
    "\n",
    "# Plot results\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "#Logistic regression and feature selection\n",
    "\n",
    "#In this exercise we'll perform feature selection on the movie review\n",
    "#sentiment data set using L1 regularization. The features and targets are\n",
    "#already loaded for you in X_train and y_train.\n",
    "\n",
    "#We'll search for the best value of C using scikit-learn's GridSearchCV(),\n",
    "#which was covered in the prerequisite course.\n",
    "\n",
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(penalty=\"l1\")\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "#searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "#searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "#print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "#best_lr = searcher.best_estimator_\n",
    "#coefs = best_lr.coef_\n",
    "#print(\"Total number of features:\", coefs.size)\n",
    "#print(\"Number of selected features:\", np.count_nonzero(coefs))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Best CV params {'C': 1}\n",
    "#    Total number of features: 2500\n",
    "#    Number of selected features: 1220"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Identifying the most positive and negative words\n",
    "\n",
    "#In this exercise we'll try to interpret the coefficients of a logistic\n",
    "#regression fit on the movie review sentiment dataset. The model object\n",
    "#is already instantiated and fit for you in the variable lr.\n",
    "\n",
    "#In addition, the words corresponding to the different features are loaded\n",
    "#into the variable vocab. For example, since vocab[100] is \"think\", that\n",
    "#means feature 100 corresponds to the number of times the word \"think\"\n",
    "#appeared in that movie review.\n",
    "\n",
    "# Get the indices of the sorted cofficients\n",
    "#inds_ascending = np.argsort(lr.coef_.flatten())\n",
    "#inds_descending = inds_ascending[::-1]\n",
    "\n",
    "# Print the most positive words\n",
    "#print(\"Most positive words: \", end=\"\")\n",
    "#for i in range(5):\n",
    "#    print(vocab[inds_descending[i]], end=\", \")\n",
    "#print(\"\\n\")\n",
    "\n",
    "# Print most negative words\n",
    "#print(\"Most negative words: \", end=\"\")\n",
    "#for i in range(5):\n",
    "#    print(vocab[inds_ascending[i]], end=\", \")\n",
    "#print(\"\\n\")\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Most positive words: favorite, superb, noir, knowing, loved,\n",
    "#    Most negative words: disappointing, waste, worst, boring, lame,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Logistic regression and probabilities**\n",
    "___\n",
    "- includes decision boundary, but includes gradients indicating probabilities/confidence\n",
    "- smaller coefficients (i.e. regularization) also means less confident predictions\n",
    "- there is a relationship between overconfidence & overfitting\n",
    "    - **ratio of coefficients** gives us the slope of the line\n",
    "    - **magnitude of coefficients** gives us confidence level\n",
    "- how are probabilities computed?\n",
    "    - logistic regression: sign of \"squashed\" raw model output using sigmoid function\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum predicted probability for C = 1.0 0.9973143426717812\n",
      "Maximum predicted probability for C = 0.1 0.9352061679007129\n"
     ]
    }
   ],
   "source": [
    "#Regularization and probabilities\n",
    "\n",
    "#In this exercise, you will observe the effects of changing the\n",
    "#regularization strength on the predicted probabilities.\n",
    "\n",
    "#smaller values of C lead to less confident predictions. That's because\n",
    "#smaller C means more regularization, which in turn means smaller\n",
    "#coefficients, which means raw model outputs closer to zero and, thus,\n",
    "#probabilities closer to 0.5 after the raw model output is squashed\n",
    "#through the sigmoid function. That's quite a chain of events!\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "X=[ [ 1.78862847,  0.43650985],\n",
    "    [ 0.09649747, -1.8634927 ],\n",
    "    [-0.2773882 , -0.35475898],\n",
    "    [-3.08274148,  2.37299932],\n",
    "    [-3.04381817,  2.52278197],\n",
    "    [-1.31386475,  0.88462238],\n",
    "    [-2.11868196,  4.70957306],\n",
    "    [-2.94996636,  2.59532259],\n",
    "    [-3.54535995,  1.45352268],\n",
    "    [ 0.98236743, -1.10106763],\n",
    "    [-1.18504653, -0.2056499 ],\n",
    "    [-1.51385164,  3.23671627],\n",
    "    [-4.02378514,  2.2870068 ],\n",
    "    [ 0.62524497, -0.16051336],\n",
    "    [-3.76883635,  2.76996928],\n",
    "    [ 0.74505627,  1.97611078],\n",
    "    [-1.24412333, -0.62641691],\n",
    "    [-0.80376609, -2.41908317],\n",
    "    [-0.92379202, -1.02387576],\n",
    "    [ 1.12397796, -0.13191423]]\n",
    "\n",
    "y=[-1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1, -1]\n",
    "\n",
    "# Set the regularization strength\n",
    "model1 = LogisticRegression(C=1)\n",
    "\n",
    "# Fit and plot\n",
    "model1.fit(X,y)\n",
    "#plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob1 = model1.predict_proba(X)\n",
    "print(\"Maximum predicted probability for C = 1.0\", np.max(prob1))\n",
    "\n",
    "# Set the regularization strength\n",
    "model2 = LogisticRegression(C=0.1)\n",
    "\n",
    "# Fit and plot\n",
    "model2.fit(X,y)\n",
    "#plot_classifier(X,y,model2,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model2.predict_proba(X)\n",
    "print(\"Maximum predicted probability for C = 0.1\", np.max(prob))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "C=1.0\n",
    "![_images/9.4.svg](_images/9.4.svg)\n",
    "\n",
    "C=0.1\n",
    "![_images/9.5.svg](_images/9.5.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing easy and difficult examples\n",
    "\n",
    "#In this exercise, you'll visualize the examples that the logistic\n",
    "#regression model is most and least confident about by looking at the\n",
    "#largest and smallest predicted probabilities.\n",
    "\n",
    "#The handwritten digits dataset is already loaded into the variables X\n",
    "#and y. The show_digit function takes in an integer index and plots the\n",
    "#corresponding image, with some extra information displayed above the\n",
    "#image.\n",
    "\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(X,y)\n",
    "\n",
    "# Get predicted probabilities\n",
    "#proba = lr.predict_proba(X)\n",
    "\n",
    "# Sort the example indices by their maximum probability\n",
    "#proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# Show the most confident (least ambiguous) digit\n",
    "#show_digit(proba_inds[-1], lr)\n",
    "\n",
    "# Show the least confident (most ambiguous) digit\n",
    "#show_digit(proba_inds[0], lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most Confident\n",
    "![_images/9.6.svg](_images/9.6.svg)\n",
    "\n",
    "Least Confident\n",
    "![_images/9.7.svg](_images/9.7.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Multi-class logistic regression**\n",
    "___\n",
    "- one-vs-rest (default)\n",
    "    - e.g.\n",
    "        - lr0.fit(X, y==0)\n",
    "        - lr0.fit(X, y==1)\n",
    "        - lr0.fit(X, y==2)\n",
    "    - raw model output\n",
    "        - lrn.decision_function(X)[0]\n",
    "    - or simply,\n",
    "        - lr.fit(X,y)\n",
    "        - lr.predict(X)[0]\n",
    "- multinomial, softmax, cross-entropy loss\n",
    "    - fits a single classifier for all classes\n",
    "    - prediction directly outputs best class\n",
    "    - complicated, but direct\n",
    "    - not as common for SVMs\n",
    "- one coefficient/intercept per feature per class\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVR training accuracy    : 1.0\n",
      "OVR test accuracy        : 0.9911111111111112\n",
      "Softmax training accuracy: 1.0\n",
      "Softmax test accuracy    : 0.9911111111111112\n"
     ]
    }
   ],
   "source": [
    "#Fitting multi-class logistic regression\n",
    "\n",
    "#In this exercise, you'll fit the two types of multi-class logistic\n",
    "#regression, one-vs-rest and softmax/multinomial, on the handwritten\n",
    "#digits data set and compare the results.\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(digits.data, digits.target, random_state=42)\n",
    "\n",
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression(max_iter=2750)\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy    :\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy        :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=2750)\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualizing multi-class logistic regression\n",
    "\n",
    "#In this exercise we'll continue with the two types of multi-class\n",
    "#logistic regression, but on a toy 2D data set specifically designed to\n",
    "#break the one-vs-rest scheme.\n",
    "\n",
    "#The data set is loaded into X_train and y_train. The two logistic\n",
    "#regression objects,lr_mn and lr_ovr, are already instantiated (with\n",
    "#C=100), fit, and plotted.\n",
    "\n",
    "#Notice that lr_ovr never predicts the dark blue class... yikes! Let's\n",
    "#explore why this happens by plotting one of the binary classifiers\n",
    "#that it's using behind the scenes.\n",
    "\n",
    "# Print training accuracies\n",
    "#print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "#print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# Create the binary classifier (class 1 vs. rest)\n",
    "#lr_class_1 = LogisticRegression(C=100)\n",
    "#lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# Plot the binary classifier (class 1 vs. rest)\n",
    "#plot_classifier(X_train, y_train==1, lr_class_1)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Softmax     training accuracy: 0.996\n",
    "#    One-vs-rest training accuracy: 0.916"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "lr_mn\n",
    "![_images/9.8.svg](_images/9.8.svg)\n",
    "\n",
    "lr_ovr\n",
    "![_images/9.9.svg](_images/9.9.svg)\n",
    "\n",
    "class 1 vs. rest\n",
    "![_images/9.10.svg](_images/9.10.svg)\n",
    "\n",
    "the binary classifier incorrectly labels almost all points in class 1 (shown as red triangles in the final plot)! Thus, this classifier is not a very effective component of the one-vs-rest classifier. In general, though, one-vs-rest often works well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#One-vs-rest SVM\n",
    "\n",
    "#As motivation for the next and final chapter on support vector\n",
    "#machines, we'll repeat the previous exercise with a non-linear SVM.\n",
    "#Once again, the data is loaded into X_train, y_train, X_test, and y_test .\n",
    "\n",
    "#Instead of using LinearSVC, we'll now use scikit-learn's SVC object,\n",
    "#which is a non-linear \"kernel\" SVM (much more on what this means in\n",
    "#Chapter 4!). Again, your task is to create a plot of the binary\n",
    "#classifier for class 1 vs. rest.\n",
    "\n",
    "# We'll use SVC instead of LinearSVC from now on\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create/plot the binary classifier (class 1 vs. rest)\n",
    "svm_class_1 = SVC()\n",
    "#svm_class_1.fit(X_train, y_train==1)\n",
    "#plot_classifier(X_train, y_train==1, svm_class_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "non-linear \"kernel\" SVM\n",
    "![_images/9.11.svg](_images/9.11.svg)\n",
    "\n",
    "class 1 vs. rest\n",
    "![_images/9.12.svg](_images/9.12.svg)\n",
    "\n",
    "The non-linear SVM works fine with one-vs-rest on this dataset because it learns to \"surround\" class 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Support vectors**\n",
    "___\n",
    "- What is an SVM?\n",
    "    - linear classifiers\n",
    "    - trained using the hinge loss and L2 regularization\n",
    "    - e.g., a training example **not** in the flat part of the loss diagram\n",
    "    - e.g., an example that is incorrectly classified **or** close to the boundary\n",
    "- if an example is not a support vector, removing it has no effect on the model\n",
    "- having a small number of support vectors makes kernel SVM's really fast\n",
    "- perpendicular lines from support vectors to the boundary line (margins) are maximized to be equal by SVM algorithms, and are mathematically equivalent to the hinge loss function. This is not necessarily relevant when the boundary is non-linear.\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Effect of removing examples\n",
    "\n",
    "#Support vectors are defined as training examples that influence the\n",
    "#decision boundary. In this exercise, you'll observe this behavior by\n",
    "#removing non support vectors from the training set.\n",
    "\n",
    "#The wine quality dataset is already loaded into X and y (first two\n",
    "#features only). (Note: we specify lims in plot_classifier() so that\n",
    "#the two plots are forced to use the same axis limits and can be\n",
    "#compared directly.)\n",
    "\n",
    "# Train a linear SVM\n",
    "#svm = SVC(kernel=\"linear\")\n",
    "#svm.fit(X,y)\n",
    "#plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# Make a new data set keeping only the support vectors\n",
    "#print(\"Number of original examples\", len(X))\n",
    "#print(\"Number of support vectors\", len(svm.support_))\n",
    "#X_small = X[svm.support_]\n",
    "#y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "#svm_small = SVC(kernel=\"linear\")\n",
    "#svm_small.fit(X_small, y_small)\n",
    "#plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Number of original examples 178\n",
    "#    Number of support vectors 81"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVM\n",
    "![_images/9.13.svg](_images/9.13.svg)\n",
    "\n",
    "SVM with only support vectors\n",
    "![_images/9.14.svg](_images/9.14.svg)\n",
    "\n",
    "Compare the decision boundaries of the two trained models: are they the same? By the definition of support vectors, they should be!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Kernel SVMs**\n",
    "___\n",
    "- $$\\text{transformed feature = (original feature)}^{2}$$\n",
    "- non-linearly separable categories can be transformed to be linearlly separable\n",
    "    - works only when one category is clustered around one point/area\n",
    "    - fitting a linear model in transformed space corresponds to fitting a non-linear model in the original space\n",
    "- hyperparameter gamma (lower is smoother) controls smoothness of boundary\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "#GridSearchCV warm-up\n",
    "\n",
    "#increasing the RBF kernel hyperparameter gamma increases training\n",
    "#accuracy. In this exercise we'll search for the gamma that maximizes\n",
    "#cross-validation accuracy using scikit-learn's GridSearchCV. A binary\n",
    "#version of the handwritten digits dataset, in which you're just trying\n",
    "#to predict whether or not an image is a \"2\", is already loaded into\n",
    "#the variables X and y\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "#searcher.fit(X,y)\n",
    "\n",
    "# Report the best parameters\n",
    "#print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Best CV params {'gamma': 0.001}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Jointly tuning gamma and C with GridSearchCV\n",
    "\n",
    "#In the previous exercise the best value of gamma was 0.001 using the\n",
    "#default value of C, which is 1. In this exercise you'll search for the\n",
    "#best combination of C and gamma using GridSearchCV.\n",
    "\n",
    "#As in the previous exercise, the 2-vs-not-2 digits dataset is already\n",
    "#loaded, but this time it's split into the variables X_train, y_train,\n",
    "#X_test, and y_test. Even though cross-validation already splits the\n",
    "#training set into parts, it's often a good idea to hold out a separate\n",
    "#test set to make sure the cross-validation results are sensible.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Best CV params {'C': 10, 'gamma': 0.0001}\n",
    "#    Best CV accuracy 0.9988864142538976\n",
    "#    Test accuracy of best grid search hypers: 0.9988876529477196"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comparing logistic regression and SVM (and beyond)**\n",
    "___\n",
    "\n",
    "<table border=\"0\">\n",
    " <tr>\n",
    "    <td><b style=\"font-size:30px\">Logistic regression</b></td>\n",
    "    <td><b style=\"font-size:30px\">Support vector machine</b></td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td><ul>\n",
    "        <li>is a linear classifier</li>\n",
    "        <li>Can use with kernels, but slow</li>\n",
    "        <li>Outputs meaningful probabilities</li>\n",
    "        <li>Can be extended to multiclass</li>\n",
    "        <li>All data points affect fit</li>\n",
    "        <li>L2 or L1 regularization</li>\n",
    "    </ul></td>\n",
    "    <td><ul>\n",
    "        <li>is a linear classifier</li>\n",
    "        <li>Can use with kernels, and fast</li>\n",
    "        <li>Does not naturally output probabilities</li>\n",
    "        <li>Can be extended to multiclass</li>\n",
    "        <li>Only \"support vectors\" affect fit</li>\n",
    "        <li>Conventionally just L2 regularization</li>\n",
    "    </ul></td>\n",
    " </tr>\n",
    "</table>\n",
    "\n",
    "Use in sklearn:\n",
    "- logistic regression\n",
    "    - linear_model.LogisticRegression\n",
    "    - Key hyperparameters\n",
    "        - C (inverse regularization strength)\n",
    "        - penalty (type of regularization)\n",
    "        - multi_class (type of multi-class)\n",
    "- SVM\n",
    "    - svm.LinearSVC and svm.SVC\n",
    "    - Key hyperparameters\n",
    "        - C (inverse regularization strength)\n",
    "        - kernel (type of kernel)\n",
    "        - gamma (inverse RBF smoothness)\n",
    "- Stochastic gradient descent\n",
    "    - linear_model.SGDClassifier\n",
    "    - scales well to large datasets\n",
    "    - logistic -> SGDClassifier(loss='log')\n",
    "    - SVM ----> SGDClassifier(loss='hinge')\n",
    "    - Key hyperparameters\n",
    "        - alpha is like 1/C\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using SGDClassifier\n",
    "\n",
    "#In this final coding exercise, you'll do a hyperparameter search over\n",
    "#the regularization type, regularization strength, and the loss (logistic\n",
    "#regression vs. linear SVM) using SGDClassifier().\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# We set random_state=0 for reproducibility\n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "             'loss':['hinge', 'log'], 'penalty':['l1','l2']}\n",
    "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
    "#searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "#print(\"Best CV params\", searcher.best_params_)\n",
    "#print(\"Best CV accuracy\", searcher.best_score_)\n",
    "#print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Best CV params {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1'}\n",
    "#    Best CV accuracy 0.94351630867144\n",
    "#    Test accuracy of best grid search hypers: 0.9592592592592593"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}