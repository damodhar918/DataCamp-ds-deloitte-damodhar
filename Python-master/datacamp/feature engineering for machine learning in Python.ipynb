{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Why generate features?**\n",
    "___\n",
    "- Different types of data\n",
    "    - continuous: integers or floats\n",
    "    - categorical: one of a limited set of values\n",
    "    - ordinal: ranked values\n",
    "    - boolean: true/false values\n",
    "    - datetime: dates and times\n",
    "- course structure\n",
    "    - chapter 1: feature creation and extraction\n",
    "    - chapter 2: engineering messy data\n",
    "    - chapter 3: feature normalization\n",
    "    - chapter 4: working with text features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Getting to know your data\n",
    "\n",
    "#Pandas is one the most popular packages used to work with tabular\n",
    "#data in Python. It is generally imported using the alias pd and can\n",
    "#be used to load a CSV (or other delimited files) using read_csv().\n",
    "\n",
    "#You will be working with a modified subset of the Stackoverflow\n",
    "#survey response data in the first three chapters of this course.\n",
    "#This data set records the details, and preferences of thousands of\n",
    "#users of the StackOverflow website.\n",
    "\n",
    "# Import pandas\n",
    "#import pandas as pd\n",
    "\n",
    "# Import so_survey_csv into so_survey_df\n",
    "#so_survey_df = pd.read_csv(so_survey_csv)\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "#print(so_survey_df.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#          SurveyDate                                    FormalEducation  ConvertedSalary Hobby       Country  ...     VersionControl Age  Years Experience  Gender   RawSalary\n",
    "#    0  2/28/18 20:20           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN   Yes  South Africa  ...                Git  21                13    Male         NaN\n",
    "#    1  6/28/18 13:26           Bachelor's degree (BA. BS. B.Eng.. etc.)          70841.0   Yes       Sweeden  ...     Git;Subversion  38                 9    Male   70,841.00\n",
    "#    2    6/6/18 3:37           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN    No       Sweeden  ...                Git  45                11     NaN         NaN\n",
    "#    3    5/9/18 1:06  Some college/university study without earning ...          21426.0   Yes       Sweeden  ...  Zip file back-ups  46                12    Male   21,426.00\n",
    "#    4  4/12/18 22:41           Bachelor's degree (BA. BS. B.Eng.. etc.)          41671.0   Yes            UK  ...                Git  39                 7    Male  Â£41,671.00\n",
    "#\n",
    "#    [5 rows x 11 columns]\n",
    "#################################################\n",
    "\n",
    "# Print the data type of each column\n",
    "#print(so_survey_df.dtypes)\n",
    "\n",
    "#################################################\n",
    "#    SurveyDate                     object\n",
    "#    FormalEducation                object\n",
    "#    ConvertedSalary               float64\n",
    "#    Hobby                          object\n",
    "#    Country                        object\n",
    "#    StackOverflowJobsRecommend    float64\n",
    "#    VersionControl                 object\n",
    "#    Age                             int64\n",
    "#    Years Experience                int64\n",
    "#    Gender                         object\n",
    "#    RawSalary                      object\n",
    "#    dtype: object\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Selecting specific data types\n",
    "#Often a data set will contain columns with several different data\n",
    "#types (like the one you are working with). The majority of machine\n",
    "#learning models require you to have a consistent data type across\n",
    "#features. Similarly, most feature engineering techniques are\n",
    "#applicable to only one type of data at a time. For these reasons\n",
    "#among others, you will often want to be able to access just the\n",
    "#columns of certain types when working with a DataFrame.\n",
    "\n",
    "#The DataFrame (so_survey_df) from the previous exercise is available\n",
    "#in your workspace.\n",
    "\n",
    "# Create subset of only the numeric columns\n",
    "#so_numeric_df = so_survey_df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Print the column names contained in so_survey_df_num\n",
    "#print(so_numeric_df.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['ConvertedSalary', 'StackOverflowJobsRecommend', 'Age', 'Years Experience'], dtype='object')\n",
    "#################################################\n",
    "# In the next lesson, you will learn the most common ways of dealing\n",
    "#with categorical data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with categorical features**\n",
    "___\n",
    "- encoding categorical features\n",
    "    - one-hot encoding\n",
    "        - converts n categories into n features\n",
    "        - explainable features\n",
    "        - problem of collinearity\n",
    "    - dummy encoding\n",
    "        - converts n categories into n-1 features\n",
    "        -\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#One-hot encoding and dummy variables\n",
    "\n",
    "#To use categorical variables in a machine learning model, you first\n",
    "#need to represent them in a quantitative way. The two most common\n",
    "#approaches are to one-hot encode the variables using or to use dummy\n",
    "#variables. In this exercise, you will create both types of encoding,\n",
    "#and compare the created column sets. We will continue using the same\n",
    "#DataFrame from previous lesson loaded as so_survey_df and focusing on\n",
    "#its Country column.\n",
    "\n",
    "# Convert the Country column to a one hot encoded Data Frame\n",
    "#one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\n",
    "\n",
    "# Print the columns names\n",
    "#print(one_hot_encoded.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend', 'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'OH_France', 'OH_India',\n",
    "#           'OH_Ireland', 'OH_Russia', 'OH_South Africa', 'OH_Spain', 'OH_Sweeden', 'OH_UK', 'OH_USA', 'OH_Ukraine'],\n",
    "#          dtype='object')\n",
    "#################################################\n",
    "\n",
    "# Create dummy variables for the Country column\n",
    "#dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\n",
    "\n",
    "# Print the columns names\n",
    "#print(dummy.columns)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend', 'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'DM_India', 'DM_Ireland',\n",
    "#           'DM_Russia', 'DM_South Africa', 'DM_Spain', 'DM_Sweeden', 'DM_UK', 'DM_USA', 'DM_Ukraine'],\n",
    "#          dtype='object')\n",
    "#################################################\n",
    "#Did you notice that the column for France was missing when you\n",
    "#created dummy variables? Now you can choose to use one-hot encoding\n",
    "#or dummy variables where appropriate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dealing with uncommon categories\n",
    "\n",
    "#Some features can have many different categories but a very uneven\n",
    "#distribution of their occurrences. Take for example Data Science's\n",
    "#favorite languages to code in, some common choices are Python, R,\n",
    "#and Julia, but there can be individuals with bespoke choices, like\n",
    "#FORTRAN, C etc. In these cases, you may not want to create a feature\n",
    "#for each value, but only the more common occurrences.\n",
    "\n",
    "# Create a series out of the Country column\n",
    "#countries = so_survey_df['Country']\n",
    "\n",
    "# Get the counts of each category\n",
    "#country_counts = countries.value_counts()\n",
    "\n",
    "# Print the count values for each category\n",
    "#print(country_counts)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    South Africa    166\n",
    "#    USA             164\n",
    "#    Spain           134\n",
    "#    Sweeden         119\n",
    "#    France          115\n",
    "#    Russia           97\n",
    "#    India            95\n",
    "#    UK               95\n",
    "#    Ukraine           9\n",
    "#    Ireland           5\n",
    "#    Name: Country, dtype: int64\n",
    "#################################################\n",
    "\n",
    "# Create a mask for only categories that occur less than 10 times\n",
    "#mask = countries.isin(country_counts[country_counts < 10].index)\n",
    "\n",
    "# Print the top 5 rows in the mask series\n",
    "#print(mask.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    False\n",
    "#    1    False\n",
    "#    2    False\n",
    "#    3    False\n",
    "#    4    False\n",
    "#    Name: Country, dtype: bool\n",
    "#################################################\n",
    "\n",
    "# Label all other categories as Other\n",
    "#countries[mask] = 'Other'\n",
    "\n",
    "# Print the updated category counts\n",
    "#print(pd.value_counts(countries))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    South Africa    166\n",
    "#    USA             164\n",
    "#    Spain           134\n",
    "#    Sweeden         119\n",
    "#    France          115\n",
    "#    Russia           97\n",
    "#    India            95\n",
    "#    UK               95\n",
    "#    Other            14\n",
    "#    Name: Country, dtype: int64\n",
    "#################################################\n",
    "#now you can work with large data sets while grouping low frequency\n",
    "#categories."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Numeric variables**\n",
    "___\n",
    "- Types of numeric features\n",
    "    - age\n",
    "    - price\n",
    "    - counts\n",
    "    - geospatial data\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Binarizing columns\n",
    "\n",
    "#While numeric values can often be used without any feature\n",
    "#engineering, there will be cases when some form of manipulation\n",
    "#can be useful. For example on some occasions, you might not care\n",
    "#about the magnitude of a value but only care about its direction,\n",
    "#or if it exists at all. In these situations, you will want to\n",
    "#binarize a column. In the so_survey_df data, you have a large\n",
    "#number of survey respondents that are working voluntarily (without\n",
    "#pay). You will create a new column titled Paid_Job indicating\n",
    "#whether each person is paid (their salary is greater than zero).\n",
    "\n",
    "# Create the Paid_Job column filled with zeros\n",
    "#so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
    "#so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print the first five rows of the columns\n",
    "#print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Paid_Job  ConvertedSalary\n",
    "#    0         0              0.0\n",
    "#    1         1          70841.0\n",
    "#    2         0              0.0\n",
    "#    3         1          21426.0\n",
    "#    4         1          41671.0\n",
    "#################################################\n",
    "#binarizing columns can also be useful for your target variables."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Binning values\n",
    "\n",
    "#For many continuous values you will care less about the exact value\n",
    "#of a numeric column, but instead care about the bucket it falls into.\n",
    "#This can be useful when plotting values, or simplifying your machine\n",
    "#learning models. It is mostly used on continuous variables where\n",
    "#accuracy is not the biggest concern e.g. age, height, wages.\n",
    "\n",
    "#Bins are created using pd.cut(df['column_name'], bins) where bins can\n",
    "#be an integer specifying the number of evenly spaced bins, or a list\n",
    "#of bin boundaries.\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary into 5 bins\n",
    "#so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], 5)\n",
    "\n",
    "# Print the first 5 rows of the equal_binned column\n",
    "#print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#              equal_binned  ConvertedSalary\n",
    "#    0  (-2000.0, 400000.0]              0.0\n",
    "#    1  (-2000.0, 400000.0]          70841.0\n",
    "#    2  (-2000.0, 400000.0]              0.0\n",
    "#    3  (-2000.0, 400000.0]          21426.0\n",
    "#    4  (-2000.0, 400000.0]          41671.0\n",
    "#################################################\n",
    "\n",
    "# Import numpy\n",
    "#import numpy as np\n",
    "\n",
    "# Specify the boundaries of the bins\n",
    "#bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# Bin labels\n",
    "#labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the continuous variable ConvertedSalary using these boundaries\n",
    "#so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'],\n",
    "#                                         bins, labels = labels)\n",
    "\n",
    "# Print the first 5 rows of the boundary_binned column\n",
    "#print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#      boundary_binned  ConvertedSalary\n",
    "#    0        Very low              0.0\n",
    "#    1          Medium          70841.0\n",
    "#    2        Very low              0.0\n",
    "#    3             Low          21426.0\n",
    "#    4             Low          41671.0\n",
    "#################################################\n",
    "#now you can bin columns with equal spacing and predefined boundaries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Why do missing values exist?**\n",
    "___\n",
    "- How gaps in data occur\n",
    "    - data not being collected properly\n",
    "    - collection and management errors\n",
    "    - data intentionally being omitted\n",
    "    - could be created due to transformations of the data\n",
    "- Why we care?\n",
    "    - some models cannot work with missing data (Nulls/NaNs)\n",
    "    - missing data may be a sign of a wider data issue\n",
    "    - missing data can be a useful feature\n",
    "- pd.info()\n",
    "- pd.isnull()\n",
    "- pd.isnull().sum()\n",
    "- df.notnull()\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#How sparse is my data?\n",
    "#Most data sets contain missing values, often represented as NaN\n",
    "#(Not a Number). If you are working with Pandas you can easily check\n",
    "#how many missing values exist in each column.\n",
    "\n",
    "#Let's find out how many of the developers taking the survey chose to\n",
    "#enter their age (found in the Age column of so_survey_df) and their\n",
    "#gender (Gender column of so_survey_df).\n",
    "\n",
    "# Subset the DataFrame\n",
    "#sub_df = so_survey_df[['Age', 'Gender']]\n",
    "\n",
    "# Print the number of non-missing values\n",
    "#print(sub_df.info())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    <class 'pandas.core.frame.DataFrame'>\n",
    "#    RangeIndex: 999 entries, 0 to 998\n",
    "#    Data columns (total 2 columns):\n",
    "#    Age       999 non-null int64\n",
    "#    Gender    693 non-null object\n",
    "#    dtypes: int64(1), object(1)\n",
    "#    memory usage: 15.7+ KB\n",
    "#    None\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Finding the missing values\n",
    "\n",
    "#While having a summary of how much of your data is missing can be\n",
    "#useful, often you will need to find the exact locations of these\n",
    "#missing values. Using the same subset of the StackOverflow data\n",
    "#from the last exercise (sub_df), you will show how a value can be\n",
    "#flagged as missing.\n",
    "\n",
    "# Print the top 10 entries of the DataFrame\n",
    "#print(sub_df.head(10))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Age  Gender\n",
    "#    0   21    Male\n",
    "#    1   38    Male\n",
    "#    2   45     NaN\n",
    "#    3   46    Male\n",
    "#    4   39    Male\n",
    "#    5   39    Male\n",
    "#    6   34    Male\n",
    "#    7   24  Female\n",
    "#    8   23    Male\n",
    "#    9   36     NaN\n",
    "#################################################\n",
    "\n",
    "# Print the locations of the missing values\n",
    "#print(sub_df.head(10).isnull())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#         Age  Gender\n",
    "#    0  False   False\n",
    "#    1  False   False\n",
    "#    2  False    True\n",
    "#    3  False   False\n",
    "#    4  False   False\n",
    "#    5  False   False\n",
    "#    6  False   False\n",
    "#    7  False   False\n",
    "#    8  False   False\n",
    "#    9  False    True\n",
    "#################################################\n",
    "\n",
    "# Print the locations of the non-missing values\n",
    "#print(sub_df.head(10).notnull())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        Age  Gender\n",
    "#    0  True    True\n",
    "#    1  True    True\n",
    "#    2  True   False\n",
    "#    3  True    True\n",
    "#    4  True    True\n",
    "#    5  True    True\n",
    "#    6  True    True\n",
    "#    7  True    True\n",
    "#    8  True    True\n",
    "#    9  True   False\n",
    "#################################################\n",
    "# finding where the missing values exist can often be important."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with missing values (I)**\n",
    "___\n",
    "- pd.dropna()\n",
    "- pd.drop()\n",
    "- random omissions\n",
    "    - complete case analysis / listwise deletion\n",
    "    - drawbacks\n",
    "        - deletes valid data points as well\n",
    "        - relies on randomness\n",
    "        - reduces information if a feature is removed (degrees of freedom)\n",
    "- replacement\n",
    "- recording missing values\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Listwise deletion\n",
    "\n",
    "#The simplest way to deal with missing values in your dataset when\n",
    "#they are occurring entirely at random is to remove those rows, also\n",
    "#called 'listwise deletion'.\n",
    "\n",
    "#Depending on the use case, you will sometimes want to remove all\n",
    "#missing values in your data while other times you may want to only\n",
    "#remove a particular column if too many values are missing in that\n",
    "#column.\n",
    "\n",
    "# Print the number of rows and columns\n",
    "#print(so_survey_df.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (999, 11)\n",
    "#################################################\n",
    "\n",
    "# Create a new DataFrame dropping all incomplete rows\n",
    "#no_missing_values_rows = so_survey_df.dropna()\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "#print(no_missing_values_rows.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (264, 11)\n",
    "#################################################\n",
    "\n",
    "# Create a new DataFrame dropping all columns with incomplete rows\n",
    "#no_missing_values_cols = so_survey_df.dropna(how='any', axis=1)\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "#print(no_missing_values_cols.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (999, 7)\n",
    "#################################################\n",
    "\n",
    "# Drop all rows where Gender is missing\n",
    "#no_gender = so_survey_df.dropna(subset=['Gender'])\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "#print(no_gender.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (693, 11)\n",
    "#################################################\n",
    "#as you can see dropping all rows that contain any missing values\n",
    "#may greatly reduce the size of your dataset. So you need to think\n",
    "#carefully and consider several trade-offs when deleting missing\n",
    "#values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Replacing missing values with constants\n",
    "\n",
    "#While removing missing data entirely maybe a correct approach in\n",
    "#many situations, this may result in a lot of information being\n",
    "#omitted from your models.\n",
    "\n",
    "#You may find categorical columns where the missing value is a valid\n",
    "#piece of information in itself, such as someone refusing to answer\n",
    "#a question in a survey. In these cases, you can fill all missing\n",
    "#values with a new category entirely, for example 'No response given'.\n",
    "\n",
    "# Print the count of occurrences\n",
    "#print(so_survey_df['Gender'].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Male                                                                        632\n",
    "#    Female                                                                 53\n",
    "#    Female;Male                                                                 2\n",
    "#    Transgender                                                               2\n",
    "#    Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming      1\n",
    "#    Female;Transgender                                                             1\n",
    "#    Male;Non-binary. genderqueer. or gender non-conforming                         1\n",
    "#    Non-binary. genderqueer. or gender non-conforming                              1\n",
    "#    Name: Gender, dtype: int64\n",
    "#################################################\n",
    "\n",
    "# Replace missing values\n",
    "#so_survey_df['Gender'].fillna(value='Not Given', inplace=True)\n",
    "\n",
    "# Print the count of each value\n",
    "#print(so_survey_df['Gender'].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Male                                                                         632\n",
    "#    Not Given                                                                    306\n",
    "#    Female                                                                        53\n",
    "#    Female;Male                                                                    2\n",
    "#    Transgender                                                                    2\n",
    "#    Female;Male;Transgender;Non-binary. genderqueer. or gender non-conforming      1\n",
    "#    Female;Transgender                                                             1\n",
    "#    Male;Non-binary. genderqueer. or gender non-conforming                         1\n",
    "#    Non-binary. genderqueer. or gender non-conforming                              1\n",
    "#    Name: Gender, dtype: int64\n",
    "#################################################\n",
    "#By filling in these missing values you can use the columns in your\n",
    "#analyses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with missing values (II)**\n",
    "___\n",
    "- If you cannot drop rows, what else can you do?\n",
    "    - **Categorical columns**: replace missing values with the most common occurring value or with a string that flags missing values such as 'None'\n",
    "    - **Numeric columns**: replace missing values with a suitable value\n",
    "        - measure of central tendency, e.g., mean, median\n",
    "- impute values based on the train set to both train and test sets.\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Filling continuous missing values\n",
    "\n",
    "#In the last lesson, you dealt with different methods of removing\n",
    "#data missing values and filling in missing values with a fixed\n",
    "#string. These approaches are valid in many cases, particularly when\n",
    "#dealing with categorical columns but have limited use when working\n",
    "#with continuous values. In these cases, it may be most valid to fill\n",
    "#the missing values in the column with a value calculated from the\n",
    "#entries present in the column.\n",
    "\n",
    "# Print the first five rows of StackOverflowJobsRecommend column\n",
    "#print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    NaN\n",
    "#    1    7.0\n",
    "#    2    8.0\n",
    "#    3    NaN\n",
    "#    4    8.0\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "\n",
    "# Fill missing values with the mean\n",
    "#so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
    "\n",
    "# Print the first five rows of StackOverflowJobsRecommend column\n",
    "#print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    7.061602\n",
    "#    1    7.000000\n",
    "#    2    8.000000\n",
    "#    3    7.061602\n",
    "#    4    8.000000\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "\n",
    "# Round the StackOverflowJobsRecommend values\n",
    "#so_survey_df['StackOverflowJobsRecommend'] = np.round (so_survey_df['StackOverflowJobsRecommend'])\n",
    "\n",
    "# Print the top 5 rows\n",
    "#print(so_survey_df['StackOverflowJobsRecommend'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    7.0\n",
    "#    1    7.0\n",
    "#    2    8.0\n",
    "#    3    7.0\n",
    "#    4    8.0\n",
    "#    Name: StackOverflowJobsRecommend, dtype: float64\n",
    "#################################################\n",
    "#remember you should only round your values if you are certain it is applicable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dealing with other data issues**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dealing with stray characters (I)\n",
    "\n",
    "#In this exercise, you will work with the RawSalary column of\n",
    "#so_survey_df which contains the wages of the respondents along with\n",
    "#the currency symbols and commas, such as $42,000. When importing\n",
    "#data from Microsoft Excel, more often than not you will come across\n",
    "#data in this form.\n",
    "\n",
    "# Remove the commas in the column\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace(',', '')\n",
    "\n",
    "# Remove the dollar signs in the column\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('$', '')\n",
    "\n",
    "#################################################\n",
    "#Replacing/removing specific characters is a very useful skill."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dealing with stray characters (II)\n",
    "\n",
    "#In the last exercise, you could tell quickly based off of the\n",
    "#df.head() call which characters were causing an issue. In many cases\n",
    "#this will not be so apparent. There will often be values deep within\n",
    "#a column that are preventing you from casting a column as a numeric\n",
    "#type so that it can be used in a model or further feature engineering.\n",
    "\n",
    "#One approach to finding these values is to force the column to the\n",
    "#data type desired using pd.to_numeric(), coercing any values causing\n",
    "#issues to NaN, Then filtering the DataFrame by just the rows containing\n",
    "#the NaN values.\n",
    "\n",
    "#Try to cast the RawSalary column as a float and it will fail as an\n",
    "#additional character can now be found in it. Find the character and\n",
    "#remove it so the column can be cast as a float.\n",
    "\n",
    "# Attempt to convert the column to numeric values\n",
    "#numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')\n",
    "\n",
    "# Find the indexes of missing values\n",
    "#idx = numeric_vals.isna()\n",
    "\n",
    "# Print the relevant rows\n",
    "#print(so_survey_df['RawSalary'][idx])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0             NaN\n",
    "#    2             NaN\n",
    "#    4       Â£41671.00\n",
    "#    6             NaN\n",
    "#    8             NaN\n",
    "#    ...\n",
    "#    49      Â£19500.00\n",
    "#    50            NaN\n",
    "#    52            NaN\n",
    "#    53      Â£36000.00\n",
    "#    54            NaN\n",
    "#    Name: RawSalary, Length: 401, dtype: object\n",
    "#################################################\n",
    "\n",
    "# Replace the offending characters\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('Â£', '')\n",
    "\n",
    "# Convert the column to float\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary'].astype('float')\n",
    "\n",
    "# Print the column\n",
    "#print(so_survey_df['RawSalary'])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0            NaN\n",
    "#    1        70841.0\n",
    "#    2            NaN\n",
    "#    3        21426.0\n",
    "#    4        41671.0\n",
    "#    ...\n",
    "#    994          NaN\n",
    "#    995      58746.0\n",
    "#    996      55000.0\n",
    "#    997          NaN\n",
    "#    998    1000000.0\n",
    "#    Name: RawSalary, Length: 999, dtype: float64\n",
    "#################################################\n",
    "#Remember that even after removing all the relevant characters, you\n",
    "#still need to change the type of the column to numeric if you want\n",
    "#to plot these continuous values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Method chaining\n",
    "\n",
    "#When applying multiple operations on the same column (like in the\n",
    "#previous exercises), you made the changes in several steps, assigning\n",
    "#the results back in each step. However, when applying multiple\n",
    "#successive operations on the same column, you can \"chain\" these\n",
    "#operations together for clarity and ease of management. This can be\n",
    "#achieved by calling multiple methods sequentially:\n",
    "\n",
    "# Method chaining\n",
    "#df['column'] = df['column'].method1().method2().method3()\n",
    "\n",
    "# Same as\n",
    "#df['column'] = df['column'].method1()\n",
    "#df['column'] = df['column'].method2()\n",
    "#df['column'] = df['column'].method3()\n",
    "\n",
    "#In this exercise you will repeat the steps you performed in the last\n",
    "#two exercises, but do so using method chaining.\n",
    "\n",
    "# Use method chaining\n",
    "#so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
    "#                              .str.replace(',', '')\\\n",
    "#                              .str.replace('$', '')\\\n",
    "#                              .str.replace('Â£', '')\\\n",
    "#                              .astype('float')\n",
    "\n",
    "# Print the RawSalary column\n",
    "#print(so_survey_df['RawSalary'])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0            NaN\n",
    "#    1        70841.0\n",
    "#    2            NaN\n",
    "#    3        21426.0\n",
    "#    4        41671.0\n",
    "#    ...\n",
    "#    994          NaN\n",
    "#    995      58746.0\n",
    "#    996      55000.0\n",
    "#    997          NaN\n",
    "#    998    1000000.0\n",
    "#    Name: RawSalary, Length: 999, dtype: float64\n",
    "#################################################\n",
    "#Custom functions can be also used when method chaining using the\n",
    "#.apply() method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data distributions**\n",
    "___\n",
    "- most models assume your data is normally distributed and/or on the same scale\n",
    "    - 1 sd = 66.27%; 2 sd = 95.45%; 3 sd = 99.73%\n",
    "- decision tree-based models do not make this assumption\n",
    "    - As decision trees split along a singular point, they do not require all the columns to be on the same scale.\n",
    "- delving deeper with box plots\n",
    "    - Interquartile Range (IQR) = 25th (Q1) percentile to 75th (Q3) percentile\n",
    "    - Minimum = Q1 - 1.5 IQR\n",
    "    - Maximum = Q3 + 1.5 IQR\n",
    "    - outliers are outside Minimum or Maximum\n",
    "- pairing distributions with seaborn library\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#What does your data look like? (I)\n",
    "\n",
    "#Up until now you have focused on creating new features and dealing\n",
    "#with issues in your data. Feature engineering can also be used to\n",
    "#make the most out of the data that you already have and use it more\n",
    "#effectively when creating machine learning models.\n",
    "\n",
    "#Many algorithms may assume that your data is normally distributed,\n",
    "#or at least that all your columns are on the same scale. This will\n",
    "#often not be the case, e.g. one feature may be measured in thousands\n",
    "#of dollars while another would be number of years. In this exercise,\n",
    "#you will create plots to examine the distributions of some numeric\n",
    "#columns in the so_survey_df DataFrame, stored in so_numeric_df.\n",
    "\n",
    "# Create a histogram\n",
    "#so_numeric_df.hist()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.1.svg](_images/16.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a boxplot of two columns\n",
    "#so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.2.svg](_images/16.2.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a boxplot of ConvertedSalary\n",
    "#so_numeric_df[['ConvertedSalary']].boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.3.svg](_images/16.3.svg)\n",
    "as you can see the distributions of columns in a dataset can vary quite a bit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#What does your data look like? (II)\n",
    "\n",
    "#In the previous exercise you looked at the distribution of individual\n",
    "#columns. While this is a good start, a more detailed view of how\n",
    "#different features interact with each other may be useful as this\n",
    "#can impact your decision on what to transform and how.\n",
    "\n",
    "# Import packages\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Plot pairwise relationships\n",
    "#sns.pairplot(so_numeric_df)\n",
    "\n",
    "# Show plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.4.svg](_images/16.4.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "#print(so_numeric_df.describe())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#           ConvertedSalary         Age  Years Experience\n",
    "#    count     9.990000e+02  999.000000        999.000000\n",
    "#    mean      6.161746e+04   36.003003          9.961962\n",
    "#    std       1.760924e+05   13.255127          4.878129\n",
    "#    min       0.000000e+00   18.000000          0.000000\n",
    "#    25%       0.000000e+00   25.000000          7.000000\n",
    "#    50%       2.712000e+04   35.000000         10.000000\n",
    "#    75%       7.000000e+04   45.000000         13.000000\n",
    "#    max       2.000000e+06   83.000000         27.000000\n",
    "#################################################\n",
    "#understanding these summary statistics of a column can be very\n",
    "#valuable when deciding what transformations are necessary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scaling and transformations**\n",
    "___\n",
    "- Min-Max scaling / Normalization\n",
    "    - distribution remains the same\n",
    "    - values change to range 0-1\n",
    "    - MinMaxScaler() from scikit-learn preprocessing module\n",
    "- Standardization\n",
    "    - centers distribution around the mean = zero\n",
    "    - StandardScaler() from scikit-learn preprocessing module\n",
    "- Log Transformation\n",
    "    - can make highly skewed distributions less skewed\n",
    "    - PowerTransformer() from scikit-learn preprocessing module\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "#As discussed in the video, in normalization you linearly scale the\n",
    "#entire column between 0 and 1, with 0 corresponding with the lowest\n",
    "#value in the column, and 1 with the largest.\n",
    "\n",
    "#When using scikit-learn (the most commonly used machine learning\n",
    "#library in Python) you can use a MinMaxScaler to apply normalization.\n",
    "#(It is called this as it scales your values between a minimum and\n",
    "#maximum value.)\n",
    "\n",
    "# Import MinMaxScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate MinMaxScaler\n",
    "#MM_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit MM_scaler to the data\n",
    "#MM_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "#so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "#print(so_numeric_df[['Age_MM', 'Age']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#         Age_MM  Age\n",
    "#    0  0.046154   21\n",
    "#    1  0.307692   38\n",
    "#    2  0.415385   45\n",
    "#    3  0.430769   46\n",
    "#    4  0.323077   39\n",
    "#################################################\n",
    "#Did you notice that all values have been scaled between 0 and 1?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Standardization\n",
    "\n",
    "#While normalization can be useful for scaling a column between two\n",
    "#data points, it is hard to compare two scaled columns if even one of\n",
    "#them is overly affected by outliers. One commonly used solution to\n",
    "#this is called standardization, where instead of having a strict\n",
    "#upper and lower bound, you center the data around its mean, and\n",
    "#calculate the number of standard deviations away from mean each data\n",
    "#point is.\n",
    "\n",
    "# Import StandardScaler\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "#SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit SS_scaler to the data\n",
    "#SS_scaler.fit(so_numeric_df[['Age']])\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "#so_numeric_df['Age_SS'] = SS_scaler.transform(so_numeric_df[['Age']])\n",
    "\n",
    "# Compare the origional and transformed column\n",
    "#print(so_numeric_df[['Age_SS', 'Age']].head())\n",
    "\n",
    "#################################################\n",
    "#       Age_SS  Age\n",
    "#    0 -1.132431   21\n",
    "#    1  0.150734   38\n",
    "#    2  0.679096   45\n",
    "#    3  0.754576   46\n",
    "#    4  0.226214   39\n",
    "#################################################\n",
    "#you can see that the values have been scaled linearly, but not\n",
    "#between set values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Log transformation\n",
    "\n",
    "#In the previous exercises you scaled the data linearly, which will\n",
    "#not affect the data's shape. This works great if your data is\n",
    "#normally distributed (or closely normally distributed), an assumption\n",
    "#that a lot of machine learning models make. Sometimes you will work\n",
    "#with data that closely conforms to normality, e.g the height or\n",
    "#weight of a population. On the other hand, many variables in the\n",
    "#real world do not follow this pattern e.g, wages or age of a\n",
    "#population. In this exercise you will use a log transform on the\n",
    "#ConvertedSalary column in the so_numeric_df DataFrame as it has a\n",
    "#large amount of its data centered around the lower values, but\n",
    "#contains very high values also. These distributions are said to have\n",
    "#a long right tail.\n",
    "\n",
    "# Import PowerTransformer\n",
    "#from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Instantiate PowerTransformer\n",
    "#pow_trans = PowerTransformer()\n",
    "\n",
    "# Train the transform on the data\n",
    "#pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Apply the power transform to the data\n",
    "#so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
    "\n",
    "# Plot the data before and after the transformation\n",
    "#so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.5.svg](_images/16.5.svg)\n",
    "Did you notice the change in the shape of the distribution?\n",
    "ConvertedSalary_LG column looks much more normal than the original\n",
    "ConvertedSalary column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Removing outliers**\n",
    "___\n",
    "- Quantile based detection\n",
    "- Standard deviation based detection\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Percentage based outlier removal\n",
    "\n",
    "#One way to ensure a small portion of data is not having an overly\n",
    "#adverse effect is by removing a certain percentage of the largest\n",
    "#and/or smallest values in the column. This can be achieved by\n",
    "#finding the relevant quantile and trimming the data using it with\n",
    "#a mask. This approach is particularly useful if you are concerned\n",
    "#that the highest values in your dataset should be avoided. When\n",
    "#using this approach, you must remember that even if there are no\n",
    "#outliers, this will still remove the same top N percentage from the\n",
    "#dataset.\n",
    "\n",
    "# Find the 95th quantile\n",
    "#quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)\n",
    "\n",
    "# Trim the outliers\n",
    "#trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
    "\n",
    "# The original histogram\n",
    "#so_numeric_df[['ConvertedSalary']].hist()\n",
    "#plt.show()\n",
    "#plt.clf()\n",
    "\n",
    "# The trimmed histogram\n",
    "#trimmed_df[['ConvertedSalary']].hist()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.6.svg](_images/16.6.svg)\n",
    "![_images/16.7.svg](_images/16.7.svg)\n",
    "In the next exercise, you will work with a more statistically sound\n",
    "approach in removing outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Statistical outlier removal\n",
    "\n",
    "#While removing the top N% of your data is useful for ensuring that\n",
    "#very spurious points are removed, it does have the disadvantage of\n",
    "#always removing the same proportion of points, even if the data is\n",
    "#correct. A commonly used alternative approach is to remove data that\n",
    "#sits further than three standard deviations from the mean. You can\n",
    "#implement this by first calculating the mean and standard deviation\n",
    "#of the relevant column to find upper and lower bounds, and applying\n",
    "#these bounds as a mask to the DataFrame. This method ensures that\n",
    "#only data that is genuinely different from the rest is removed, and\n",
    "#will remove fewer points if the data is close together."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.3.svg](_images/16.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the mean and standard dev\n",
    "#std = so_numeric_df['ConvertedSalary'].std()\n",
    "#mean = so_numeric_df['ConvertedSalary'].mean()\n",
    "\n",
    "# Calculate the cutoff\n",
    "#cut_off = std * 3\n",
    "#lower, upper = mean - cut_off, mean + cut_off\n",
    "\n",
    "# Trim the outliers\n",
    "#trimmed_df = so_numeric_df[(so_numeric_df['ConvertedSalary'] < upper) \\\n",
    "#                           & (so_numeric_df['ConvertedSalary'] > lower)]\n",
    "\n",
    "# The trimmed box plot\n",
    "#trimmed_df[['ConvertedSalary']].boxplot()\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/16.8.svg](_images/16.8.svg)\n",
    "Did you notice the scale change on the y-axis?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scaling and transforming new data**\n",
    "___\n",
    "- you fit and transform the training data, but only transform test data\n",
    "- remove outliers from training set, but generally not on test test\n",
    "- Why only use training data?\n",
    "    - **data leakage** - using data that you won't have access to when assessing the performance of your model\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train and testing transformations (I)\n",
    "\n",
    "#So far you have created scalers based on a column, and then applied\n",
    "#the scaler to the same data that it was trained on. When creating\n",
    "#machine learning models you will generally build your models on\n",
    "#historic data (train set) and apply your model to new unseen data\n",
    "#(test set). In these cases you will need to ensure that the same\n",
    "#scaling is being applied to both the training and test data.\n",
    "\n",
    "#To do this in practice you train the scaler on the train set, and\n",
    "#keep the trained scaler to apply it to the test set. You should\n",
    "#never retrain a scaler on the test set.\n",
    "\n",
    "#For this exercise and the next, we split the so_numeric_df\n",
    "#DataFrame into train (so_train_numeric) and test (so_test_numeric)\n",
    "#sets.\n",
    "\n",
    "# Import StandardScaler\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply a standard scaler to the data\n",
    "#SS_scaler = StandardScaler()\n",
    "\n",
    "# Fit the standard scaler to the data\n",
    "#SS_scaler.fit(so_train_numeric[['Age']])\n",
    "\n",
    "# Transform the test data using the fitted scaler\n",
    "#so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
    "#print(so_test_numeric[['Age', 'Age_ss']].head())\n",
    "\n",
    "#################################################\n",
    "#       Age    Age_ss\n",
    "#    700   35 -0.069265\n",
    "#    701   18 -1.343218\n",
    "#    702   47  0.829997\n",
    "#    703   57  1.579381\n",
    "#    704   41  0.380366\n",
    "#################################################\n",
    "#Data leakage is one of the most common mistakes data scientists tend\n",
    "#to make, and I hope that you won't!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train and testing transformations (II)\n",
    "\n",
    "#Similar to applying the same scaler to both your training and test\n",
    "#sets, if you have removed outliers from the train set, you probably\n",
    "#want to do the same on the test set as well. Once again you should\n",
    "#ensure that you use the thresholds calculated only from the train\n",
    "#set to remove outliers from the test set.\n",
    "\n",
    "#Similar to the last exercise, we split the so_numeric_df DataFrame\n",
    "#into train (so_train_numeric) and test (so_test_numeric) sets.\n",
    "\n",
    "#train_std = so_train_numeric['ConvertedSalary'].std()\n",
    "#train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
    "\n",
    "#cut_off = train_std * 3\n",
    "#train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n",
    "\n",
    "# Trim the test DataFrame\n",
    "#trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) \\\n",
    "#                             & (so_test_numeric['ConvertedSalary'] > train_lower)]\n",
    "\n",
    "#################################################\n",
    "#In the next chapter, you will deal with unstructured (text) data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Encoding text**\n",
    "___\n",
    "- Standardizing your text\n",
    "    - remove unwanted characters using str.replace() and regular expressions\n",
    "        - [a-zA-Z]: all letter characters\n",
    "        - [^a-zA-Z]: all non letter characters\n",
    "- Standardize the case\n",
    "    - str.lower()\n",
    "- length of text\n",
    "    - .len()\n",
    "- word count\n",
    "- average length of word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Cleaning up your text\n",
    "\n",
    "#Unstructured text data cannot be directly used in most analyses.\n",
    "#Multiple steps need to be taken to go from a long free form string\n",
    "#to a set of numeric columns in the right format that can be\n",
    "#ingested by a machine learning model. The first step of this process\n",
    "#is to standardize the data and eliminate any characters that could\n",
    "#cause problems later on in your analytic pipeline.\n",
    "\n",
    "#In this chapter you will be working with a new dataset containing\n",
    "#the inaugural speeches of the presidents of the United States\n",
    "#loaded as speech_df, with the speeches stored in the text column.\n",
    "\n",
    "# Print the first 5 rows of the text column\n",
    "#print(speech_df['text'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    Fellow-Citizens of the Senate and of the House...\n",
    "#    1    Fellow Citizens:  I AM again called upon by th...\n",
    "#    2    WHEN it was first perceived, in early times, t...\n",
    "#    3    Friends and Fellow-Citizens:  CALLED upon to u...\n",
    "#    4    PROCEEDING, fellow-citizens, to that qualifica...\n",
    "#    Name: text, dtype: object\n",
    "#################################################\n",
    "\n",
    "# Replace all non letter characters with a whitespace\n",
    "#speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "\n",
    "# Change to lower case\n",
    "#speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the text_clean column\n",
    "#print(speech_df['text_clean'].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    fellow citizens of the senate and of the house...\n",
    "#    1    fellow citizens   i am again called upon by th...\n",
    "#    2    when it was first perceived  in early times  t...\n",
    "#    3    friends and fellow citizens   called upon to u...\n",
    "#    4    proceeding  fellow citizens  to that qualifica...\n",
    "#    Name: text_clean, dtype: object\n",
    "#################################################\n",
    "#now your text strings have been standardized and cleaned up. You\n",
    "#can now use this new column (text_clean) to extract information\n",
    "#about the speeches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#High level text features\n",
    "\n",
    "#Once the text has been cleaned and standardized you can begin\n",
    "#creating features from the data. The most fundamental information\n",
    "#you can calculate about free form text is its size, such as its\n",
    "#length and number of words. In this exercise (and the rest of this\n",
    "#chapter), you will focus on the cleaned/transformed text column\n",
    "#(text_clean) you created in the last exercise.\n",
    "\n",
    "# Find the length of each text\n",
    "#speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "\n",
    "# Count the number of words in each text\n",
    "#speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "#speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "#print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                                               text_clean  char_cnt  word_cnt  avg_word_length\n",
    "#    0   fellow citizens of the senate and of the house...      8616      1432         6.016760\n",
    "#    1   fellow citizens   i am again called upon by th...       787       135         5.829630\n",
    "#    2   when it was first perceived  in early times  t...     13871      2323         5.971158\n",
    "#    3   friends and fellow citizens   called upon to u...     10144      1736         5.843318\n",
    "#    4   proceeding  fellow citizens  to that qualifica...     12902      2169         5.948363\n",
    "#################################################\n",
    "#These features may appear basic but can be quite useful in ML models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Word counts**\n",
    "___\n",
    "- text to columns\n",
    "    - one column per word with word counts for each word\n",
    "    - CountVectorizer in sklearn.feature_extraction.text\n",
    "        - min_df, max_df = 0.1, 0.9\n",
    "        - creates sparse array, convert to array using .toarray()\n",
    "            - to get feature names from the array .get_feature_names()\n",
    "        - combine into dataframe\n",
    "            - pd.DataFrame(cv_tranformed.toarray(), columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "        - updating/combining your dataframe\n",
    "            - pd.concat([list], axis=1, sort=False)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Counting words (I)\n",
    "\n",
    "#Once high level information has been recorded you can begin creating\n",
    "#features based on the actual content of each text. One way to do\n",
    "#this is to approach it in a similar way to how you worked with\n",
    "#categorical variables in the earlier lessons.\n",
    "\n",
    "#For each unique word in the dataset a column is created.\n",
    "\n",
    "#For each entry, the number of times this word occurs is counted and\n",
    "#the count value is entered into the respective column.\n",
    "\n",
    "#These \"count\" columns can then be used to train machine learning\n",
    "#models.\n",
    "\n",
    "# Import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "#cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer\n",
    "#cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "#print(cv.get_feature_names())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    ['abandon', 'abandoned', 'abandonment', 'abate', 'abdicated', 'abeyance', 'abhorring', 'abide', 'abiding', 'abilities', 'ability', 'abject', 'able', 'ably', 'abnormal', 'abode', 'abolish', 'abolished', 'abolishing', 'aboriginal', 'aborigines', 'abound', 'abounding', 'abounds', 'about', 'above', 'abraham', 'abreast', 'abridging',\n",
    "#################################################\n",
    "#this vectorizer can be applied to both the text it was trained on, and new texts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Counting words (II)\n",
    "\n",
    "#Once the vectorizer has been fit to the data, it can be used to\n",
    "#transform the text to an array representing the word counts. This\n",
    "#array will have a row per block of text and a column for each of\n",
    "#the features generated by the vectorizer that you observed in the\n",
    "#last exercise.\n",
    "\n",
    "#The vectorizer to you fit in the last exercise (cv) is available\n",
    "#in your workspace.\n",
    "\n",
    "# Apply the vectorizer\n",
    "#cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "#cv_array = cv_transformed.toarray()\n",
    "#print(cv_array)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [[0 0 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]\n",
    "#     [0 1 0 ... 0 0 0]\n",
    "#     ...\n",
    "#     [0 1 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]\n",
    "#     [0 0 0 ... 0 0 0]]\n",
    "#################################################\n",
    "\n",
    "# Print the shape of cv_array\n",
    "#print(cv_array.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (58, 9043)\n",
    "#################################################\n",
    "#The speeches have 9043 unique words, which is a lot! In the next\n",
    "#exercise, you will see how to create a limited set of features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Limiting your features\n",
    "\n",
    "#As you have seen, using the CountVectorizer with its default\n",
    "#settings creates a feature for every single word in your corpus.\n",
    "#This can create far too many features, often including ones that\n",
    "#will provide very little analytical value.\n",
    "\n",
    "#For this purpose CountVectorizer has parameters that you can set to\n",
    "#reduce the number of features:\n",
    "\n",
    "#min_df : Use only words that occur in more than this percentage of documents.\n",
    "#This can be used to remove outlier words that will not generalize across texts.\n",
    "\n",
    "#max_df : Use only words that occur in less than this percentage of documents.\n",
    "#This is useful to eliminate very common words that occur in every corpus without\n",
    "#adding value such as \"and\" or \"the\".\n",
    "\n",
    "# Import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Specify arguments to limit the number of features generated\n",
    "#cv = CountVectorizer(min_df=0.2, max_df=0.8)\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "#cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "#cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the array shape\n",
    "#print(cv_array.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (58, 818)\n",
    "#################################################\n",
    "# the number of features (unique words) greatly reduced from 9043 to 818."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Text to DataFrame\n",
    "#Now that you have generated these count based features in an array\n",
    "#you will need to reformat them so that they can be combined with\n",
    "#the rest of the dataset. This can be achieved by converting the\n",
    "#array into a pandas DataFrame, with the feature names you found\n",
    "#earlier as the column names, and then concatenate it with the\n",
    "#original DataFrame.\n",
    "\n",
    "#The numpy array (cv_array) and the vectorizer (cv) you fit in the\n",
    "#last exercise are available in your workspace.\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "#cv_df = pd.DataFrame(cv_array,\n",
    "#                     columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "#speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "#print(speech_df_new.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                    Name         Inaugural Address                      Date                                               text                                         text_clean  ...  Counts_years  \\\n",
    "#    0  George Washington   First Inaugural Address  Thursday, April 30, 1789  Fellow-Citizens of the Senate and of the House...  fellow citizens of the senate and of the house...  ...             1\n",
    "#    1  George Washington  Second Inaugural Address     Monday, March 4, 1793  Fellow Citizens:  I AM again called upon by th...  fellow citizens   i am again called upon by th...  ...             0\n",
    "#    2         John Adams         Inaugural Address   Saturday, March 4, 1797  WHEN it was first perceived, in early times, t...  when it was first perceived  in early times  t...  ...             3\n",
    "#    3   Thomas Jefferson   First Inaugural Address  Wednesday, March 4, 1801  Friends and Fellow-Citizens:  CALLED upon to u...  friends and fellow citizens   called upon to u...  ...             0\n",
    "#    4   Thomas Jefferson  Second Inaugural Address     Monday, March 4, 1805  PROCEEDING, fellow-citizens, to that qualifica...  proceeding  fellow citizens  to that qualifica...  ...             2\n",
    "#\n",
    "#       Counts_yet  Counts_you  Counts_young  Counts_your\n",
    "#    0           0           5             0            9\n",
    "#    1           0           0             0            1\n",
    "#    2           0           0             0            1\n",
    "#    3           2           7             0            7\n",
    "#    4           2           4             0            4\n",
    "#\n",
    "#    [5 rows x 826 columns]\n",
    "#################################################\n",
    "#With the new features combined with the orginial DataFrame they can\n",
    "#be now used for ML models or analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Term frequency-inverse document frequency**\n",
    "___\n",
    "- TF-IDF =\n",
    "    - count of word occurances / Total words in document\n",
    "        - Divided by\n",
    "    - log (Number of docs word is in / Total number of documents)\n",
    "- reduces the weight of common words and increases weights of uncommon words\n",
    "- from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    - max_features - maximum number of columns\n",
    "    - stop_words - list of common words to omit\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tf-idf\n",
    "\n",
    "#While counts of occurrences of words can be useful to build models,\n",
    "#words that occur many times may skew the results undesirably. To\n",
    "#limit these common words from overpowering your model a form of\n",
    "#normalization can be used. In this lesson you will be using Term\n",
    "#frequency-inverse document frequency (Tf-idf) as was discussed in\n",
    "#the video. Tf-idf has the effect of reducing the value of common\n",
    "#words, while increasing the weight of words that do not occur in\n",
    "#many documents.\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "#tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer and transform the data\n",
    "#tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "#tv_df = pd.DataFrame(tv_transformed.toarray(),\n",
    "#                     columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "#print(tv_df.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       TFIDF_action  TFIDF_administration  TFIDF_america  TFIDF_american  TFIDF_americans  ...  TFIDF_war  TFIDF_way  TFIDF_work  TFIDF_world  TFIDF_years\n",
    "#    0      0.000000              0.133415       0.000000        0.105388              0.0  ...   0.000000   0.060755    0.000000     0.045929     0.052694\n",
    "#    1      0.000000              0.261016       0.266097        0.000000              0.0  ...   0.000000   0.000000    0.000000     0.000000     0.000000\n",
    "#    2      0.000000              0.092436       0.157058        0.073018              0.0  ...   0.024339   0.000000    0.000000     0.063643     0.073018\n",
    "#    3      0.000000              0.092693       0.000000        0.000000              0.0  ...   0.036610   0.000000    0.039277     0.095729     0.000000\n",
    "#    4      0.041334              0.039761       0.000000        0.031408              0.0  ...   0.094225   0.000000    0.000000     0.054752     0.062817\n",
    "#\n",
    "#    [5 rows x 100 columns]\n",
    "#################################################\n",
    "#Did you notice that counting the word occurences and calculating\n",
    "#the Tf-idf weights are very similar? This is one of the reasons\n",
    "#scikit-learn is very popular, a consistent API."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Inspecting Tf-idf values\n",
    "\n",
    "#After creating Tf-idf features you will often want to understand\n",
    "#what are the most highest scored words for each corpus. This can be\n",
    "#achieved by isolating the row you want to examine and then sorting\n",
    "#the scores from high to low.\n",
    "\n",
    "#The DataFrame from the last exercise (tv_df) is available in your\n",
    "#workspace.\n",
    "\n",
    "# Isolate the row to be examined\n",
    "#sample_row = tv_df.iloc[0]\n",
    "\n",
    "# Print the top 5 words of the sorted output\n",
    "#print(sample_row.sort_values(ascending=False).head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    TFIDF_government    0.367430\n",
    "#    TFIDF_public        0.333237\n",
    "#    TFIDF_present       0.315182\n",
    "#    TFIDF_duty          0.238637\n",
    "#    TFIDF_citizens      0.229644\n",
    "#    Name: 0, dtype: float64\n",
    "#################################################\n",
    "#Do you think these scores make sense for the corresponding words?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Transforming unseen data\n",
    "\n",
    "#When creating vectors from text, any transformations that you\n",
    "#perform before training a machine learning model, you also need to\n",
    "#apply on the new unseen (test) data. To achieve this follow the\n",
    "#same approach from the last chapter: fit the vectorizer only on the\n",
    "#training data, and apply it to the test data.\n",
    "\n",
    "#For this exercise the speech_df DataFrame has been split in two:\n",
    "\n",
    "#train_speech_df: The training set consisting of the first 45 speeches.\n",
    "#test_speech_df: The test set consisting of the remaining speeches.\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "#tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer and transform the data\n",
    "#tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "\n",
    "# Transform test data\n",
    "#test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "\n",
    "# Create new features for the test set\n",
    "#test_tv_df = pd.DataFrame(test_tv_transformed.toarray(),\n",
    "#                          columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "#print(test_tv_df.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       TFIDF_action  TFIDF_administration  TFIDF_america  TFIDF_american  TFIDF_authority  ...  TFIDF_war  TFIDF_way  TFIDF_work  TFIDF_world  TFIDF_years\n",
    "#    0      0.000000              0.029540       0.233954        0.082703         0.000000  ...   0.079050   0.033313    0.000000     0.299983     0.134749\n",
    "#    1      0.000000              0.000000       0.547457        0.036862         0.000000  ...   0.052851   0.066817    0.078999     0.277701     0.126126\n",
    "#    2      0.000000              0.000000       0.126987        0.134669         0.000000  ...   0.042907   0.054245    0.096203     0.225452     0.043884\n",
    "#    3      0.037094              0.067428       0.267012        0.031463         0.039990  ...   0.030073   0.038020    0.235998     0.237026     0.061516\n",
    "#    4      0.000000              0.000000       0.221561        0.156644         0.028442  ...   0.021389   0.081124    0.119894     0.299701     0.153133\n",
    "#\n",
    "#    [5 rows x 100 columns]\n",
    "#################################################\n",
    "#the vectorizer should only be fit on the train set, never on your\n",
    "#test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**N-grams**\n",
    "___\n",
    "- Bag of words\n",
    "    - words viewed/analyzed independently\n",
    "    - valence (positive/negative) is ignored\n",
    "- ngram_range\n",
    "    -argument in TfidfVectorizer\n",
    "    - indicates bigrams, trigrams, etc for more context to be considered\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using longer n-grams\n",
    "\n",
    "#So far you have created features based on individual words in each\n",
    "#of the texts. This can be quite powerful when used in a machine\n",
    "#learning model but you may be concerned that by looking at words\n",
    "#individually a lot of the context is being ignored. To deal with\n",
    "#this when creating models you can use n-grams which are sequence\n",
    "#of n words grouped together. For example:\n",
    "\n",
    "#bigrams: Sequences of two consecutive words\n",
    "#trigrams: Sequences of two consecutive words\n",
    "\n",
    "#These can be automatically created in your dataset by specifying\n",
    "#the ngram_range argument as a tuple (n1, n2) where all n-grams in\n",
    "#the n1 to n2 range are included.\n",
    "\n",
    "# Import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate a trigram vectorizer\n",
    "#cv_trigram_vec = CountVectorizer(max_features=100,\n",
    "#                                 stop_words='english',\n",
    "#                                 ngram_range = (3,3))\n",
    "\n",
    "# Fit and apply trigram vectorizer\n",
    "#cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the trigram features\n",
    "#print(cv_trigram_vec.get_feature_names())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "# ['ability preserve protect', 'agriculture commerce manufactures',\n",
    "# 'america ideal freedom', 'amity mutual concession', 'anchor peace home',\n",
    "# 'ask bow heads', 'best ability preserve', 'best interests country',\n",
    "# 'bless god bless', 'bless united states', 'chief justice mr',\n",
    "# 'children children children', 'citizens united states',\n",
    "# 'civil religious liberty', 'civil service reform', 'commerce united states',\n",
    "# 'confidence fellow citizens', 'congress extraordinary session', 'constitution does expressly', 'constitution united states', 'coordinate branches government', 'day task people', 'defend constitution united', 'distinction powers granted', 'distinguished guests fellow', 'does expressly say', 'equal exact justice', 'era good feeling', 'executive branch government', 'faithfully execute office', 'fellow citizens assembled', 'fellow citizens called', 'fellow citizens large', 'fellow citizens world', 'form perfect union', 'general welfare secure', 'god bless america', 'god bless god', 'good greatest number', 'government peace war', 'government united states', 'granted federal government', 'great body people', 'great political parties', 'greatest good greatest', 'guests fellow citizens', 'invasion wars powers', 'land new promise', 'laws faithfully executed', 'letter spirit constitution', 'liberty pursuit happiness', 'life liberty pursuit', 'local self government', 'make hard choices', 'men women children', 'mr chief justice', 'mr majority leader', 'mr president vice', 'mr speaker mr', 'mr vice president', 'nation like person', 'new breeze blowing', 'new states admitted', 'north south east', 'oath prescribed constitution', 'office president united', 'passed generation generation', 'peace shall strive', 'people united states', 'physical moral political', 'policy united states', 'power general government', 'preservation general government', 'preservation sacred liberty', 'preserve protect defend', 'president united states', 'president vice president', 'promote general welfare', 'proof confidence fellow', 'protect defend constitution', 'protection great interests', 'reform civil service', 'reserved states people', 'respect individual human', 'right self government', 'secure blessings liberty', 'south east west', 'sovereignty general government', 'states admitted union', 'territories united states', 'thank god bless', 'turning away old', 'united states america', 'united states best', 'united states government', 'united states great', 'united states maintain', 'united states territory', 'vice president mr', 'welfare secure blessings']\n",
    "#################################################\n",
    "#ere you can see that by taking sequential word pairings, some\n",
    "# context is preserved."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Finding the most common words\n",
    "\n",
    "#Its always advisable once you have created your features to inspect\n",
    "#them to ensure that they are as you would expect. This will allow\n",
    "#you to catch errors early, and perhaps influence what further\n",
    "#feature engineering you will need to do.\n",
    "\n",
    "#The vectorizer (cv) you fit in the last exercise and the sparse\n",
    "#array consisting of word counts (cv_trigram) is available in your\n",
    "#workspace.\n",
    "\n",
    "# Create a DataFrame of the features\n",
    "#cv_tri_df = pd.DataFrame(cv_trigram.toarray(),\n",
    "#                         columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\n",
    "\n",
    "# Print the top 5 words in the sorted output\n",
    "#print(cv_tri_df.sum().sort_values(ascending=False).head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Counts_constitution united states    20\n",
    "#    Counts_people united states          13\n",
    "#    Counts_preserve protect defend       10\n",
    "#    Counts_mr chief justice              10\n",
    "#    Counts_president united states        8\n",
    "#    dtype: int64\n",
    "#################################################\n",
    "#that the most common trigram is constitution united states makes a\n",
    "#lot of sense for US presidents speeches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Wrap-up**\n",
    "___\n",
    "- Chapter 1\n",
    "    - how to understand your data types\n",
    "    - efficient encoding of categorical features\n",
    "    - different ways to work with continuous variables\n",
    "- Chapter 2\n",
    "    - how to locate gaps in your data\n",
    "    - best practices in dealing with incomplete rows\n",
    "    - methods to find and deal with unwanted characters\n",
    "- Chapter 3\n",
    "    - how to observe your data's distribution\n",
    "    - why and how to modify this distribution\n",
    "    - best practices of finding outliers and their removal\n",
    "- Chapter 4\n",
    "    - the foundations of word embeddings\n",
    "    - usage of Term Frequency Inverse Document Frequency (Tf-idf)\n",
    "    - n-grams and its advantages over bag of words\n",
    "- Next steps\n",
    "    - Kaggle competitions\n",
    "    - more DataCamp courses\n",
    "    - your own project\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}