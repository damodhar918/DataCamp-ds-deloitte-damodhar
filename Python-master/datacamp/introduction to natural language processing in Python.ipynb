{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Introduction to regular expressions**\n",
    "___\n",
    "- What is Natural Language Processing?\n",
    "    - field of study focused on making sense of language\n",
    "        - using statistics and computers\n",
    "    - you will learn the basics of NLP\n",
    "        - topic identification\n",
    "        - text classification\n",
    "    - other NLP applications\n",
    "        - chatbots\n",
    "        - translation\n",
    "        - sentiment analysis\n",
    "- What exactly are regular expressions?\n",
    "    - strings with a special syntax\n",
    "    - allow us to match patterns in other strings\n",
    "    - applications of regular expressions:\n",
    "        - find all web links in a document\n",
    "        - parse email addresses\n",
    "        - remove/replace unwanted characters\n",
    "- in Python\n",
    "    - import re\n",
    "    - re.match('pattern', 'string')\n",
    "![_images/18.1.PNG](_images/18.1.PNG)\n",
    "![_images/18.2.PNG](_images/18.2.PNG)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "#Practicing regular expressions: re.split() and re.findall()\n",
    "\n",
    "#Now you'll get a chance to write some regular expressions to match\n",
    "#digits, strings and non-alphanumeric characters. Take a look at\n",
    "#my_string first by printing it in the IPython Shell, to determine\n",
    "#how you might best match the different steps.\n",
    "\n",
    "#Note: It's important to prefix your regex patterns with r to ensure\n",
    "#that your patterns are interpreted in the way you want them to. Else,\n",
    "#you may encounter problems to do with escape sequences in strings. For\n",
    "#example, \"\\n\" in Python is used to indicate a new line, but if you use\n",
    "#the r prefix, it will be interpreted as the raw string \"\\n\" - that is,\n",
    "#the character \"\\\" followed by the character \"n\" - and not as a new line.\n",
    "\n",
    "#The regular expression module re has already been imported for you.\n",
    "\n",
    "#Remember from the video that the syntax for the regex library is to\n",
    "#always to pass the pattern first, and then the string second.\n",
    "\n",
    "import re\n",
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n",
    "\n",
    "#################################################\n",
    "#Practice is the key to mastering RegEx."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to tokenization**\n",
    "___\n",
    "- What is tokenization?\n",
    "    - turning a string or document into **tokens** (smaller chunks)\n",
    "    - one step in preparing a text for NLP\n",
    "    - many different theories and rules\n",
    "    - you can create your own rules using regular expressions\n",
    "    - some examples:\n",
    "        - breaking our words or sentences\n",
    "        - separating punctuation\n",
    "        - separating all hashtags in a tweet\n",
    "    - Why tokenize?\n",
    "        - easier to map part of the speech\n",
    "        - matching common words\n",
    "        - removing unwanted tokens\n",
    "    - nltk library\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beat', 'under', 'winter', 'matter', 'kingdom', 'Oh', 'It', 'suggesting', 'minute', 'King', 'SCENE', 'halves', 'get', 'they', 'use', 'house', 'Patsy', 'coconut', \"'em\", 'They', 'together', \"'s\", 'course', 'sovereign', 'since', 'covered', 'I', \"n't\", 'five', 'speak', 'zone', 'and', 'point', 'strand', 'why', 'horse', 'Whoa', 'in', '.', 'In', 'Well', 'or', 'England', '1', 'Halt', 'here', \"'re\", 'Where', 'What', 'to', 'times', 'one', 'weight', 'bird', 'he', 'carrying', 'creeper', 'master', 'who', 'bring', 'ounce', 'Found', 'this', 'lord', 'at', 'coconuts', 'No', 'tropical', 'Please', 'goes', 'martin', 'other', 'African', 'through', 'seek', 'maintain', 'Mercea', 'Not', 'join', 'bangin', 'be', 'ask', 'non-migratory', 'anyway', 'grips', 'by', 'will', 'am', 'does', 'migrate', 'empty', 'KING', '[', 'plover', 'an', 'you', 'carry', 'found', 'are', 'We', 'could', 'fly', 'but', 'Yes', 'it', 'Will', 'mean', 'You', 'just', \"'d\", 'Ridden', 'swallow', 'simple', 'tell', '!', 'these', 'my', '--', '#', 'Arthur', 'yet', 'swallows', 'climes', 'pound', 'castle', 'Am', 'temperate', 'husk', 'is', 'then', 'second', 'dorsal', 'ratios', 'may', 'held', ']', 'me', 'a', 'Listen', 'air-speed', 'SOLDIER', 'son', 'back', 'search', 'Britons', 'breadth', 'ridden', 'wings', ',', 'two', 'if', 'guiding', 'servant', 'strangers', 'land', 'Wait', 'on', 'question', 'Who', '?', 'A', 'go', 'interested', 'order', 'using', 'carried', \"'\", ':', 'that', '2', 'But', 'clop', 'do', 'with', 'ARTHUR', 'The', 'the', 'from', 'your', 'snows', 'court', 'of', 'Uther', 'length', 'So', 'needs', 'That', 'wants', 'Saxons', 'Camelot', \"'m\", 'them', 'European', 'have', \"'ve\", 'maybe', 'line', 'sun', 'where', 'Are', 'there', 'not', 'all', 'grip', 'velocity', 'got', '...', 'wind', 'must', 'Court', 'trusty', 'agree', 'right', 'south', 'warmer', 'our', 'feathers', 'Pull', 'defeator', 'knights', 'every', 'yeah', 'Supposing', 'Pendragon', 'forty-three', 'its'}\n"
     ]
    }
   ],
   "source": [
    "#Word tokenization with NLTK\n",
    "\n",
    "#Here, you'll be using the first scene of Monty Python's Holy Grail,\n",
    "#which has been pre-loaded as scene_one. Feel free to check it out in\n",
    "#the IPython Shell!\n",
    "\n",
    "#Your job in this exercise is to utilize word_tokenize and sent_tokenize\n",
    "#from nltk.tokenize to tokenize both words and sentences from Python\n",
    "#strings - in this case, the first scene of Monty Python's Holy Grail.\n",
    "\n",
    "scene_one=\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "#More regex with re.search()\n",
    "\n",
    "#In this exercise, you'll utilize re.search() and re.match() to find\n",
    "#specific tokens. Both search and match expect regex patterns, similar\n",
    "#to those you defined in an earlier exercise. You'll apply these regex\n",
    "#library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "#You have both scene_one and sentences available from the last exercise;\n",
    "#now you can use them with re.search() and re.match() to extract and\n",
    "#match more text.\n",
    "\n",
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "scene_one=\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))\n",
    "\n",
    "#################################################\n",
    "#Now that you're familiar with the basics of tokenization and\n",
    "#regular expressions, it's time to learn about more advanced tokenization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Advanced tokenization with NLTK and regex**\n",
    "___\n",
    "- Regex groups using the \"|\"\n",
    "    - OR is represented using |\n",
    "    - You can define a group using ()\n",
    "    - You can define explicit character ranges using []\n",
    "- Regex ranges and groups\n",
    "![_images/18.3.PNG](_images/18.3.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n",
      "['@datacamp', '#nlp', '#python']\n",
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "#Regex with NLTK tokenization\n",
    "\n",
    "#Twitter is a frequently used source for NLP text and tasks. In this\n",
    "#exercise, you'll build a more complex tokenizer for tweets with\n",
    "#hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer\n",
    "#class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "#Here, you're given some example tweets to parse using both\n",
    "#TweetTokenizer and regexp_tokenize from the nltk.tokenize module.\n",
    "#These example tweets have been pre-loaded into the variable tweets.\n",
    "#Feel free to explore it in the IPython Shell!\n",
    "\n",
    "#Unlike the syntax for the regex library, with nltk_tokenize() you\n",
    "#pass the pattern as the second argument.\n",
    "\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[2], pattern2)\n",
    "print(mentions_hashtags)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "#Non-ascii tokenization\n",
    "\n",
    "#In this exercise, you'll practice advanced tokenization by tokenizing\n",
    "#some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "#Here, you have access to a string called german_text, which has\n",
    "#been printed for you in the Shell. Notice the emoji and the German\n",
    "#characters!\n",
    "\n",
    "#The following modules have been pre-imported from nltk.tokenize:\n",
    "#regexp_tokenize and word_tokenize.\n",
    "\n",
    "#Unicode ranges for emoji are:\n",
    "\n",
    "#('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'),\n",
    "#('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF').\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Charting word length with NLTK**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Charting practice\n",
    "\n",
    "#Try using your new skills to find and chart the number of words per\n",
    "#line in the script using matplotlib. The Holy Grail script is loaded\n",
    "#for you, and you need to use regex to find the words per line.\n",
    "\n",
    "#Using list comprehensions here will speed up your computations. For\n",
    "#example: my_lines = [tokenize(l) for l in lines] will call a function\n",
    "#tokenize on each line in the list lines. The new transformed list\n",
    "#will be saved in the my_lines variable.\n",
    "\n",
    "#You have access to the entire script in the variable holy_grail.\n",
    "#Go for it!\n",
    "\n",
    "# Split the script into lines: lines\n",
    "#lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "#pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "#lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "#tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "#line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "#plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/18.1.svg](_images/18.1.svg)\n",
    "See you in Chapter 2, where you'll begin learning about topic identification!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Word counts with bag-of-words**\n",
    "___\n",
    "- first create tokens using tokenization\n",
    "- count all of the tokens\n",
    "- the more frequent a word, the more important it might be\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Building a Counter with bag-of-words\n",
    "\n",
    "#In this exercise, you'll build your first (in this course)\n",
    "#bag-of-words counter using a Wikipedia article, which has been\n",
    "#pre-loaded as article. Try doing the bag-of-words without looking\n",
    "#at the full article text, and guessing what the topic is! If you'd\n",
    "#like to peek at the title at the end, we've included it as\n",
    "#article_title. Note that this article text has had very little\n",
    "#preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "#word_tokenize has been imported for you.\n",
    "\n",
    "# Import Counter\n",
    "#from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "#tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "#lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "#bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "#print(bow_simple.most_common(10))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 68), ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('debugging', 40)]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Simple text preprocessing**\n",
    "___\n",
    "- Why preprocess?\n",
    "    - helps make for better input data\n",
    "        - when performing machine learning or other statistical methods\n",
    "    - examples:\n",
    "        - tokenization to create bag of words\n",
    "        - lowercasing words\n",
    "    - lemmatization/stemming\n",
    "        - shorten words to their root stems\n",
    "    - removing stop words, punctuation, or unwanted tokens\n",
    "    - good to experiment with different approaches\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Text preprocessing practice\n",
    "\n",
    "#Now, it's your turn to apply the techniques you've learned to help\n",
    "#clean up text for better NLP results. You'll need to remove stop\n",
    "#words and non-alphabetic characters, lemmatize, and perform a new\n",
    "#bag-of-words on your cleaned text.\n",
    "\n",
    "#You start with the same tokens you created in the last exercise:\n",
    "#lower_tokens. You also have the Counter class imported.\n",
    "\n",
    "# Import WordNetLemmatizer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "#alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "#no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "#lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "#bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "#print(bow.most_common(10))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [('debugging', 40), ('system', 25), ('software', 16), ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('used', 12)]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to gensim**\n",
    "___\n",
    "- What is gensim?\n",
    "    - popular open-source NLP library\n",
    "- Uses top academic models to perform complex tasks\n",
    "    - building a document or words vectors\n",
    "    - performing topic identification and document comparison\n",
    "![_images/18.4.PNG](_images/18.4.PNG)\n",
    "![_images/18.5.PNG](_images/18.5.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating and querying a corpus with gensim\n",
    "\n",
    "#It's time to apply the methods you learned in the previous video to\n",
    "#create your first gensim dictionary and corpus!\n",
    "\n",
    "#You'll use these data structures to investigate word trends and\n",
    "#potential interesting topics in your document set. To get started,\n",
    "#we have imported a few additional messy articles from Wikipedia,\n",
    "#which were preprocessed by lowercasing all words, tokenizing them,\n",
    "#and removing stop words and punctuation. These were then stored in\n",
    "#a list of document tokens called articles. You'll need to do some\n",
    "#light preprocessing and then generate the gensim dictionary and corpus.\n",
    "\n",
    "# Import Dictionary\n",
    "#from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "#dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "#computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "#print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "#corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "#print(corpus[4][:10])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    computer\n",
    "#    [(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Gensim bag-of-words\n",
    "\n",
    "#Now, you'll use your new gensim corpus and dictionary to see the\n",
    "#most common terms per document and across all documents. You can\n",
    "#use your dictionary to look up the terms. Take a guess at what the\n",
    "#topics are and feel free to explore more documents in the IPython\n",
    "#Shell!\n",
    "\n",
    "#You have access to the dictionary and corpus objects you created in\n",
    "#the previous exercise, as well as the Python defaultdict and itertools\n",
    "#to help with the creation of intermediate data structures for analysis.\n",
    "\n",
    "#defaultdict allows us to initialize a dictionary that will assign\n",
    "#a default value to non-existent keys. By supplying the argument\n",
    "#int, we are able to ensure that any non-existent keys are automatically\n",
    "#assigned a default value of 0. This makes it ideal for storing the\n",
    "#counts of words in this exercise.\n",
    "\n",
    "#itertools.chain.from_iterable() allows us to iterate through a set\n",
    "#of sequences as if they were one continuous sequence. Using this\n",
    "#function, we can easily iterate through our corpus object (which is\n",
    "#a list of lists).\n",
    "\n",
    "#The fifth document from corpus is stored in the variable doc, which\n",
    "#has been sorted in descending order.\n",
    "\n",
    "# Save the fifth document: doc\n",
    "#doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "#bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "#for word_id, word_count in bow_doc[:5]:\n",
    "#    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "# Create the defaultdict: total_word_count\n",
    "#total_word_count = defaultdict(int)\n",
    "#for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "#    total_word_count[word_id] += word_count\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    engineering 91\n",
    "#    '' 88\n",
    "#    reverse 71\n",
    "#    software 51\n",
    "#    cite 26\n",
    "#################################################\n",
    "\n",
    "# Create the defaultdict: total_word_count\n",
    "#total_word_count = defaultdict(int)\n",
    "#for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "#    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "#sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "#for word_id, word_count in sorted_word_count[:5]:\n",
    "#    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    engineering 91\n",
    "#    '' 88\n",
    "#    reverse 71\n",
    "#    software 51\n",
    "#    cite 26\n",
    "#    '' 1042\n",
    "#    computer 594\n",
    "#    software 450\n",
    "#    `` 345\n",
    "#    cite 322\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tf-idf with gensim**\n",
    "___\n",
    "- What is tf-idf?\n",
    "    - term frequency-inverse document frequency\n",
    "    - allows you to determine the most important words in each document\n",
    "    - each corpus nay have shared words beyond just stopwords\n",
    "    - these words should be down-weighted in importance\n",
    "    - ensures most common words do not show up as key words\n",
    "    - keeps document specific frequent words weighted high\n",
    "![_images/18.6.PNG](_images/18.6.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tf-idf with Wikipedia\n",
    "\n",
    "#Now it's your turn to determine new significant terms for your\n",
    "#corpus by applying gensim's tf-idf. You will again have access to\n",
    "#the same corpus and dictionary objects you created in the previous\n",
    "#exercises - dictionary, corpus, and doc. Will tf-idf make for more\n",
    "#interesting results on the document level?\n",
    "\n",
    "#TfidfModel has been imported for you from gensim.models.tfidfmodel.\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "#tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "#tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "#print(tfidf_weights[:5])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [(24, 0.0022836332291091273), (39, 0.0043409401554717324), (41, 0.008681880310943465), (55, 0.011988285029371418), (56, 0.005482756770026296)]\n",
    "#################################################\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "#sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "#for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "#    print(dictionary.get(term_id), weight)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    reverse 0.4884961428651127\n",
    "#    infringement 0.18674529210288995\n",
    "#    engineering 0.16395041814479536\n",
    "#    interoperability 0.12449686140192663\n",
    "#    reverse-engineered 0.12449686140192663\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Named Entity Recognition**\n",
    "___\n",
    "- What is Named Entity Recognition?\n",
    "    - NLP task to identify important named entities in the text\n",
    "        - people, places, organizations\n",
    "        - dates, states, works of art\n",
    "        - ... and other categories\n",
    "    - can be used alongside topic identification\n",
    "    - Who? What? When? Where?\n",
    "- nltk and the Stanford CoreNLP library\n",
    "    - the Stanford CoreNLP library:\n",
    "        - integrated into Python via nltk\n",
    "        - Java based\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#NER with NLTK\n",
    "\n",
    "#You're now going to have some fun with named-entity recognition! A\n",
    "#scraped news article has been pre-loaded into your workspace. Your\n",
    "#task is to use nltk to find the named entities in this article.\n",
    "\n",
    "#What might the article be about, given the names you found?\n",
    "\n",
    "#Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize\n",
    "#have been pre-imported.\n",
    "\n",
    "# Tokenize the article into sentences: sentences\n",
    "#sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "#token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "#pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "#chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "#for sent in chunked_sentences:\n",
    "#    for chunk in sent:\n",
    "#        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "#            print(chunk)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (NE Uber/NNP)\n",
    "#    (NE Beyond/NN)\n",
    "#    (NE Apple/NNP)\n",
    "#    (NE Uber/NNP)\n",
    "#    (NE Uber/NNP)\n",
    "#    (NE Travis/NNP Kalanick/NNP)\n",
    "#    (NE Tim/NNP Cook/NNP)\n",
    "#    (NE Apple/NNP)\n",
    "#    (NE Silicon/NNP Valley/NNP)\n",
    "#    (NE CEO/NNP)\n",
    "#    (NE Yahoo/NNP)\n",
    "#    (NE Marissa/NNP Mayer/NNP)\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Charting practice\n",
    "\n",
    "#In this exercise, you'll use some extracted named entities and\n",
    "#their groupings from a series of newspaper articles to chart the\n",
    "#diversity of named entity types in the articles.\n",
    "\n",
    "#You'll use a defaultdict called ner_categories, with keys representing\n",
    "#every named entity group type, and values to count the number of\n",
    "#each different named entity type. You have a chunked sentence list\n",
    "#called chunked_sentences similar to the last exercise, but this\n",
    "#time with non-binary category names.\n",
    "\n",
    "#You can use hasattr() to determine if each chunk has a 'label' and\n",
    "#then simply use the chunk's .label() method as the dictionary key.\n",
    "\n",
    "# Create the defaultdict: ner_categories\n",
    "#ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "#for sent in chunked_sentences:\n",
    "#    for chunk in sent:\n",
    "#        if hasattr(chunk, 'label'):\n",
    "#           ner_categories[chunk.label()] += 1\n",
    "\n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "#labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "#values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "#plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/18.2.svg](_images/18.2.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Introduction to SpaCy**\n",
    "___\n",
    "- What is SpaCy?\n",
    "    - NLP library similar to gensim, with different implementations\n",
    "    - focus on creating NLP pipelines to generate models and corpora\n",
    "    - open source, with extra libraries and tools\n",
    "        - Displacy for visualizing parse trees and interactive text\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG unroll.me\n",
      "ORG Apple\n",
      "PERSON Travis Kalanick\n",
      "PERSON Tim Cook\n",
      "ORG Apple\n",
      "CARDINAL Millions\n",
      "PERSON Uber\n",
      "LOC Silicon Valley\n",
      "ORG Yahoo\n",
      "PERSON Marissa Mayer\n",
      "MONEY 186\n"
     ]
    }
   ],
   "source": [
    "#Comparing NLTK with spaCy NER\n",
    "\n",
    "#Using the same text you used in the first exercise of this chapter,\n",
    "#you'll now see the results using spaCy's NER annotator. How will they\n",
    "#compare?\n",
    "\n",
    "#The article has been pre-loaded as article. To minimize execution\n",
    "#times, you'll be asked to specify the keyword arguments tagger=False,\n",
    "#parser=False, matcher=False when loading the spaCy model, because\n",
    "#you only care about the entity in this exercise.\n",
    "\n",
    "article = '\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic. Uber wanted to know as much as possible about the people who use its service, and those who don’t. It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies. Even if their email was notionally anonymised, this use of it was not something the users had bargained for. Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Apple’s phones even thought it is forbidden by the company.\\r\\n\\r\\n\\r\\nUber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars. Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation. Uber deny this was the intention. The punishment for this behaviour was negligible. Uber promised not to use this “greyball” software against law enforcement – one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it. Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app. Too much money was at stake for that.\\r\\n\\r\\n\\r\\nMillions of people around the world value the cheapness and convenience of Uber’s rides too much to care about the lack of drivers’ rights or pay. Many of the users themselves are not much richer than the drivers. The “sharing economy” encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires. Silicon Valley’s culture seems hostile to humane and democratic values. The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout. This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria. Yet there’s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.'\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Multilingual NER with polyglot**\n",
    "___\n",
    "- What is polyglot?\n",
    "    - NLP library which uses word vectors\n",
    "    - Why polyglot?\n",
    "        - vectors for many different languages\n",
    "        - more than 130!\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#French NER with polyglot I\n",
    "\n",
    "#In this exercise and the next, you'll use the polyglot library to\n",
    "#identify French entities. The library functions slightly differently\n",
    "#than spacy, so you'll use a few of the new things you learned in the\n",
    "#last video to display the named entity text and category.\n",
    "\n",
    "#You have access to the full article string in article. Additionally,\n",
    "#the Text class of polyglot has been imported from polyglot.text.\n",
    "\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "#txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "#for ent in txt.entities:\n",
    "#    print(ent)\n",
    "\n",
    "# Print the type of ent\n",
    "#print(type(ent))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    ['Charles', 'Cuvelliez']\n",
    "#    ['Charles', 'Cuvelliez']\n",
    "#    ['Bruxelles']\n",
    "#    ['l’IA']\n",
    "#    ['Julien', 'Maldonato']\n",
    "#    ['Deloitte']\n",
    "#    ['Ethiquement']\n",
    "#    ['l’IA']\n",
    "#    ['.']\n",
    "#    <class 'polyglot.text.Chunk'>\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#French NER with polyglot II\n",
    "\n",
    "#Here, you'll complete the work you began in the previous exercise.\n",
    "\n",
    "#Your task is to use a list comprehension to create a list of tuples,\n",
    "#in which the first element is the entity tag, and the second element\n",
    "#is the full string of the entity text.\n",
    "\n",
    "# Create the list of tuples: entities\n",
    "#entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "#print(entities)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [('I-PER', 'Charles Cuvelliez'), ('I-PER', 'Charles Cuvelliez'), ('I-ORG', 'Bruxelles'), ('I-PER', 'l’IA'), ('I-PER', 'Julien Maldonato'), ('I-ORG', 'Deloitte'), ('I-PER', 'Ethiquement'), ('I-LOC', 'l’IA'), ('I-PER', '.')]\n",
    "#################################################\n",
    "#Let's see how polyglot can handle Spanish now!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Spanish NER with polyglot\n",
    "\n",
    "#You'll continue your exploration of polyglot now with some Spanish\n",
    "#annotation. This article is not written by a newspaper, so it is\n",
    "#your first example of a more blog-like text. How do you think that\n",
    "#might compare when finding entities?\n",
    "\n",
    "#The Text object has been created as txt, and each entity has been\n",
    "#printed, as you can see in the IPython Shell.\n",
    "\n",
    "#Your specific task is to determine how many of the entities contain\n",
    "#the words \"Márquez\" or \"Gabo\" - these refer to the same person in\n",
    "#different ways!\n",
    "\n",
    "#################################################\n",
    "#['Lina']\n",
    "#['Castillo']\n",
    "#['Teresa', 'Lozano', 'Long']\n",
    "#['Universidad', 'de', 'Texas']\n",
    "#['Austin']\n",
    "#['Austin', '.']\n",
    "#['Austin', '.', 'Ella']\n",
    "#['Gabriel', 'García', 'Márquez']\n",
    "#['Gabriel', 'García', 'Márquez']\n",
    "#...\n",
    "#['Gabo']\n",
    "#['Pastrana']\n",
    "#['Ejército', 'de', 'Liberación', 'Nacional']\n",
    "#['ELN']\n",
    "#['García', 'Márquez']\n",
    "#...\n",
    "#################################################\n",
    "\n",
    "# Initialize the count variable: count\n",
    "#count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "#for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "#    if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "#        count += 1\n",
    "\n",
    "# Print count\n",
    "#print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "#percentage = count / len(txt.entities)\n",
    "#print(percentage)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    29\n",
    "#    0.29591836734693877\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Classifying fake news using supervised learning with NLP**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Building word count vectors with scikit-learn**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#CountVectorizer for text classification\n",
    "\n",
    "#It's time to begin building your text classifier! The data has been\n",
    "#loaded into a DataFrame called df. Explore it in the IPython Shell\n",
    "#to investigate what columns you can use. The .head() method is\n",
    "#particularly informative.\n",
    "\n",
    "#In this exercise, you'll use pandas alongside scikit-learn to\n",
    "#create a sparse text vectorizer you can use to train and test a\n",
    "#simple supervised model. To begin, you'll set up a CountVectorizer\n",
    "#and investigate some of its features.\n",
    "\n",
    "# Import the necessary modules\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "#print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "#y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "#count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train\n",
    "#count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test\n",
    "#count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "#print(count_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Unnamed: 0                                              title  \\\n",
    "#    0        8476                       You Can Smell Hillary’s Fear\n",
    "#    1       10294  Watch The Exact Moment Paul Ryan Committed Pol...\n",
    "#    2        3608        Kerry to go to Paris in gesture of sympathy\n",
    "#    3       10142  Bernie supporters on Twitter erupt in anger ag...\n",
    "#    4         875   The Battle of New York: Why This Primary Matters\n",
    "#\n",
    "#                                                    text label\n",
    "#    0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE\n",
    "#    1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE\n",
    "#    2  U.S. Secretary of State John F. Kerry said Mon...  REAL\n",
    "#    3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE\n",
    "#    4  It's primary day in New York and front-runners...  REAL\n",
    "#    ['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TfidfVectorizer for text classification\n",
    "\n",
    "#Similar to the sparse CountVectorizer created in the previous\n",
    "#exercise, you'll work on creating tf-idf vectors for your documents.\n",
    "#You'll set up a TfidfVectorizer and investigate some of its features.\n",
    "\n",
    "#In this exercise, you'll use pandas and sklearn along with the same\n",
    "#X_train, y_train and X_test, y_test DataFrames and Series you created\n",
    "#in the last exercise.\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "#tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train\n",
    "#tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test\n",
    "#tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "#print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "#print(tfidf_train.A[:5])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    ['00', '000', '001', '008s', '00am', '00pm', '01', '01am', '02', '024']\n",
    "#    [[0.         0.01928563 0.         ... 0.         0.         0.        ]\n",
    "#     [0.         0.         0.         ... 0.         0.         0.        ]\n",
    "#     [0.         0.02895055 0.         ... 0.         0.         0.        ]\n",
    "#     [0.         0.03056734 0.         ... 0.         0.         0.        ]\n",
    "#     [0.         0.         0.         ... 0.         0.         0.        ]]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Inspecting the vectors\n",
    "\n",
    "#To get a better idea of how the vectors work, you'll investigate\n",
    "#them by converting them into pandas DataFrames.\n",
    "\n",
    "#Here, you'll use the same data structures you created in the previous\n",
    "#two exercises (count_train, count_vectorizer, tfidf_train,\n",
    "#tfidf_vectorizer) as well as pandas, which is imported as pd.\n",
    "\n",
    "# Create the CountVectorizer DataFrame: count_df\n",
    "#count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "#tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "#print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "#print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "#difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "#print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "#print(count_df.equals(tfidf_df))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       000  00am  0600  10  100  107  11  110  1100  12    ...      younger  \\\n",
    "#    0    0     0     0   0    0    0   0    0     0   0    ...            0\n",
    "#    1    0     0     0   3    0    0   0    0     0   0    ...            0\n",
    "#    2    0     0     0   0    0    0   0    0     0   0    ...            0\n",
    "#    3    0     0     0   0    0    0   0    0     0   0    ...            1\n",
    "#    4    0     0     0   0    0    0   0    0     0   0    ...            0\n",
    "#\n",
    "#       youth  youths  youtube  ypg  yuan  zawahiri  zeitung  zero  zerohedge\n",
    "#    0      0       0        0    0     0         0        0     1          0\n",
    "#    1      0       0        0    0     0         0        0     0          0\n",
    "#    2      0       0        0    0     0         0        0     0          0\n",
    "#    3      0       0        0    0     0         0        0     0          0\n",
    "#    4      0       0        0    0     0         0        0     0          0\n",
    "#\n",
    "#    [5 rows x 5111 columns]\n",
    "#\n",
    "#       000  00am  0600        10  100  107   11  110  1100   12    ...      \\\n",
    "#    0  0.0   0.0   0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0    ...\n",
    "#    1  0.0   0.0   0.0  0.105636  0.0  0.0  0.0  0.0   0.0  0.0    ...\n",
    "#    2  0.0   0.0   0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0    ...\n",
    "#    3  0.0   0.0   0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0    ...\n",
    "#    4  0.0   0.0   0.0  0.000000  0.0  0.0  0.0  0.0   0.0  0.0    ...\n",
    "#\n",
    "#        younger  youth  youths  youtube  ypg  yuan  zawahiri  zeitung      zero  \\\n",
    "#    0  0.000000    0.0     0.0      0.0  0.0   0.0       0.0      0.0  0.033579\n",
    "#    1  0.000000    0.0     0.0      0.0  0.0   0.0       0.0      0.0  0.000000\n",
    "#    2  0.000000    0.0     0.0      0.0  0.0   0.0       0.0      0.0  0.000000\n",
    "#    3  0.015175    0.0     0.0      0.0  0.0   0.0       0.0      0.0  0.000000\n",
    "#    4  0.000000    0.0     0.0      0.0  0.0   0.0       0.0      0.0  0.000000\n",
    "#\n",
    "#       zerohedge\n",
    "#    0        0.0\n",
    "#    1        0.0\n",
    "#    2        0.0\n",
    "#    3        0.0\n",
    "#    4        0.0\n",
    "#\n",
    "#    [5 rows x 5111 columns]\n",
    "#    set()\n",
    "#    False\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training and testing a classification model with scikit-learn**\n",
    "___\n",
    "- Naive Bayes classifier\n",
    "    - Naive Bayes Model\n",
    "        - commonly used for testing NLP classification problems\n",
    "        - basis in probability\n",
    "    - given a particular piece of data, how likely is a particular outcome?\n",
    "    - examples:\n",
    "        - if the plot has a spaceship, how likely is it to be sci-fi?\n",
    "        - given a spaceship **and** an alien, how likely **now** is it sci-fi?\n",
    "    - each word from CountVectorizer acts as a feature\n",
    "    - naive Bayes: simple and effective\n",
    "- Confusion matrix is used to visualize  distribution of labels.\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training and testing the \"fake news\" model with CountVectorizer\n",
    "\n",
    "#Now it's your turn to train the \"fake news\" model using the\n",
    "#features you identified and extracted. In this first exercise you'll\n",
    "#train and test a Naive Bayes model using the CountVectorizer data.\n",
    "\n",
    "#The training and test sets have been created, and count_vectorizer,\n",
    "#count_train, and count_test have been computed.\n",
    "\n",
    "# Import the necessary modules\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "#nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "#nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "#pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "#score = metrics.accuracy_score(y_test, pred)\n",
    "#print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "#cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "#print(cm)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.893352462936394\n",
    "#    [[ 865  143]\n",
    "#     [  80 1003]]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training and testing the \"fake news\" model with TfidfVectorizer\n",
    "\n",
    "#Now that you have evaluated the model using the CountVectorizer,\n",
    "#you'll do the same using the TfidfVectorizer with a Naive Bayes\n",
    "#model.\n",
    "\n",
    "#The training and test sets have been created, and tfidf_vectorizer,\n",
    "#tfidf_train, and tfidf_test have been computed. Additionally,\n",
    "#MultinomialNB and metrics have been imported from, respectively,\n",
    "#sklearn.naive_bayes and sklearn.\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "#nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "#nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "#pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "#score = metrics.accuracy_score(y_test, pred)\n",
    "#print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "#cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "#print(cm)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.8565279770444764\n",
    "#    [[ 739  269]\n",
    "#     [  31 1052]]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Simple NLP, complex problems**\n",
    "___\n",
    "- Translation problems (German-English translation of 'economics')\n",
    "![_images/18.7.PNG](_images/18.7.PNG)\n",
    "\n",
    "- Sentiment analysis (snark, sarcasm, contextual negation, location context)\n",
    "![_images/18.8.PNG](_images/18.8.PNG)\n",
    "\n",
    "- Language biases (gendered languages)\n",
    "![_images/18.9.PNG](_images/18.9.PNG)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Improving your model\n",
    "\n",
    "#Your job in this exercise is to test a few different alpha levels\n",
    "#using the Tfidf vectors to determine if there is a better\n",
    "#performing combination.\n",
    "\n",
    "#The training and test sets have been created, and tfidf_vectorizer,\n",
    "#tfidf_train, and tfidf_test have been computed.\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "#alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "#def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "#    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "#    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "#    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "#    score = metrics.accuracy_score(y_test, pred)\n",
    "#    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "#for alpha in alphas:\n",
    "#    print('Alpha: ', alpha)\n",
    "#    print('Score: ', train_and_predict(alpha))\n",
    "#    print()\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Alpha:  0.0\n",
    "#    Score:  0.8813964610234337\n",
    "#\n",
    "#    Alpha:  0.1\n",
    "#    Score:  0.8976566236250598\n",
    "#\n",
    "#    Alpha:  0.2\n",
    "#    Score:  0.8938307030129125\n",
    "#\n",
    "#    Alpha:  0.30000000000000004\n",
    "#    Score:  0.8900047824007652\n",
    "#\n",
    "#    Alpha:  0.4\n",
    "#    Score:  0.8857006217120995\n",
    "#\n",
    "#    Alpha:  0.5\n",
    "#    Score:  0.8842659014825442\n",
    "#\n",
    "#    Alpha:  0.6000000000000001\n",
    "#    Score:  0.874701099952176\n",
    "#\n",
    "#    Alpha:  0.7000000000000001\n",
    "#    Score:  0.8703969392635102\n",
    "#\n",
    "#    Alpha:  0.8\n",
    "#    Score:  0.8660927785748446\n",
    "#\n",
    "#    Alpha:  0.9\n",
    "#    Score:  0.8589191774270684\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Inspecting your model\n",
    "\n",
    "#Now that you have built a \"fake news\" classifier, you'll investigate\n",
    "#what it has learned. You can map the important vector weights back\n",
    "#to actual words using some simple inspection techniques.\n",
    "\n",
    "#You have your well performing tfidf Naive Bayes classifier\n",
    "#available as nb_classifier, and the vectors as tfidf_vectorizer.\n",
    "\n",
    "# Get the class labels: class_labels\n",
    "#class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "#feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "#feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "#print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "#print(class_labels[1], feat_with_weights[-20:])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    FAKE [(-12.641778440826338, '0000'), (-12.641778440826338, '000035'), (-12.641778440826338, '0001'), (-12.641778440826338, '0001pt'), (-12.641778440826338, '000km'), (-12.641778440826338, '0011'), (-12.641778440826338, '006s'), (-12.641778440826338, '007'), (-12.641778440826338, '007s'), (-12.641778440826338, '008s'), (-12.641778440826338, '0099'), (-12.641778440826338, '00am'), (-12.641778440826338, '00p'), (-12.641778440826338, '00pm'), (-12.641778440826338, '014'), (-12.641778440826338, '015'), (-12.641778440826338, '018'), (-12.641778440826338, '01am'), (-12.641778440826338, '020'), (-12.641778440826338, '023')]\n",
    "#    REAL [(-6.790929954967984, 'states'), (-6.765360557845786, 'rubio'), (-6.751044290367751, 'voters'), (-6.701050756752027, 'house'), (-6.695547793099875, 'republicans'), (-6.6701912490429685, 'bush'), (-6.661945235816139, 'percent'), (-6.589623788689862, 'people'), (-6.559670340096453, 'new'), (-6.489892292073901, 'party'), (-6.452319082422527, 'cruz'), (-6.452076515575875, 'state'), (-6.397696648238072, 'republican'), (-6.376343060363355, 'campaign'), (-6.324397735392007, 'president'), (-6.2546017970213645, 'sanders'), (-6.144621899738043, 'obama'), (-5.756817248152807, 'clinton'), (-5.596085785733112, 'said'), (-5.357523914504495, 'trump')]\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}