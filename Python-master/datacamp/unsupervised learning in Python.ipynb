{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Unsupervised learning**\n",
    "___\n",
    "- Unsupervised learning finds patterns in data\n",
    "- Dimension = number of features\n",
    "- k-means clustering\n",
    "    - finds clusters of samples\n",
    "    - number of clusters must be specified\n",
    "    - implemented in sklearn (\"scikit-learn\")\n",
    "    - new samples can be assigned to existing clusters\n",
    "        - k-means remembers the mean of each cluster (the \"centroids\")\n",
    "        - finds the nearest centroid to each new sample\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#clustering 2D points\n",
    "\n",
    "# Import pyplot\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Import KMeans\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "#model = KMeans(n_clusters = 3)\n",
    "\n",
    "# Fit model to points\n",
    "#model.fit(points)\n",
    "\n",
    "# Determine the cluster labels of new_points: labels\n",
    "#labels = model.predict(new_points)\n",
    "\n",
    "# Print cluster labels of new_points\n",
    "#print(labels)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [1 2 0 1 2 1 2 2 2 0 1 2 2 0 0 2 0 0 2 2 0 2 1 2 1 0 2 0 0 1 1 2 2 2 0 1 2\n",
    "#     2 1 2 0 1 1 0 1 2 0 0 2 2 2 2 0 0 1 1 0 0 0 1 1 2 2 2 1 2 0 2 1 0 1 1 1 2\n",
    "#     1 0 0 1 2 0 1 0 1 2 0 2 0 1 2 2 2 1 2 2 1 0 0 0 0 1 2 1 0 0 1 1 2 1 0 0 1\n",
    "#     0 0 0 2 2 2 2 0 0 2 1 2 0 2 1 0 2 0 0 2 0 2 0 1 2 1 1 2 0 1 2 1 1 0 2 2 1\n",
    "#     0 1 0 2 1 0 0 1 0 2 2 0 2 0 0 2 2 1 2 2 0 1 0 1 1 2 1 2 2 1 1 0 1 1 1 0 2\n",
    "#     2 1 0 1 0 0 2 2 2 1 2 2 2 0 0 1 2 1 1 1 0 2 2 2 2 2 2 0 0 2 0 0 0 0 2 0 0\n",
    "#     2 2 1 0 1 1 0 1 0 1 0 2 2 0 2 2 2 0 1 1 0 2 2 0 2 0 0 2 0 0 1 0 1 1 1 2 0\n",
    "#     0 0 1 2 1 0 1 0 0 2 1 1 1 0 2 2 2 1 2 0 0 2 1 1 0 1 1 0 1 2 1 0 0 0 0 2 0\n",
    "#     0 2 2 1]\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "#xs = new_points[:,0]\n",
    "#ys = new_points[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "#plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "#centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "#centroids_x = centroids[:,0]\n",
    "#centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "#plt.scatter(centroids_x, centroids_y, marker ='D', s=50)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.1.svg](_images/8.1.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluating a clustering**\n",
    "___\n",
    "- compare against known clustering\n",
    "    - using crosstabs\n",
    "- inertia measures clustering quality\n",
    "    - how spread out the clusters are (lower is better)\n",
    "    - distance from each sample to centroid of its cluster\n",
    "    - best choice is elbow in inertia plot\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How many clusters?\n",
    "\n",
    "# Import pyplot\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Import KMeans\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "#ks = range(1, 6)\n",
    "#inertias = []\n",
    "\n",
    "#for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "#    model = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit model to samples\n",
    "#    model.fit(samples)\n",
    "\n",
    "    # Append the inertia to the list of inertias\n",
    "#    inertias.append(model.inertia_)\n",
    "\n",
    "# Plot ks vs inertias\n",
    "#plt.plot(ks, inertias, '-o')\n",
    "#plt.xlabel('number of clusters, k')\n",
    "#plt.ylabel('inertia')\n",
    "#plt.xticks(ks)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.2.svg](_images/8.2.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Evaluating clusters\n",
    "\n",
    "#import libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Create a KMeans model with 3 clusters: model\n",
    "#model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "#labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "#df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "#ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "#print(ct)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    varieties  Canadian wheat  Kama wheat  Rosa wheat\n",
    "#    labels\n",
    "#    0                       0           1          60\n",
    "#    1                      68           9           0\n",
    "#    2                       2          60          10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Transforming features for better clusterings**\n",
    "___\n",
    "- when features have different variances, clustering will be inaccurate.\n",
    "    - for K Means clustering, variance = influence\n",
    "- StandardScaler from sklearn.preprocessing transforms each feature to have mean 0 and variance 1\n",
    "    - StandardScaler has fit() and transform()\n",
    "    - KMeans has fit() and predict()\n",
    "- Normalizer rescales samples (rather than features) independently of the other\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#scaling data for clustering\n",
    "\n",
    "# Perform the necessary imports\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "# Create scaler: scaler\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "#kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "#pipeline = make_pipeline(scaler, kmeans)\n",
    "\n",
    "# Fit the pipeline to samples\n",
    "#pipeline.fit(samples)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "#labels = pipeline.predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and species as columns: df\n",
    "#df = pd.DataFrame({'labels' : labels, 'species' : species})\n",
    "\n",
    "# Create crosstab: ct\n",
    "#ct = pd.crosstab(df['labels'], df['species'])\n",
    "\n",
    "# Display ct\n",
    "#print(ct)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    species  Bream  Pike  Roach  Smelt\n",
    "#    labels\n",
    "#    0            0     0      0     13\n",
    "#    1           33     0      1      0\n",
    "#    2            0    17      0      0\n",
    "#    3            1     0     19      1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Clustering stocks using KMeans and Normalizer\n",
    "\n",
    "# Perform the necessary imports\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#from sklearn.preprocessing import Normalizer\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "#normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "#kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "#pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "#pipeline.fit(movements)\n",
    "\n",
    "# Import pandas\n",
    "#import pandas as pd\n",
    "\n",
    "# Predict the cluster labels: labels\n",
    "#labels = pipeline.predict(movements)\n",
    "\n",
    "# Create a DataFrame aligning labels and companies: df\n",
    "#df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "#print(df.sort_values('labels'))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        labels                           companies\n",
    "#    59       0                               Yahoo\n",
    "#    15       0                                Ford\n",
    "#    35       0                            Navistar\n",
    "#    26       1                      JPMorgan Chase\n",
    "#    16       1                   General Electrics\n",
    "#    58       1                               Xerox\n",
    "#    11       1                               Cisco\n",
    "#    18       1                       Goldman Sachs\n",
    "#    20       1                          Home Depot\n",
    "#    5        1                     Bank of America\n",
    "#    3        1                    American express\n",
    "#    55       1                         Wells Fargo\n",
    "#    1        1                                 AIG\n",
    "#    38       2                               Pepsi\n",
    "#    40       2                      Procter Gamble\n",
    "#    28       2                           Coca Cola\n",
    "#    27       2                      Kimberly-Clark\n",
    "#    9        2                   Colgate-Palmolive\n",
    "#    54       3                            Walgreen\n",
    "#    36       3                    Northrop Grumman\n",
    "#    29       3                     Lookheed Martin\n",
    "#    4        3                              Boeing\n",
    "#    0        4                               Apple\n",
    "#    47       4                            Symantec\n",
    "#    33       4                           Microsoft\n",
    "#    32       4                                  3M\n",
    "#    31       4                           McDonalds\n",
    "#    30       4                          MasterCard\n",
    "#    50       4  Taiwan Semiconductor Manufacturing\n",
    "#    14       4                                Dell\n",
    "#    17       4                     Google/Alphabet\n",
    "#    24       4                               Intel\n",
    "#    23       4                                 IBM\n",
    "#    2        4                              Amazon\n",
    "#    51       4                   Texas instruments\n",
    "#    43       4                                 SAP\n",
    "#    45       5                                Sony\n",
    "#    48       5                              Toyota\n",
    "#    21       5                               Honda\n",
    "#    22       5                                  HP\n",
    "#    34       5                          Mitsubishi\n",
    "#    7        5                               Canon\n",
    "#    56       6                            Wal-Mart\n",
    "#    57       7                               Exxon\n",
    "#    44       7                        Schlumberger\n",
    "#    8        7                         Caterpillar\n",
    "#    10       7                      ConocoPhillips\n",
    "#    12       7                             Chevron\n",
    "#    13       7                   DuPont de Nemours\n",
    "#    53       7                       Valero Energy\n",
    "#    39       8                              Pfizer\n",
    "#    41       8                       Philip Morris\n",
    "#    25       8                   Johnson & Johnson\n",
    "#    49       9                               Total\n",
    "#    46       9                      Sanofi-Aventis\n",
    "#    37       9                            Novartis\n",
    "#    42       9                   Royal Dutch Shell\n",
    "#    19       9                     GlaxoSmithKline\n",
    "#    52       9                            Unilever\n",
    "#    6        9            British American Tobacco"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualizing hierarchies**\n",
    "___\n",
    "- t-SNE\n",
    "    - creates a 2D map of a dataset\n",
    "- Hierarchical clustering\n",
    "    - 2D array of scores\n",
    "    - dendrogram\n",
    "    - number of operations = # samples compared - 1\n",
    "    - agglomerative clustering\n",
    "        - each row begins in a separate cluster, at each step the two closest clusters are merged\n",
    "        - continues until all rows are in a single cluster\n",
    "    - divisive clustering\n",
    "        - opposite to agglomerative clustering\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Hierarchical clustering of grain data\n",
    "\n",
    "# Perform the necessary imports\n",
    "#from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "#mergings = linkage(samples, method='complete')\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "#dendrogram(mergings,\n",
    "#           labels=varieties,\n",
    "#           leaf_rotation=90,\n",
    "#           leaf_font_size=6,\n",
    "#)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.3.svg](_images/8.3.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Hierarchical clustering of stock data with normalize()\n",
    "\n",
    "# Perform the necessary imports\n",
    "#from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the movements: normalized_movements\n",
    "#normalized_movements = normalize(movements)\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "#mergings = linkage(normalized_movements, method='complete')\n",
    "\n",
    "# Plot the dendrogram\n",
    "#dendrogram(mergings,\n",
    "#            labels=companies,\n",
    "#            leaf_rotation=90,\n",
    "#            leaf_font_size=6\n",
    "#)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.4.svg](_images/8.4.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cluster labels in hierarchical clustering**\n",
    "___\n",
    "- cluster labels from intermediate stages can be recovered and crosstabulated\n",
    "- y axis of a dendrogram indicates height\n",
    "    - distance between merging clusters\n",
    "    - linkage method is called using fcluster() in scipy.cluster.hierarchy\n",
    "- linkage\n",
    "    - **complete** - distance between clusters is the distance between the furthest points of the clusters\n",
    "    - **single** - distance between clusters is the distance between the closest points of the clusters\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#single linkage, different dendrogram\n",
    "\n",
    "# Perform the necessary imports\n",
    "#import matplotlib.pyplot as plt\n",
    "#from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "#mergings = linkage(samples, method='single')\n",
    "\n",
    "# Plot the dendrogram\n",
    "#dendrogram(mergings,\n",
    "#            labels=country_names,\n",
    "#            leaf_rotation=90,\n",
    "#            leaf_font_size=6\n",
    "#)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.5.svg](_images/8.5.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#extracting cluster labels\n",
    "\n",
    "# Perform the necessary imports\n",
    "#import matplotlib.pyplot as plt\n",
    "#from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "#import pandas as pd\n",
    "#from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "#mergings = linkage(samples, method='single')\n",
    "\n",
    "# Use fcluster to extract labels: labels\n",
    "#labels = fcluster(mergings, t=6, criterion='distance')\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "#df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "#ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "#print(ct)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    varieties  Canadian wheat  Kama wheat  Rosa wheat\n",
    "#    labels\n",
    "#    1                      14           3           0\n",
    "#    2                       0           0          14\n",
    "#    3                       0          11           0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**t-SNE for 2-dimensional maps**\n",
    "___\n",
    "\"t-distributed stochastic neighbor embedding\"\n",
    "- maps samples to 2D or 3D space\n",
    "- map approximately preserves nearness of samples\n",
    "- great for inspecting datasets\n",
    "- only has fit_transform() method\n",
    "- t-SNE learning rate - values between 50 and 200\n",
    "    - if points are clustered together, it is a bad value\n",
    "- axis values are not interpretable\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#t-SNE visualization of grain dataset\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TSNE\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "#model = TSNE(learning_rate=200)\n",
    "\n",
    "# Apply fit_transform to samples: tsne_features\n",
    "#tsne_features = model.fit_transform(samples)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "#xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "#ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot, coloring by variety_numbers\n",
    "#plt.scatter(xs, ys, c=variety_numbers)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.6.svg](_images/8.6.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#t-SNE map of the stock market\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Import TSNE\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "#model = TSNE(learning_rate=50)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "#tsne_features = model.fit_transform(normalized_movements)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "#xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "#ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "#plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "# Annotate the points\n",
    "#for x, y, company in zip(xs, ys, companies):\n",
    "#    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.7.svg](_images/8.7.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualizing the PCA transformation**\n",
    "___\n",
    "Dimension reduction - more efficient storage and computation\n",
    "- remove less-informative \"noise\" features\n",
    "- PCA = principal component analysis\n",
    "    - rotates data samples to be aligned with axes\n",
    "    - shifts data samples so they have mean 0\n",
    "    - no information is lost\n",
    "    - PCA follows the fit/transform pattern\n",
    "    - PCA de-correlates samples that are correlated\n",
    "- Principal components\n",
    "    - indicates directions of variance, and aligns with axes\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#correlated data in nature\n",
    "\n",
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assign the 0th column of grains: width\n",
    "#width = grains[:,0]\n",
    "\n",
    "# Assign the 1st column of grains: length\n",
    "#length = grains[:,1]\n",
    "\n",
    "# Scatter plot width vs length\n",
    "#plt.scatter(width, length)\n",
    "#plt.axis('equal')\n",
    "#plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation\n",
    "#correlation, pvalue = pearsonr(width, length)\n",
    "\n",
    "# Display the correlation\n",
    "#print(correlation)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.8604149377143466"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.8.svg](_images/8.8.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decorrelating the grain measurements with PCA\n",
    "\n",
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA instance: model\n",
    "#model = PCA()\n",
    "\n",
    "# Apply the fit_transform method of model to grains: pca_features\n",
    "#pca_features = model.fit_transform(grains)\n",
    "\n",
    "# Assign 0th column of pca_features: xs\n",
    "#xs = pca_features[:,0]\n",
    "\n",
    "# Assign 1st column of pca_features: ys\n",
    "#ys = pca_features[:,1]\n",
    "\n",
    "# Scatter plot xs vs ys\n",
    "#plt.scatter(xs, ys)\n",
    "#plt.axis('equal')\n",
    "#plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation of xs and ys\n",
    "#correlation, pvalue = pearsonr(xs, ys)\n",
    "\n",
    "# Display the correlation\n",
    "#print(correlation)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    2.5478751053409354e-17"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.9.svg](_images/8.9.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Intrinsic dimension**\n",
    "___\n",
    "Intrinsic dimension = number of features needed to approximate the dataset\n",
    "- What is the most compact representation of the samples?\n",
    "- Can be approximated with PCA\n",
    "- Intrinsic dimension = number of PCA features with **significant variance**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#the first principal component - direction in which grain data varies the most\n",
    "\n",
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Make a scatter plot of the untransformed points\n",
    "#plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "#model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "#model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "#mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "#first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "#plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.10.svg](_images/8.10.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Variance of the PCA features\n",
    "\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "#pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "#pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "#pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "#features = range(pca.n_components_)\n",
    "#plt.bar(features, pca.explained_variance_)\n",
    "#plt.xlabel('PCA feature')\n",
    "#plt.ylabel('variance')\n",
    "#plt.xticks(features)\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.11.svg](_images/8.11.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dimension reduction with PCA**\n",
    "___\n",
    "- specify how many features to keep (i.e. intrinsic dimensions)\n",
    "- alternatives to PCA\n",
    "    - word frequency arrays - row = document, column = dictionary word\n",
    "    - scipy.sparse.csr_matrix used for sparse arrays, where most entries = 0\n",
    "    - remembers only non-zero entries\n",
    "    - TruncatedSVD used instead of PCA on sparse arrays (same library/behaviors)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dimension reduction of the fish measurements from 6 to 2\n",
    "\n",
    "# Import PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model with 2 components: pca\n",
    "#pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "#pca.fit(scaled_samples)\n",
    "\n",
    "# Transform the scaled samples: pca_features\n",
    "#pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "# Print the shape of pca_features\n",
    "#print(pca_features.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    (85, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51785612 0.         0.         0.68091856 0.51785612 0.        ]\n",
      " [0.         0.         0.51785612 0.         0.51785612 0.68091856]\n",
      " [0.51785612 0.68091856 0.51785612 0.         0.         0.        ]]\n",
      "['cats', 'chase', 'dogs', 'meow', 'say', 'woof']\n"
     ]
    }
   ],
   "source": [
    "#A tf-idf word-frequency array\n",
    "documents = ['cats say meow', 'dogs say woof', 'dogs chase cats']\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()\n",
    "\n",
    "# Print words\n",
    "print(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Clustering Wikipedia\n",
    "#from https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/\n",
    "\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Create a TruncatedSVD instance: svd\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)\n",
    "\n",
    "# Fit the pipeline to word freqency array articles\n",
    "#pipeline.fit(articles)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "#labels = pipeline.predict(articles)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "#df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "#print(df.sort_values('label'))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        label                                        article\n",
    "#    59      0                                    Adam Levine\n",
    "#    57      0                          Red Hot Chili Peppers\n",
    "#    56      0                                       Skrillex\n",
    "#    55      0                                  Black Sabbath\n",
    "#    54      0                                 Arctic Monkeys\n",
    "#    53      0                                   Stevie Nicks\n",
    "#    52      0                                     The Wanted\n",
    "#    51      0                                     Nate Ruess\n",
    "#    50      0                                   Chad Kroeger\n",
    "#    58      0                                         Sepsis\n",
    "#    30      1                  France national football team\n",
    "#    31      1                              Cristiano Ronaldo\n",
    "#    32      1                                   Arsenal F.C.\n",
    "#    33      1                                 Radamel Falcao\n",
    "#    37      1                                       Football\n",
    "#    35      1                Colombia national football team\n",
    "#    36      1              2014 FIFA World Cup qualification\n",
    "#    38      1                                         Neymar\n",
    "#    39      1                                  Franck Ribéry\n",
    "#    34      1                             Zlatan Ibrahimović\n",
    "#    26      2                                     Mila Kunis\n",
    "#    28      2                                  Anne Hathaway\n",
    "#    27      2                                 Dakota Fanning\n",
    "#    25      2                                  Russell Crowe\n",
    "#    29      2                               Jennifer Aniston\n",
    "#    23      2                           Catherine Zeta-Jones\n",
    "#    22      2                              Denzel Washington\n",
    "#    21      2                             Michael Fassbender\n",
    "#    20      2                                 Angelina Jolie\n",
    "#    24      2                                   Jessica Biel\n",
    "#    10      3                                 Global warming\n",
    "#    11      3       Nationally Appropriate Mitigation Action\n",
    "#    13      3                               Connie Hedegaard\n",
    "#    14      3                                 Climate change\n",
    "#    12      3                                   Nigel Lawson\n",
    "#    16      3                                        350.org\n",
    "#    17      3  Greenhouse gas emissions by the United States\n",
    "#    18      3  2010 United Nations Climate Change Conference\n",
    "#    19      3  2007 United Nations Climate Change Conference\n",
    "#    15      3                                 Kyoto Protocol\n",
    "#    8       4                                        Firefox\n",
    "#    1       4                                 Alexa Internet\n",
    "#    2       4                              Internet Explorer\n",
    "#    3       4                                    HTTP cookie\n",
    "#    4       4                                  Google Search\n",
    "#    5       4                                         Tumblr\n",
    "#    6       4                    Hypertext Transfer Protocol\n",
    "#    7       4                                  Social search\n",
    "#    49      4                                       Lymphoma\n",
    "#    42      4                                    Doxycycline\n",
    "#    47      4                                          Fever\n",
    "#    46      4                                     Prednisone\n",
    "#    44      4                                           Gout\n",
    "#    43      4                                       Leukemia\n",
    "#    9       4                                       LinkedIn\n",
    "#    48      4                                     Gabapentin\n",
    "#    0       4                                       HTTP 404\n",
    "#    45      5                                    Hepatitis C\n",
    "#    41      5                                    Hepatitis B\n",
    "#    40      5                                    Tonsillitis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Non-negative matrix factorization (NMF)**\n",
    "___\n",
    "- NMF models are interpretable, unlike PCA\n",
    "- **requires all sample features to be non-negative** (>= 0)\n",
    "- NMF expresses images as combinations of patterns\n",
    "- scikit-learn using fit()/transform() pattern\n",
    "    - must always specify number of components\n",
    "    - works with NumPy arrays and csr_matrix sparse arrays\n",
    "- sample reconstruction\n",
    "    - multiply components by feature values, and add up\n",
    "    - can be expressed as product of matrices\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#NMF applied to Wikipedia articles & NMF features\n",
    "\n",
    "# Import NMF\n",
    "#from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "#model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "#model.fit(articles)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "#nmf_features = model.transform(articles)\n",
    "\n",
    "# Print the NMF features\n",
    "#print(nmf_features)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
    "#      0.00000000e+00 4.40623130e-01]\n",
    "#     [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
    "#      0.00000000e+00 5.66807830e-01]\n",
    "#     [3.82006369e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
    "#      0.00000000e+00 3.98789405e-01]\n",
    "#     [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
    "#      0.00000000e+00 3.81876582e-01]\n",
    "#    [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
    "#    ...\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame: df\n",
    "#df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "# Print the row for 'Anne Hathaway'\n",
    "#print(df.loc['Anne Hathaway'])\n",
    "\n",
    "# Print the row for 'Denzel Washington'\n",
    "#print(df.loc['Denzel Washington'])\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    0.003845\n",
    "#    1    0.000000\n",
    "#    2    0.000000\n",
    "#    3    0.575711\n",
    "#    4    0.000000\n",
    "#    5    0.000000\n",
    "#    Name: Anne Hathaway, dtype: float64\n",
    "#    0    0.000000\n",
    "#    1    0.005601\n",
    "#    2    0.000000\n",
    "#    3    0.422380\n",
    "#    4    0.000000\n",
    "#    5    0.000000\n",
    "#    Name: Denzel Washington, dtype: float64\n",
    "\n",
    "#Notice that for both actors, the NMF feature 3 has by far the highest\n",
    "#value. This means that both articles are reconstructed using mainly\n",
    "#the 3rd NMF component. In the next video, you'll see why: NMF components\n",
    "#represent topics (for instance, acting!)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NMF learns interpretable parts**\n",
    "___\n",
    "- NMF components\n",
    "    - represent topics\n",
    "    - for images, are parts of images\n",
    "        - Grayscale is 0(totally black) to 1(totally white)\n",
    "        - each row is a flattened array for an image\n",
    "            - use reshape() method to recover unflattened array\n",
    "            - use matplotlib.pyplot imshow() method to recreate image\n",
    "        - each column corresponds to a pixel\n",
    "- NMF features\n",
    "    - combine topics into documents\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#NMF learns topics of documents\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: components_df\n",
    "#components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "#print(components_df.shape)\n",
    "\n",
    "# Select row 3: component\n",
    "#component = components_df.iloc[3]\n",
    "\n",
    "# Print result of nlargest\n",
    "#print(component.nlargest())\n",
    "\n",
    "#################################################\n",
    "#(6, 13125)\n",
    "#film       0.627877\n",
    "#award      0.253131\n",
    "#starred    0.245284\n",
    "#role       0.211451\n",
    "#actress    0.186398\n",
    "#Name: 3, dtype: float64\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Explore the LED digits dataset\n",
    "\n",
    "#Firstly, explore the image dataset and see how it is encoded as an array.\n",
    "#You are given 100 images as a 2D array samples, where each row represents\n",
    "#a single 13x8 image. The images in your dataset are pictures of a LED\n",
    "#digital display.\n",
    "\n",
    "# Import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Select the 0th row: digit\n",
    "#digit = samples[0,:]\n",
    "\n",
    "# Print digit\n",
    "#print(digit)\n",
    "\n",
    "# Reshape digit to a 13x8 array: bitmap\n",
    "#bitmap = digit.reshape(13,8)\n",
    "\n",
    "# Print bitmap\n",
    "#print(bitmap)\n",
    "\n",
    "# Use plt.imshow to display bitmap\n",
    "#plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
    "#     0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
    "#     0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
    "#     0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "#     0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "#    [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "#     [0. 0. 1. 1. 1. 1. 0. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "#     [0. 0. 0. 0. 0. 0. 0. 0.]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.12.svg](_images/8.12.svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#NMF learns the parts of images\n",
    "\n",
    "# Import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_as_image(sample):\n",
    "    bitmap = sample.reshape((13, 8))\n",
    "    plt.figure()\n",
    "    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF model: model\n",
    "model = NMF(7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "#features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "#for component in model.components_:\n",
    "#    show_as_image(component)\n",
    "\n",
    "# Assign the 0th row of features: digit_features\n",
    "#digit_features = features[0,:]\n",
    "\n",
    "# Print digit_features\n",
    "#print(digit_features)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [4.76823559e-01 0.00000000e+00 0.00000000e+00 5.90605054e-01\n",
    "#     4.81559442e-01 0.00000000e+00 7.37557191e-16]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.13.svg](_images/8.13.svg)\n",
    "![_images/8.14.svg](_images/8.14.svg)\n",
    "![_images/8.15.svg](_images/8.15.svg)\n",
    "![_images/8.16.svg](_images/8.16.svg)\n",
    "![_images/8.17.svg](_images/8.17.svg)\n",
    "![_images/8.18.svg](_images/8.18.svg)\n",
    "![_images/8.19.svg](_images/8.19.svg)\n",
    "\n",
    "notice how NMF has expressed the digit as a sum of the components!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PCA doesn't learn parts\n",
    "\n",
    "# Import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_as_image(sample):\n",
    "    bitmap = sample.reshape((13, 8))\n",
    "    plt.figure()\n",
    "    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA(7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "#features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "#for component in model.components_:\n",
    "#    show_as_image(component)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![_images/8.20.svg](_images/8.20.svg)\n",
    "![_images/8.21.svg](_images/8.21.svg)\n",
    "![_images/8.22.svg](_images/8.22.svg)\n",
    "![_images/8.23.svg](_images/8.23.svg)\n",
    "![_images/8.24.svg](_images/8.24.svg)\n",
    "![_images/8.25.svg](_images/8.25.svg)\n",
    "![_images/8.26.svg](_images/8.26.svg)\n",
    "\n",
    "Notice that the components of PCA do not represent meaningful parts of images of LED digits!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Building recommender systems using NMF**\n",
    "___\n",
    "Strategy\n",
    "- Apply NMF to the word-frequency array\n",
    "    - NMF feature values describe the topics\n",
    "- Compare feature values\n",
    "    - **cosine similarity** for weak vs. strong feature sets\n",
    "        - higher values indicate greater similarity between documents with weak vs. strong feature sets\n",
    "        - completed using .dot() method of pandas DataFrame object\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Which articles are similar to 'Cristiano Ronaldo'?\n",
    "\n",
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features: norm_features\n",
    "#norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame: df\n",
    "#df = pd.DataFrame(norm_features, index=titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "#article = df.loc['Cristiano Ronaldo']\n",
    "\n",
    "# Compute the dot products: similarities\n",
    "#similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "#print(similarities.nlargest())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Cristiano Ronaldo                1.000000\n",
    "#    Franck Ribéry                    0.999972\n",
    "#    Radamel Falcao                   0.999942\n",
    "#    Zlatan Ibrahimović               0.999942\n",
    "#    France national football team    0.999923\n",
    "#    dtype: float64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Recommend musical artists\n",
    "\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a MaxAbsScaler: scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Create an NMF model: nmf\n",
    "nmf = NMF(n_components=20)\n",
    "\n",
    "# Create a Normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
    "\n",
    "# Apply fit_transform to artists: norm_features\n",
    "#norm_features = pipeline.fit_transform(artists)\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: df\n",
    "#df = pd.DataFrame(norm_features, index=artist_names)\n",
    "\n",
    "# Select row of 'Bruce Springsteen': artist\n",
    "#artist = df.loc['Bruce Springsteen']\n",
    "\n",
    "# Compute cosine similarities: similarities\n",
    "#similarities = df.dot(artist)\n",
    "\n",
    "# Display those with highest cosine similarity\n",
    "#print(similarities.nlargest())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Bruce Springsteen    1.000000\n",
    "#    Neil Young           0.955896\n",
    "#    Van Morrison         0.872452\n",
    "#    Leonard Cohen        0.864763\n",
    "#    Bob Dylan            0.859047\n",
    "#    dtype: float64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}