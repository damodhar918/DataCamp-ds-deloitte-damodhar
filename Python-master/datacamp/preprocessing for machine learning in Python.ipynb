{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**What is data preprocessing?**\n",
    "___\n",
    "- beyond cleaning and exploratory data analysis\n",
    "- prepping data for modeling\n",
    "    e.g. transforming categorical data to numeric\n",
    "- Pandas\n",
    "    - .columns\n",
    "    - .dtypes\n",
    "    - .describe()\n",
    "    - remove missing data\n",
    "        - .dropna() - drop rows with NA values (axis=0, thresh=1)\n",
    "        - df[\"B\"].isnull().sum() - sum of all null values for specific column\n",
    "        - df[df[\"B\"].notnull()] - index for values that are not null for specific columns\n",
    "    - .drop([1, 2, 3]) - drop specific rows\n",
    "    - .drop(\"A\", axis = 1) - drop specific columns\n",
    "    - df[df[\"B\"] == 7] - boolean indexing of columns\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Missing data - rows\n",
    "\n",
    "#Taking a look at the volunteer dataset again, we want to drop rows\n",
    "#where the category_desc column values are missing. We're going to do\n",
    "#this using boolean indexing, by checking to see if we have any null\n",
    "#values, and then filtering the dataset so that we only have rows\n",
    "#with those values.\n",
    "\n",
    "# Check how many values are missing in the category_desc column\n",
    "#print(volunteer[\"category_desc\"].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset\n",
    "#volunteer_subset = volunteer[volunteer[\"category_desc\"].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "#print(volunteer_subset.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    48\n",
    "#    (617, 35)\n",
    "#################################################\n",
    "#Remember that you can use boolean indexing to effectively subset\n",
    "#DataFrames."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Working with data types**\n",
    "___\n",
    "- Why are types important?\n",
    "    - .dtypes\n",
    "        - object - string/mixed types\n",
    "        - int64 - integer\n",
    "        - float64 - float\n",
    "        - datetime64 (or timedelta) - datetime\n",
    "- Converting column types\n",
    "    - .astype(\"float\")\n",
    "___"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Converting a column type\n",
    "\n",
    "#If you take a look at the volunteer dataset types, you'll see that\n",
    "#the column hits is type object. But, if you actually look at the\n",
    "#column, you'll see that it consists of integers. Let's convert that\n",
    "#column to type int.\n",
    "\n",
    "# Print the head of the hits column\n",
    "#print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "#volunteer[\"hits\"] = volunteer[\"hits\"].astype(\"int\")\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "#print(volunteer.dtypes)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    737\n",
    "#    1     22\n",
    "#    2     62\n",
    "#    3     14\n",
    "#    4     31\n",
    "#\n",
    "#    Name: hits, dtype: object\n",
    "#    opportunity_id          int64\n",
    "#    content_id              int64\n",
    "#    vol_requests            int64\n",
    "#    event_time              int64\n",
    "#    title                  object\n",
    "#    hits                    int64\n",
    "#    summary                object\n",
    "#    is_priority            object\n",
    "#    category_id           float64\n",
    "#    category_desc          object\n",
    "#    amsl                  float64\n",
    "#    amsl_unit             float64\n",
    "#    org_title              object\n",
    "#    org_content_id          int64\n",
    "#    addresses_count         int64\n",
    "#    locality               object\n",
    "#    region                 object\n",
    "#    postalcode            float64\n",
    "#    primary_loc           float64\n",
    "#    display_url            object\n",
    "#    recurrence_type        object\n",
    "#    hours                   int64\n",
    "#    created_date           object\n",
    "#    last_modified_date     object\n",
    "#    start_date_date        object\n",
    "#    end_date_date          object\n",
    "#    status                 object\n",
    "#    Latitude              float64\n",
    "#    Longitude             float64\n",
    "#    Community Board       float64\n",
    "#    Community Council     float64\n",
    "#    Census Tract          float64\n",
    "#    BIN                   float64\n",
    "#    BBL                   float64\n",
    "#    NTA                   float64\n",
    "#    dtype: object\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Class distribution**\n",
    "___\n",
    "- How do you split train/test when your samples are not normally distributed?\n",
    "- Stratified sampling\n",
    "    - from train_test_split method .value_counts()\n",
    "    - parameter for train_test_split is \"stratify=\"\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Stratified sampling\n",
    "\n",
    "#We know that the distribution of variables in the category_desc\n",
    "#column in the volunteer dataset is uneven. If we wanted to train a\n",
    "#model to try to predict category_desc, we would want to train the\n",
    "#model on a sample of data that is representative of the entire\n",
    "#dataset. Stratified sampling is a way to achieve this.\n",
    "\n",
    "# Create a data with all columns except category_desc\n",
    "#volunteer_X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "#volunteer_y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "#X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "#print(y_train[\"category_desc\"].value_counts())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Strengthening Communities    230\n",
    "#    Helping Neighbors in Need     89\n",
    "#    Education                     69\n",
    "#    Health                        39\n",
    "#    Environment                   24\n",
    "#    Emergency Preparedness        11\n",
    "#    Name: category_desc, dtype: int64\n",
    "#    Strengthening Communities    230\n",
    "#    Helping Neighbors in Need     89\n",
    "#    Education                     69\n",
    "#    Health                        39\n",
    "#    Environment                   24\n",
    "#    Emergency Preparedness        11\n",
    "#    Name: category_desc, dtype: int64\n",
    "#################################################\n",
    "#ou'll use train_test_split frequently while building models, so\n",
    "#it's useful to be familiar with the function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standardizing Data**\n",
    "___\n",
    "- scikit-learn models assume normally distributed data\n",
    "- applied to continuous numerical data\n",
    "- linearity assumptions\n",
    "- types discussed\n",
    "    - log normalization\n",
    "    - feature scaling\n",
    "- when to standardize models\n",
    "    - model in linear space\n",
    "    - dataset features have high variance\n",
    "    - dataset features are continuous and on different scales\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Modeling without normalizing\n",
    "\n",
    "#Let's take a look at what might happen to your model's accuracy if\n",
    "#you try to model data without doing some sort of standardization\n",
    "#first. Here we have a subset of the wine dataset. One of the\n",
    "#columns, Proline, has an extremely high variance compared to the\n",
    "#other columns. This is an example of where a technique like log\n",
    "#normalization would come in handy, which you'll learn about in\n",
    "#the next section.\n",
    "\n",
    "#The scikit-learn model training process should be familiar to you\n",
    "#at this point, so we won't go too in-depth with it. You already\n",
    "#have a k-nearest neighbors model available (knn) as well as the X\n",
    "#and y sets you need to fit and score on.\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "#print(knn.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.5333333333333333\n",
    "#################################################\n",
    "#You can see that the accuracy score is pretty low. Let's explore\n",
    "#methods to improve this score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Log normalization**\n",
    "___\n",
    "- applies log transformation to feature(s) with high variance relative to other features\n",
    "- helps feature(s) approach normality\n",
    "- takes the log of e (2.718)\n",
    "    - e.g. log 30 = 3.4, log 300 = 5.7, log 3000 = 8\n",
    "- captures relative changes, the magnitude of change, and keeps everything in the positive space\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Log normalization in Python\n",
    "#Now that we know that the Proline column in our wine dataset has a\n",
    "#large amount of variance, let's log normalize it.\n",
    "\n",
    "#Numpy has been imported as np in your workspace.\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "#print(wine[\"Proline\"].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "#wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "#print(wine[\"Proline_log\"].var())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    99166.71735542436\n",
    "#    0.17231366191842012\n",
    "#################################################\n",
    "#  The np.log() function is an easy way to log normalize a column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scaling data for feature comparison**\n",
    "___\n",
    "- What is feature scaling?\n",
    "    - features on different scales\n",
    "    - model with linear characteristics\n",
    "    - center features with mean of zero and transform unit variance to same\n",
    "    - transforms to approximately normal distribution\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Scaling data - standardizing columns\n",
    "\n",
    "#Since we know that the Ash, Alcalinity of ash, and Magnesium\n",
    "#columns in the wine dataset are all on different scales, let's\n",
    "#standardize them in a way that allows for use in a linear model.\n",
    "\n",
    "# Import StandardScaler from scikit-learn\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "#ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale\n",
    "#wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "#wine_subset_scaled = ss.fit_transform(wine_subset)\n",
    "\n",
    "#################################################\n",
    "# In scikit-learn, running fit_transform during preprocessing will\n",
    "#both fit the method to the data as well as transform the data in a\n",
    "#single step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standardized data and modeling**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#KNN on non-scaled data\n",
    "\n",
    "#Let's first take a look at the accuracy of a K-nearest neighbors\n",
    "#model on the wine dataset without standardizing the data. The knn\n",
    "#model as well as the X and y data and labels sets have been created\n",
    "#already. Most of this process of creating models in scikit-learn\n",
    "#should look familiar to you.\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "#print(knn.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.6444444444444445\n",
    "#################################################\n",
    "#This scikit-learn workflow should be very familiar to you at this point."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#KNN on scaled data\n",
    "\n",
    "#The accuracy score on the unscaled wine dataset was decent, but we\n",
    "#can likely do better if we scale the dataset. The process is mostly\n",
    "#the same as the previous exercise, with the added step of scaling\n",
    "#the data. Once again, the knn model as well as the X and y data and\n",
    "#labels set have already been created for you.\n",
    "\n",
    "# Create the scaling method.\n",
    "#ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "#X_scaled = ss.fit_transform(X)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "#print(knn.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.9555555555555556\n",
    "#################################################\n",
    "#The increase in accuracy is worth the extra step of scaling the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature engineering**\n",
    "___\n",
    "- What is feature engineering?\n",
    "    - the creation of new features based on existing features\n",
    "    - insight into relationships between features\n",
    "    - extract and expand data\n",
    "    - dataset-dependent\n",
    "- scenarios\n",
    "    - text data\n",
    "    - categorical data\n",
    "    - time stamps\n",
    "    - averages\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Encoding categorical variables**\n",
    "___\n",
    "- encoding binary values\n",
    "    - Pandas\n",
    "        - .apply() plus lambda function\n",
    "        - .get_dummies() for one-hot encoding of variables with two or more labels\n",
    "    - scikit-learn\n",
    "        - LabelEncoder\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical variables - binary\n",
    "\n",
    "#Take a look at the hiking dataset. There are several columns here\n",
    "#that need encoding, one of which is the Accessible column, which\n",
    "#needs to be encoded in order to be modeled. Accessible is a binary\n",
    "#feature, so it has two values - either Y or N - so it needs to be\n",
    "#encoded into 1s and 0s. Use scikit-learn's LabelEncoder method to\n",
    "#do that transformation.\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "#enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "#hiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n",
    "\n",
    "# Compare the two columns\n",
    "#print(hiking[[\"Accessible_enc\", \"Accessible\"]].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Accessible_enc Accessible\n",
    "#    0               1          Y\n",
    "#    1               0          N\n",
    "#    2               0          N\n",
    "#    3               0          N\n",
    "#    4               0          N\n",
    "#################################################\n",
    "#.fit_transform() is a good way to both fit an encoding and\n",
    "#transform the data in a single step."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical variables - one-hot\n",
    "\n",
    "#One of the columns in the volunteer dataset, category_desc, gives\n",
    "#category descriptions for the volunteer opportunities listed.\n",
    "#Because it is a categorical variable with more than two categories,\n",
    "#we need to use one-hot encoding to transform this column numerically.\n",
    "#Use Pandas' get_dummies() function to do so.\n",
    "\n",
    "# Transform the category_desc column\n",
    "#category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "#print(category_enc.head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#       Education            ...              Strengthening Communities\n",
    "#    0          0            ...                                      0\n",
    "#    1          0            ...                                      1\n",
    "#    2          0            ...                                      1\n",
    "#    3          0            ...                                      1\n",
    "#    4          0            ...                                      0\n",
    "#\n",
    "#    [5 rows x 6 columns]\n",
    "#\n",
    "#################################################\n",
    "#get_dummies() is a simple and quick way to encode categorical variables."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Engineering numerical features**\n",
    "___\n",
    "- Aggregation\n",
    "    - taking means across columns using .apply(), lambda and .mean()\n",
    "    - dates\n",
    "        - convert to datetime using .to_datetime() in Pandas\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Engineering numerical features - taking an average\n",
    "\n",
    "#A good use case for taking an aggregate statistic to create a new\n",
    "#feature is to take the mean of columns. Here, you have a DataFrame\n",
    "#of running times named running_times_5k. For each name in the\n",
    "#dataset, take the mean of their 5 run times.\n",
    "\n",
    "# Create a list of the columns to average\n",
    "#run_columns = [\"run1\", \"run2\", \"run3\", \"run4\", \"run5\"]\n",
    "\n",
    "# Use apply to create a mean column\n",
    "#running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "#print(running_times_5k)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#          name  run1  run2  run3  run4  run5   mean\n",
    "#    0      Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
    "#    1     Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
    "#    2     Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
    "#    3     Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
    "#    4    Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n",
    "#    5  Russell  30.9  29.6  31.4  30.4  29.9  30.44\n",
    "#################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Engineering numerical features - datetime\n",
    "#There are several columns in the volunteer dataset comprised of\n",
    "#datetimes. Let's take a look at the start_date_date column and\n",
    "#extract just the month to use as a feature for modeling.\n",
    "\n",
    "# First, convert string column to date column\n",
    "#volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "#volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "#print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#      start_date_converted  start_date_month\n",
    "#    0           2011-07-30                 7\n",
    "#    1           2011-02-01                 2\n",
    "#    2           2011-01-29                 1\n",
    "#    3           2011-02-14                 2\n",
    "#    4           2011-02-05                 2\n",
    "#################################################\n",
    "#You can also use attributes like .day to get the day and .year to\n",
    "#get the year from datetime columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Text classification**\n",
    "___\n",
    "- Text extraction\n",
    "    - regular expressions\n",
    "        - data = re.match(pattern, data_string)\n",
    "        - data.group(0)\n",
    "        - \\d - digits\n",
    "        - + - as many as possible that are adjacent\n",
    "        - \\. - decimal point\n",
    "    - TF/IDF vectorization\n",
    "        - Term Frequency Inverse Document Frequency\n",
    "        - from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        - .fit_transform()\n",
    "    - Naive Bayes classifier\n",
    "        - P(A|B) = [P(B|A)*P(A)] / P(B)\n",
    "        - treats each feature as independent of the other\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Engineering features from strings - extraction\n",
    "\n",
    "#The Length column in the hiking dataset is a column of strings, but\n",
    "#contained in the column is the mileage for the hike. We're going to\n",
    "#extract this mileage using regular expressions, and then use a lambda\n",
    "#in Pandas to apply the extraction to the DataFrame.\n",
    "\n",
    "# Write a pattern to extract numbers and decimals\n",
    "#def return_mileage(length):\n",
    "#    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "\n",
    "    # Search the text for matches\n",
    "#    mile = re.match(pattern, length)\n",
    "\n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "#    if mile is not None:\n",
    "#        return float(mile.group(0))\n",
    "\n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "#hiking[\"Length_num\"] = hiking[\"Length\"].apply(lambda row: return_mileage(row))\n",
    "#print(hiking[[\"Length\", \"Length_num\"]].head())\n",
    "\n",
    "#################################################\n",
    "#script.py> output:\n",
    "#           Length  Length_num\n",
    "#    0   0.8 miles        0.80\n",
    "#    1    1.0 mile        1.00\n",
    "#    2  0.75 miles        0.75\n",
    "#    3   0.5 miles        0.50\n",
    "#    4   0.5 miles        0.50\n",
    "#################################################\n",
    "#Regular expressions are a useful way to perform text extraction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Engineering features from strings - tf/idf\n",
    "\n",
    "#Let's transform the volunteer dataset's title column into a text\n",
    "#vector, to use in a prediction task in the next exercise.\n",
    "\n",
    "# Take the title text\n",
    "#title_text = volunteer[\"title\"]\n",
    "\n",
    "# Create the vectorizer method\n",
    "#tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "#text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "#Text classification using tf/idf vectors\n",
    "\n",
    "#Now that we've encoded the volunteer dataset's title column into\n",
    "#tf/idf vectors, let's use those vectors to try to predict the\n",
    "#category_desc column.\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "#y = volunteer[\"category_desc\"]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "#nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "#print(nb.score(X_test, y_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.567741935483871\n",
    "#################################################\n",
    "# Notice that the model doesn't score very well. We'll work on\n",
    "#selecting the best features for modeling in the next chapter."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature selection**\n",
    "___\n",
    "- What is feature selection?\n",
    "    - selecting features to be used in modeling\n",
    "    - doesn't create new features\n",
    "    - improve model's performance\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Removing redundant features**\n",
    "___\n",
    "- remove:\n",
    "    - noisy features\n",
    "    - correlated features\n",
    "        - statistically correlated: features move together directionally\n",
    "        - linear models assume feature independence\n",
    "        - pearson correlation coefficient\n",
    "    - duplicated features\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Selecting relevant features\n",
    "\n",
    "#Now let's identify the redundant columns in the volunteer dataset\n",
    "#and perform feature selection on the dataset to return a DataFrame\n",
    "#of the relevant features.\n",
    "\n",
    "#For example, if you explore the volunteer dataset in the console,\n",
    "#you'll see three features which are related to location: locality,\n",
    "#region, and postalcode. They contain repeated information, so it\n",
    "#would make sense to keep only one of the features.\n",
    "\n",
    "#There are also features that have gone through the feature\n",
    "#engineering process: columns like Education and Emergency\n",
    "#Preparedness are a product of encoding the categorical variable\n",
    "#category_desc, so category_desc itself is redundant now.\n",
    "\n",
    "#Take a moment to examine the features of volunteer in the console,\n",
    "#and try to identify the redundant features.\n",
    "\n",
    "# Create a list of redundant column names to drop\n",
    "#to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "#volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of the new dataset\n",
    "#print(volunteer_subset.head())\n",
    "\n",
    "#################################################\n",
    "#script.py> output:\n",
    "#                                                   title            ...              Strengthening Communities\n",
    "#    1                                       Web designer            ...                                      1\n",
    "#    2      Urban Adventures - Ice Skating at Lasker Rink            ...                                      1\n",
    "#    3  Fight global hunger and support women farmers ...            ...                                      1\n",
    "#    4                                      Stop 'N' Swap            ...                                      0\n",
    "#    5                               Queens Stop 'N' Swap            ...                                      0\n",
    "#\n",
    "#    [5 rows x 11 columns]\n",
    "#################################################\n",
    "#It's often easier to collect a list of columns to drop, rather than\n",
    "#dropping them individually."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Checking for correlated features\n",
    "\n",
    "#Let's take a look at the wine dataset again, which is made up of\n",
    "#continuous, numerical features. Run Pearson's correlation\n",
    "#coefficient on the dataset to determine which columns are good\n",
    "#candidates for eliminating. Then, remove those columns from the\n",
    "#DataFrame.\n",
    "\n",
    "# Print out the column correlations of the wine dataset\n",
    "#print(wine.corr())\n",
    "\n",
    "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
    "#to_drop = \"Flavanoids\"\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "#wine = wine.drop(to_drop, axis=1)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                                  Flavanoids    ...          Hue\n",
    "#    Flavanoids                      1.000000    ...     0.543479\n",
    "#    Total phenols                   0.864564    ...     0.433681\n",
    "#    Malic acid                     -0.411007    ...    -0.561296\n",
    "#    OD280/OD315 of diluted wines    0.787194    ...     0.565468\n",
    "#    Hue                             0.543479    ...     1.000000\n",
    "#\n",
    "#    [5 rows x 5 columns]\n",
    "#################################################\n",
    "# Dropping correlated features is often an iterative process, so you\n",
    "#may need to try different combinations in your model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Selecting features using text vectors**\n",
    "___\n",
    "- looking at word weights\n",
    "    - print(tfidf_vec.vocabulary_)\n",
    "    - print(text_tfidf[3].data)\n",
    "    - print(text_tfidf[3].indices)\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Exploring text vectors, part 1\n",
    "\n",
    "#Let's expand on the text vector exploration method we just learned\n",
    "#about, using the volunteer dataset's title tf/idf vectors. In this\n",
    "#first part of text vector exploration, we're going to add to that\n",
    "#function we learned about in the slides. We'll return a list of\n",
    "#numbers with the function. In the next exercise, we'll write\n",
    "#another function to collect the top words across all documents,\n",
    "#extract them, and then use that list to filter down our text_tfidf\n",
    "#vector.\n",
    "\n",
    "# Add in the rest of the parameters\n",
    "#def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "#    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "\n",
    "    # Let's transform that zipped dict into a series\n",
    "#    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "\n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "#    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "#    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "#print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [189, 942, 466]\n",
    "#################################################\n",
    "#This is a little complicated, but you'll see how it comes together\n",
    "#in the next exercise."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Exploring text vectors, part 2\n",
    "\n",
    "#Using the function we wrote in the previous exercise, we're going\n",
    "#to extract the top words from each document in the text vector,\n",
    "#return a list of the word indices, and use that list to filter the\n",
    "#text vector down to those top words.\n",
    "\n",
    "#def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "#    filter_list = []\n",
    "#    for i in range(0, vector.shape[0]):\n",
    "\n",
    "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "#        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "#        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "#    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "#filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "#filtered_text = text_tfidf[:, list(filtered_words)]\n",
    "\n",
    "#################################################\n",
    "#In the next section, you'll train a model using the filtered vector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training Naive Bayes with feature selection\n",
    "\n",
    "#Let's re-run the Naive Bayes text classification model we ran at\n",
    "#the end of chapter 3, with our selection choices from the previous\n",
    "#exercise, on the volunteer dataset's title and category_desc columns.\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "#train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "#nb.fit(train_X, train_y)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "#print(nb.score(test_X, test_y))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.567741935483871\n",
    "#################################################\n",
    "#You can see that our accuracy score wasn't that different from the\n",
    "#score at the end of chapter 3. That's okay; the title field is a\n",
    "#very small text field, appropriate for demonstrating how filtering\n",
    "#vectors works."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Dimensionality reduction**\n",
    "___\n",
    "- Dimensionality reduction and PCA\n",
    "    - unsupervised learning\n",
    "    - combines/decomposes a feature space\n",
    "    - feature extraction used to reduce feature space\n",
    "- Principal Component Analysis\n",
    "    - linear transformation to uncorrelated space\n",
    "    - captures as much variance as possible in each component\n",
    "- PCA caveats\n",
    "    - difficult to interpret components\n",
    "    - end of preprocessing journey\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Using PCA\n",
    "#Let's apply PCA to the wine dataset, to see if we can get an\n",
    "#increase in our model's accuracy.\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up PCA and the X vector for diminsionality reduction\n",
    "#pca = PCA()\n",
    "#wine_X = wine.drop(\"Type\", axis=1)\n",
    "\n",
    "# Apply PCA to the wine dataset\n",
    "#transformed_X = pca.fit_transform(wine_X)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "#print(pca.explained_variance_ratio_)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    [9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
    "#     1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
    "#     1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
    "#     8.25392788e-08]\n",
    "#################################################\n",
    "# In the next section you'll train a model using the PCA-transformed vector."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training a model with PCA\n",
    "#Now that we have run PCA on the wine dataset, let's try training a\n",
    "#model with it.\n",
    "\n",
    "# Split the transformed X and the y labels into training and test sets\n",
    "#X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\n",
    "\n",
    "# Fit knn to the training data\n",
    "#knn.fit(X_wine_train, y_wine_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "#print(knn.score(X_wine_test, y_wine_test))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.6444444444444445\n",
    "#################################################\n",
    "#PCA is a decent choice for the wine dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**UFOs and preprocessing**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Checking column types\n",
    "#Take a look at the UFO dataset's column types using the dtypes\n",
    "#attribute. Two columns jump out for transformation: the seconds\n",
    "#column, which is a numeric column but is being read in as object,\n",
    "#and the date column, which can be transformed into the datetime\n",
    "#type. That will make our feature engineering efforts easier later on.\n",
    "\n",
    "# Check the column types\n",
    "#print(ufo.dtypes)\n",
    "\n",
    "# Change the type of seconds to float\n",
    "#ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "#ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Check the column types\n",
    "#print(ufo[[\"seconds\", \"date\"]].dtypes)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    date               object\n",
    "#    city               object\n",
    "#    state              object\n",
    "#    country            object\n",
    "#    type               object\n",
    "#    seconds            object\n",
    "#    length_of_time     object\n",
    "#    desc               object\n",
    "#    recorded           object\n",
    "#    lat                object\n",
    "#    long              float64\n",
    "\n",
    "#    dtype: object\n",
    "#    seconds           float64\n",
    "#    date       datetime64[ns]\n",
    "#    dtype: object\n",
    "#################################################\n",
    "#Nice job on transforming the column types! This will make feature\n",
    "#engineering and standardization easier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dropping missing data\n",
    "\n",
    "#Let's remove some of the rows where certain columns have missing\n",
    "#values. We're going to look at the length_of_time column, the\n",
    "#state column, and the type column. If any of the values in these\n",
    "#columns are missing, we're going to drop the rows.\n",
    "\n",
    "# Check how many values are missing in the length_of_time, state, and type columns\n",
    "#print(ufo[[\"length_of_time\", \"state\", \"type\"]].isnull().sum())\n",
    "\n",
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "#ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() &\n",
    "#          ufo[\"state\"].notnull() &\n",
    "#          ufo[\"type\"].notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "#print(ufo_no_missing.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    length_of_time    143\n",
    "#    state             419\n",
    "#    type              159\n",
    "#\n",
    "#    dtype: int64\n",
    "#    (4283, 4)\n",
    "#################################################\n",
    "#We'll work with this set going forward."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Categorical variables and standardization**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Extracting numbers from strings\n",
    "\n",
    "#The length_of_time field in the UFO dataset is a text field that\n",
    "#has the number of minutes within the string. Here, you'll extract\n",
    "#that number from that text field using regular expressions.\n",
    "\n",
    "#def return_minutes(time_string):\n",
    "\n",
    "    # We'll use \\d+ to grab digits and match it to the column values\n",
    "#    pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "    # Use match on the pattern and column\n",
    "#    num = re.match(pattern, time_string)\n",
    "#    if num is not None:\n",
    "#        return int(num.group(0))\n",
    "\n",
    "# Apply the extraction to the length_of_time column\n",
    "#ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "#print(ufo[[\"length_of_time\", \"minutes\"]].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#        length_of_time  minutes\n",
    "#    2  about 5 minutes      NaN\n",
    "#    4       10 minutes     10.0\n",
    "#    7        2 minutes      2.0\n",
    "#    8        2 minutes      2.0\n",
    "#    9        5 minutes      5.0\n",
    "#################################################\n",
    "#As you can see, we end up with some NaNs in the DataFrame. That's\n",
    "#okay for now; we'll take care of those before modeling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Identifying features for standardization\n",
    "\n",
    "#In this section, you'll investigate the variance of columns in the\n",
    "#UFO dataset to determine which features should be standardized.\n",
    "#After taking a look at the variances of the seconds and minutes\n",
    "#column, you'll see that the variance of the seconds column is\n",
    "#extremely high. Because seconds and minutes are related to each\n",
    "#other (an issue we'll deal with when we select features for\n",
    "#modeling), let's log normalize the seconds column.\n",
    "\n",
    "# Check the variance of the seconds and minutes columns\n",
    "#print(ufo[[\"seconds\", \"minutes\"]].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "#ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "#print(ufo[\"seconds_log\"].var())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    seconds    424087.417474\n",
    "#    minutes       117.546372\n",
    "#\n",
    "#    dtype: float64\n",
    "#    1.1223923881183004\n",
    "#################################################\n",
    "# In the next section, we'll focus on engineering new features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Engineering new features**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoding categorical variables\n",
    "\n",
    "#There are couple of columns in the UFO dataset that need to be\n",
    "#encoded before they can be modeled through scikit-learn. You'll do\n",
    "#that transformation here, using both binary and one-hot encoding\n",
    "#methods.\n",
    "\n",
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "#ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda val: 1 if val == \"us\" else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "#print(len(ufo[\"type\"].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "#type_set = pd.get_dummies(ufo[\"type\"])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "#ufo = pd.concat([ufo, type_set], axis=1)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    21\n",
    "#################################################\n",
    "#Let's continue on by extracting some date parts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Features from dates\n",
    "\n",
    "#Another feature engineering task to perform is month and year\n",
    "#extraction. Perform this task on the date column of the ufo dataset.\n",
    "\n",
    "# Look at the first 5 rows of the date column\n",
    "#print(ufo[\"date\"].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "#ufo[\"month\"] = ufo[\"date\"].apply(lambda row: row.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "#ufo[\"year\"] = ufo[\"date\"].apply(lambda row: row.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "#print(ufo[[\"date\", \"month\", \"year\"]].head())\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0   2002-11-21 05:45:00\n",
    "#    1   2012-06-16 23:00:00\n",
    "#    2   2013-06-09 00:00:00\n",
    "#    3   2013-04-26 23:27:00\n",
    "#    4   2013-09-13 20:30:00\n",
    "#    Name: date, dtype: datetime64[ns]\n",
    "\n",
    "#                     date  month  year\n",
    "#    0 2002-11-21 05:45:00     11  2002\n",
    "#    1 2012-06-16 23:00:00      6  2012\n",
    "#    2 2013-06-09 00:00:00      6  2013\n",
    "#    3 2013-04-26 23:27:00      4  2013\n",
    "#    4 2013-09-13 20:30:00      9  2013\n",
    "#################################################\n",
    "# 'apply' and 'lambda' are extremely useful for extraction tasks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Text vectorization\n",
    "\n",
    "#Let's transform the desc column in the UFO dataset into tf/idf\n",
    "#vectors, since there's likely something we can learn from this\n",
    "#field.\n",
    "\n",
    "# Take a look at the head of the desc field\n",
    "#print(ufo[\"desc\"].head())\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "#vec = TfidfVectorizer()\n",
    "\n",
    "# Use vec's fit_transform method on the desc field\n",
    "#desc_tfidf = vec.fit_transform(ufo[\"desc\"])\n",
    "\n",
    "# Look at the number of columns this creates.\n",
    "#print(desc_tfidf.shape)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0    It was a large&#44 triangular shaped flying ob...\n",
    "#    1    Dancing lights that would fly around and then ...\n",
    "#    2    Brilliant orange light or chinese lantern at o...\n",
    "#    3    Bright red light moving north to north west fr...\n",
    "#    4    North-east moving south-west. First 7 or so li...\n",
    "#    Name: desc, dtype: object\n",
    "#\n",
    "#    (1866, 3422)\n",
    "#################################################\n",
    "#ou'll notice that the text vector has a large number of columns.\n",
    "#We'll work on selecting the features we want to use for modeling\n",
    "#in the next section."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature selection and modeling**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Selecting the ideal dataset\n",
    "\n",
    "#Let's get rid of some of the unnecessary features. Because we have\n",
    "#an encoded country column, country_enc, keep it and drop other\n",
    "#columns related to location: city, country, lat, long, state.\n",
    "\n",
    "#We have columns related to month and year, so we don't need the\n",
    "#date or recorded columns.\n",
    "\n",
    "#We vectorized desc, so we don't need it anymore. For now we'll\n",
    "#keep type.\n",
    "\n",
    "#We'll keep seconds_log and drop seconds and minutes.\n",
    "\n",
    "#Let's also get rid of the length_of_time column, which is\n",
    "#unnecessary after extracting minutes.\n",
    "\n",
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "#print(ufo[[\"seconds\", \"seconds_log\", \"minutes\"]].corr())\n",
    "\n",
    "# Make a list of features to drop\n",
    "#to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]\n",
    "\n",
    "# Drop those features\n",
    "#ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "#filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#                  seconds  seconds_log   minutes\n",
    "#    seconds      1.000000     0.853371  0.980341\n",
    "#    seconds_log  0.853371     1.000000  0.824493\n",
    "#    minutes      0.980341     0.824493  1.000000\n",
    "#################################################\n",
    "#You're almost done. In the next exercises, we'll try modeling the\n",
    "#UFO data in a couple of different ways."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Modeling the UFO dataset, part 1\n",
    "\n",
    "#In this exercise, we're going to build a k-nearest neighbor model\n",
    "#to predict which country the UFO sighting took place in. Our X\n",
    "#dataset has the log-normalized seconds column, the one-hot encoded\n",
    "#type columns, as well as the month and year when the sighting took\n",
    "#place. The y labels are the encoded country column, where 1 is us\n",
    "#and 0 is ca.\n",
    "\n",
    "# Take a look at the features in the X set of data\n",
    "#print(X.columns)\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "#train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "#knn.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "#print(knn.score(test_X, test_y))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
    "#           'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
    "#           'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
    "#           'teardrop', 'triangle', 'unknown', 'month', 'year'],\n",
    "#          dtype='object')\n",
    "#    0.8693790149892934\n",
    "#################################################\n",
    "#This model performs pretty well. It seems like we've made pretty\n",
    "#good feature selection choices here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Modeling the UFO dataset, part 2\n",
    "\n",
    "#Finally, let's build a model using the text vector we created,\n",
    "#desc_tfidf, using the filtered_words list to create a filtered\n",
    "#text vector. Let's see if we can predict the type of the sighting\n",
    "#based on the text. We'll use a Naive Bayes model for this.\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "#filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "#train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "#nb.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "#print(nb.score(test_X, test_y))\n",
    "\n",
    "#################################################\n",
    "#<script.py> output:\n",
    "#    0.16274089935760172\n",
    "#################################################\n",
    "#Congrats, you've completed the course! As you can see, this model\n",
    "#performs very poorly on this text data. This is a clear case where\n",
    "#iteration would be necessary to figure out what subset of text\n",
    "#improves the model, and if perhaps any of the other features are\n",
    "#useful in predicting type."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Congratulations!**\n",
    "___"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}