{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47679ab0-94e6-4f41-b3e5-804ab6cd9686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/words.txt MapPartitionsRDD[1] at textFile at <console>:25\n",
       "res1: Long = 6\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = sc.textFile(\"hdfs://localhost:9000/words.txt\")\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2ee6c2-ba9d-492a-bc38-af305b8bf768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(\"   spark kafka  \", \" kafka   spark pyspark \", \"                \", spark, \"\", \"APACHE Kafka APache SParK \")\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876bec9c-74f0-4c44-9860-209d8a6f5427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lowerCaseRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at <console>:26\n",
       "res4: Array[String] = Array(spark kafka, kafka   spark pyspark, \"\", spark, \"\", apache kafka apache spark)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lowerCaseRdd = textFile.map (line => line.trim().toLowerCase())\n",
    "lowerCaseRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3846b6b8-aa61-4b87-830a-71194e821749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordArrayRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at <console>:26\n",
       "res5: Array[Array[String]] = Array(Array(spark, kafka), Array(kafka, \"\", \"\", spark, pyspark), Array(\"\"), Array(spark), Array(\"\"), Array(apache, kafka, apache, spark))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordArrayRdd = lowerCaseRdd.map (line => line.split(\" \"))\n",
    "wordArrayRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4526e7-5386-47c2-971c-2a68eb08a7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:26\n",
       "res6: Array[String] = Array(spark, kafka, kafka, \"\", \"\", spark, pyspark, \"\", spark, \"\", apache, kafka, apache, spark)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordRdd = wordArrayRdd.flatMap(arr => arr)\n",
    "wordRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0dd535-9c22-49f0-95d3-e917bc6b6305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[8] at map at <console>:31\n",
       "res9: Array[(String, Int)] = Array((spark,1), (kafka,1), (kafka,1), (spark,1), (pyspark,1), (spark,1), (apache,1), (kafka,1), (apache,1), (spark,1))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Tuple 2\n",
    "val wordPairRdd = wordRdd\n",
    "                        .filter (word =>  !word.isEmpty())\n",
    "                        .map ( word => (word, 1) )\n",
    "\n",
    "wordPairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc365944-14f4-4d6d-91c8-28f1d7c1edf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCountRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at reduceByKey at <console>:28\n",
       "res10: Array[(String, Int)] = Array((pyspark,1), (apache,2), (kafka,3), (spark,4))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCountRdd = wordPairRdd.reduceByKey( (acc, value) => acc + value )\n",
    "wordCountRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab0dc22-fbc0-4e4e-966a-15b8eb7557f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountRdd.saveAsTextFile(\"hdfs://localhost:9000/word-count-scala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c896ca9-2ce4-470d-95ce-2492df6b3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "// hdfs dfs -ls /\n",
    "// hdfs dfs -cat /word-count-scala/part-00000\n",
    "// hdfs dfs -cat /word-count-scala/part-00000\n",
    "/// hdfs dfs -cat /word-count-scala/part-00001\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
