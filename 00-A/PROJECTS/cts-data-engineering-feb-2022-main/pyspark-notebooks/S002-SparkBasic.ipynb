{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c67fd4-6047-4aab-9d7e-6d30ba06afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark\n",
    "# finds spark installation in the system, \n",
    "# automatically set path, environment needed for pyspark\n",
    "# load spark libraries etc\n",
    "# /opt/spark-2.4.7.......\n",
    "# good for single machine development, learning\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca86ff57-0579-4ddc-a77a-6fbd04adf2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/28 21:20:04 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.80.128 instead (on interface ens33)\n",
      "22/02/28 21:20:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/02/28 21:20:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Context - helps to create rdd, dag, job, task execute task etc\n",
    "# this code is called spark application, or spark driver\n",
    "# every spark driver shall have ONLY ONE spark context\n",
    "from pyspark import SparkContext\n",
    "# local is execution mode, spark driver, \n",
    "# spark executor runs in same JVM in same machine / not distributed\n",
    "# good for development, learning , not for production\n",
    "# SparkBasic is application name of your choice\n",
    "sc = SparkContext(\"local\", \"SparkBasic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29be3a8-32c8-4926-8171-3fab7c669ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD, from hardcoded data\n",
    "# data is hardcoded in Spark Driver, this notebook\n",
    "# RDD shall be created in Executor process\n",
    "# Lazy evaluation \n",
    "# intellisense - editor/notebook automatically brings up functions as you type\n",
    "# sc.<TAB><TAB><TAB> - will bring up all functions\n",
    "# creating RDD using parallelize method, by loading hardcoded data\n",
    "# RDD shall have partition(s)\n",
    "# the data hardcoded shall be loaded into partitions\n",
    "# at this moment, no data shall be loaded, as this is lazy loading\n",
    "# when we apply action only then data shall be loaded\n",
    "rdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c27d39e-e0e7-41fe-8466-25f531c18a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter operation, we call this as TRANSFORMATION\n",
    "# TRANSFORMATION is code/task applied on partitioned data\n",
    "# filter is higher order function, accept a function as input\n",
    "# filter apply data (n) from partition to function supplied lambda n: n % 2 == 1\n",
    "# lambda n: n % 2 == 1 returns either true or false,\n",
    "# filter collect all the numbers where fitler return true 1, 3, 5, 7, 9\n",
    "# LAZY Evaluation, no partition, no data , not code loaded into Exeuctor\n",
    "# until we apply ACTION on RDD\n",
    "oddRdd = rdd.filter (lambda n: n % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc393700-222e-4106-9c02-7f2b54ee450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# collect is an ACTION method\n",
    "# Every ACTION Methods create JOB\n",
    "# JOB is split into STAGES\n",
    "# Each STAGE shall have TASKs\n",
    "# TASKs shall be running on Executor on PARTITION\n",
    "# Finally, collect bring the output back to DRIVER from EXECUTORs\n",
    "# Action is the one will create and distribute partitions, run tasks on executors\n",
    "results = oddRdd.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ee242b-a33d-40db-9f3d-a04d14cace14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# min is an ACTION,\n",
    "# this create job, stages, DAG, tasks execute on cluster independently \n",
    "r = oddRdd.min ()\n",
    "print(r) # print min of odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0dd9e4e-7fb7-4a99-9011-24d60f45bfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "r2 = oddRdd.max()\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892d1437-fba5-4ad1-87b1-65f8eb28ff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "r3 = oddRdd.mean()\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cba76d7-c173-4a83-a0e4-a3830317d175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "r4 = oddRdd.sum()\n",
    "print(r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd16bc-4ec8-466f-b5af-ab3daacf0e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
