{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\nhttps://www.gutenberg.org/files/2701/2701-h/2701-h.htm\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":null,"lastSuccessfullyExecutedCode":null,"executionCancelledAt":null,"lastExecutedAt":null,"lastScheduledRunId":null,"outputsMetadata":{"0":{"height":57,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":1}]},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Get the Moby Dick HTML  \nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n\n# Set the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n\n# Extract the HTML from the request object\nhtml = r.text\n\n# Print the first 2000 characters in html\nprint(html[0:2000])\n\n# Create a BeautifulSoup object from the HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n\n# Get the text out of the soup\nmoby_text = html_soup.get_text()\n\n# Create a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)\n\n# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]\n\n# Print out the first eight words\nwords[:8]\n\n# Get the English stop words from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n\n# Print out the first eight stop words\nstop_words[:8]\n\n# Create a list words_ns containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n\n# Print the first five words_no_stop to check that stop words are gone\nwords_no_stop[:5]\n\n# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n\n# Print the top ten words and their counts\nprint(top_ten)","metadata":{"executionCancelledAt":null,"executionTime":898,"lastExecutedAt":1702445090412,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Get the Moby Dick HTML  \nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n\n# Set the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n\n# Extract the HTML from the request object\nhtml = r.text\n\n# Print the first 2000 characters in html\nprint(html[0:2000])\n\n# Create a BeautifulSoup object from the HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n\n# Get the text out of the soup\nmoby_text = html_soup.get_text()\n\n# Create a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)\n\n# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]\n\n# Print out the first eight words\nwords[:8]\n\n# Get the English stop words from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n\n# Print out the first eight stop words\nstop_words[:8]\n\n# Create a list words_ns containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n\n# Print the first five words_no_stop to check that stop words are gone\nwords_no_stop[:5]\n\n# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n\n# Print the top ten words and their counts\nprint(top_ten)","outputsMetadata":{"0":{"height":57,"type":"stream"},"1":{"height":606,"type":"stream"}}},"cell_type":"code","id":"69444d33-4dc5-427a-aa02-0dd89124b1bf","execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"stream","name":"stdout","text":"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<!DOCTYPE html\n   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n  <head>\n    <title>\n      Moby Dick; Or the Whale, by Herman Melville\n    </title>\n    <style type=\"text/css\" xml:space=\"preserve\">\n\n    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\n    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n    hr  { width: 50%; text-align: center;}\n    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\n    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n    .toc       { margin-left: 10%; margin-bottom: .75em;}\n    .toc2      { margin-left: 20%;}\n    div.fig    { display:block; margin:0 auto; text-align:center; }\n    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\n    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\n    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\n    .pagenum   {display:inline; font-size: 70%; font-style:normal;\n               margin: 0; padding: 0; position: absolute; right: 1%;\n               text-align: right;}\n    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n\n    table      {margin-left: 10%;}\n\na:link {color:blue;\n\t\ttext-decoration:none}\nlink {color:blue;\n\t\ttext-decoration:none}\na:visited {color:blue;\n\t\ttext-decoration:none}\na:hover {color:red}\n\n</style>\n  </head>\n  <body>\n<pre xml:space=\"preserve\">\n\nThe Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\n\nThis eBook is for the use of anyone anywh\n[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}]},{"source":"How to approach the project\n1. Request and encode the text\n\n2. Extract the text\n\n3. Create a BeautifulSoup object and get the text\n\n4. Tokenize the text\n\n5. Convert words to lowercase\n\n6. Load in stop words\n\n7. Remove stop words from the text\n\n8. Count the frequency of words\n\nSteps to complete\n\n1\nRequest and encode the text\nRequest the Moby Dick HTML file from the following URL, assign it to r, and set the text encoding to utf-8.\n\nhttps://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\n\n\nHow to perform a GET request\nHow to encode text\n2\nExtract the text\nExtract the text from r and assign it to html, then print out the first 2000 characters in html.\n\n\nHow to extract text from a GET request\nHow to print a slice of a string\n3\nCreate a BeautifulSoup object and get the text\nCreate a BeautifulSoup object from html using html.parser, assign it to html_soup, and get the text and save to moby_text.\n\n\nCreating a BeautifulSoup object\nGet text from a BeautifulSoup object\n4\nTokenize the text\nInitialize a regex tokenizer object, tokenizer, using nltk.tokenize.RegexpTokenizer, passing in a regular expression that will keep only alphanumeric text; tokenize the text to split it into individual words and assign the resulting list of words to tokens.\n\n\nUsing regex to match alphanumeric text\nHow to tokenize text\n5\nConvert words to lowercase\nLoop through the words in tokens, make them lowercase, and store them in a list called words, then print out the first eight words.\n\n\nConverting to lowercase\n6\nLoad in stop words\nLoad in the English stop words from nltk and assign them to stop_words, then print out the first eight stop words in stop_words.\n\n\nLoading stop words\n7\nRemove stop words from the text\nCreate a new list words_no_stop with the words from Moby Dick where stop words have been removed, then print the first five words in words_no_stop.\n\n\nCreate a new list from a list\n8\nCount the frequency of words\nInitialize a Counter object called count using our words_no_stop list; use the corresponding method to return the ten most common words and their counts, assigning the result to top_ten then print.\n\n\nUsing Counter\nFinding the frequency","metadata":{},"cell_type":"markdown","id":"825a5534-1d9c-42f0-8dc6-a22303ddd85e"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}