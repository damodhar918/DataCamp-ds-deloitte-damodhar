{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('local-1698603390093',\n",
       " 'local[*]',\n",
       " '3.5.0',\n",
       " 'http://USHYDJDAMODH6.us.deloitte.com:4041',\n",
       " 16,\n",
       " '3.11',\n",
       " 'dj',\n",
       " <bound method SparkContext.sparkUser of <SparkContext master=local[*] appName=dj>>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create Spark context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('dj').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('dj').getOrCreate()\n",
    "spark\n",
    "(sc.applicationId, sc.master, sc.version, sc.uiWebUrl, sc.defaultParallelism, sc.pythonVer, sc.appName, sc.sparkUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 16.367500 seconds\n",
      "Counting 139358 rows again took 0.214770 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "departures_df = spark.read.csv(\"./AA_DFW_2017_Departures_Short.csv.gz\", header=True)\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t157199\n",
      "Time to run: 0.276433\n",
      "Total rows in split DataFrame:\t583722\n",
      "Time to run: 0.327415\n"
     ]
    }
   ],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('./AA_DFW_2014_Departures_Short.csv.gz')\n",
    "split_df = spark.read.csv('./AA*gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: dj\n",
      "Driver TCP port: 59470\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 2\n",
      "Partition count after change: 2\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('./AA_DFW_2015_Departures_Short.csv.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://openflights.org/data.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = spark.read.csv('./airports.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flights_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jdamodhar\\Desktop\\python_essential\\DataCamp-ds-deloitte-master\\00-A\\53-Cleaning Data with PySpark\\03-Improving Performance.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/53-Cleaning%20Data%20with%20PySpark/03-Improving%20Performance.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Join the flights_df and aiports_df DataFrames\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/53-Cleaning%20Data%20with%20PySpark/03-Improving%20Performance.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m normal_df \u001b[39m=\u001b[39m flights_df\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/53-Cleaning%20Data%20with%20PySpark/03-Improving%20Performance.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     airports_df,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/53-Cleaning%20Data%20with%20PySpark/03-Improving%20Performance.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     flights_df[\u001b[39m\"\u001b[39m\u001b[39mDestination Airport\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m airports_df[\u001b[39m\"\u001b[39m\u001b[39mIATA\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/53-Cleaning%20Data%20with%20PySpark/03-Improving%20Performance.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/53-Cleaning%20Data%20with%20PySpark/03-Improving%20Performance.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Show the query plan   \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/53-Cleaning%20Data%20with%20PySpark/03-Improving%20Performance.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m normal_df\u001b[39m.\u001b[39mexplain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flights_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(\n",
    "    airports_df,\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"]\n",
    ")\n",
    "\n",
    "# Show the query plan   \n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+---------------+----+-------------------------------------+-------------+-------------+----+----+-----------------+-----------+--------+--------+---+---------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|flight_duration|id  |Name                                 |City'        | 'Country    |IATA|ICAO|Latitude         |Longitude  |Altitude|Timezone|DST|Tz database time zone|\n",
      "+-----------------+-------------+-------------------+---------------+----+-------------------------------------+-------------+-------------+----+----+-----------------+-----------+--------+--------+---+---------------------+\n",
      "|01/01/2017       |0005         |HNL                |537            |3728|Daniel K Inouye International Airport|Honolulu     |United States|HNL |PHNL|21.32062         |-157.924228|13      |-10     |N  |Pacific/Honolulu     |\n",
      "|01/01/2017       |0007         |OGG                |498            |3456|Kahului Airport                      |Kahului      |United States|OGG |PHOG|20.8986          |-156.429993|54      |-10     |N  |Pacific/Honolulu     |\n",
      "|01/01/2017       |0037         |SFO                |241            |3469|San Francisco International Airport  |San Francisco|United States|SFO |KSFO|37.61899948120117|-122.375   |13      |-8      |A  |America/Los_Angeles  |\n",
      "+-----------------+-------------+-------------------+---------------+----+-------------------------------------+-------------+-------------+----+----+-----------------+-----------+--------+--------+---+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normal_df.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#76], [IATA#548], Inner, BuildRight, false\n",
      "   :- Union\n",
      "   :  :- Project [Date (MM/DD/YYYY)#74, Flight Number#75, Destination Airport#76, Actual elapsed time (Minutes)#77 AS flight_duration#158]\n",
      "   :  :  +- Filter isnotnull(Destination Airport#76)\n",
      "   :  :     +- FileScan csv [Date (MM/DD/YYYY)#74,Flight Number#75,Destination Airport#76,Actual elapsed time (Minutes)#77] Batched: false, DataFilters: [isnotnull(Destination Airport#76)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   :  +- Project [Date (MM/DD/YYYY)#99, Flight Number#100, Destination Airport#101, Actual elapsed time (Minutes)#102 AS flight_duration#720]\n",
      "   :     +- Filter isnotnull(Destination Airport#101)\n",
      "   :        +- FileScan csv [Date (MM/DD/YYYY)#99,Flight Number#100,Destination Airport#101,Actual elapsed time (Minutes)#102] Batched: false, DataFilters: [isnotnull(Destination Airport#101)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[4, string, false]),false), [plan_id=780]\n",
      "      +- Filter isnotnull(IATA#548)\n",
      "         +- FileScan csv [id#544,Name#545,City'#546, 'Country#547,IATA#548,ICAO#549,Latitude#550,Longitude#551,Altitude#552,Timezone#553,DST#554,Tz database time zone#555] Batched: false, DataFilters: [isnotnull(IATA#548)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte..., PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<id:string,Name:string,City':string, 'Country:string,IATA:string,ICAO:string,Latitude:strin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and aiports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(\n",
    "    broadcast(airports_df),\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"]\n",
    ")\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t279962\tduration: 0.640231\n",
      "Broadcast count:\t279962\tduration: 0.415035\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
