{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d7a689-c0a5-4418-beb9-8c31e9f36be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1698603637136'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create Spark context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('dj').setMaster('local[4]')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('dj').getOrCreate()\n",
    "# Print spark\n",
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111ee5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b321bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|01/01/2017       |0005         |537                          |hnl    |\n",
      "|01/01/2017       |0007         |498                          |ogg    |\n",
      "|01/01/2017       |0037         |241                          |sfo    |\n",
      "|01/01/2017       |0043         |134                          |dtw    |\n",
      "|01/01/2017       |0051         |88                           |stl    |\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('./AA_DFW_2017_Departures_Short.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d4c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format('csv').options(Header=True).load('./AA_DFW_2017_Departures_Short.csv.gz')\n",
    "df2 = spark.read.format('csv').options(Header=True).load('./AA_DFW_2016_Departures_Short.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b02d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139358\n",
      "df2 Count: 140604\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ff162b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|01/01/2017       |0005         |HNL                |537                          |\n",
      "|01/01/2017       |0007         |OGG                |498                          |\n",
      "|01/01/2017       |0037         |SFO                |241                          |\n",
      "|01/01/2017       |0043         |DTW                |134                          |\n",
      "|01/01/2017       |0051         |STL                |88                           |\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346cd65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279962\n"
     ]
    }
   ],
   "source": [
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.format('parquet').save('tmp_AA_DFW_ALL.parquet')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('tmp_AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0dc3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date (MM/DD/YYYY)', 'string'),\n",
       " ('Flight Number', 'string'),\n",
       " ('Destination Airport', 'string'),\n",
       " ('flight_duration', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df3.withColumnRenamed('Actual elapsed time (Minutes)', 'flight_duration')\n",
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a1014ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 151\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('tmp_AA_DFW_ALL.parquet')\n",
    "flights_df = df3\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f4a230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.60996492381108"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT avg(flight_duration) from flights').collect()[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
