# Pyspark Developer's Quick Review

## Verify SparkContext

Make sure `SparkContext` is set up correctly. It should be initialized in the following way:

```python
from pyspark import SparkContext

sc = SparkContext("local", "pyspark app")
```

You can verify the context by running:

```python
print(sc)
```
To create an instance of `SparkContext` and `SparkSession` in PySpark, you can use the following code:

```python
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

# Create Spark configuration
conf = SparkConf().setAppName("myApp")

# Create Spark context
sc = SparkContext(conf=conf)

# Create Spark session
spark = SparkSession.builder.appName("myApp").getOrCreate()
```

In the above example, we first import `SparkContext` and `SparkSession` classes from the `pyspark` package. Then, we create a `SparkConf` object to configure our application name. We use this configuration object to create an instance of `SparkContext` by passing it as the argument.

Finally, we create a `SparkSession` using the `SparkSession.builder` object and specifying the application name, and `.getOrCreate()` to obtain an existing `SparkSession` or create a new one if none exists.

## Print Spark version

It's always a good idea to check the Spark version. You can do this easily:

```python
print(sc.version)
```

This will print the current Spark version, such as `3.1.2`.

## Creating RDDs

### From a list

One of the most common ways to create an RDD is using a list. Here's an example:

```python
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)
```

This will create an RDD containing the numbers 1 through 5.

### From a text file

Another commonly used method is to create an RDD from a text file:

```python
rdd = sc.textFile("path/to/file.txt")
```

This will create an RDD where each line of the text file is an element in the RDD.

## Transformations and Actions

### map

The `map` transformation is used to apply a function to each element of an RDD. Here's an example:

```python
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

def square(x):
    return x ** 2

squared_rdd = rdd.map(square)
```

This will create a new RDD, `squared_rdd`, where each element is the square of the corresponding element in the original RDD.

### filter

The `filter` transformation is used to remove elements from an RDD that don't match a certain condition. Here's an example:

```python
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

def is_odd(x):
    return x % 2 == 1

filtered_rdd = rdd.filter(is_odd)
```

This will create a new RDD, `filtered_rdd`, that only contains the odd elements from the original RDD.

### collect

The `collect` action is used to retrieve all the elements from an RDD and return them as a list. Here's an example:

```python
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

result = rdd.collect()

print(result)
```

This will print `[1, 2, 3, 4, 5]`.

## Conclusion

These are just a few of the basic concepts and examples for a PySpark developer. There's a lot more you can do with PySpark, including data manipulation, SQL queries, machine learning, and more. But these examples should help you get started with the fundamentals of RDDs, transformations, and actions.