{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| NULL|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| NULL|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('flights').getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| NULL|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| NULL|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('./flights.csv', sep=',', header=True, inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_km = flights_none_missing.withColumn('km', round(flights_none_missing.mile * 1.60934, 0)).drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not(0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|  9.48|     351| NULL|3465.0| NULL|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|   419|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  4|  2|  5|     AA|   325|ORD|  8.92|      65| NULL| 415.0| NULL|\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column (1 mile is equivalent to 1.60934 km)\n",
    "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[11.0,20.0,6.0,6.0,2.0,3465.0,9.48,351.0]|NULL |\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
      "|[4.0,2.0,5.0,1.0,0.0,415.0,8.92,65.0]    |NULL |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow',\n",
    "    'carrier_idx', \n",
    "    'org_idx',\n",
    "    'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+------+-----+-----------+-------+------------------------------------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|km    |label|carrier_idx|org_idx|features                                  |\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+-----+-----------+-------+------------------------------------------+\n",
      "|0  |22 |2  |UA     |1107  |ORD|16.33 |82      |30   |509.0 |1    |0.0        |0.0    |[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]   |\n",
      "|2  |20 |4  |UA     |226   |SFO|6.17  |82      |-8   |542.0 |0    |0.0        |1.0    |[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]    |\n",
      "|9  |13 |1  |AA     |419   |ORD|10.33 |195     |-5   |1989.0|0    |1.0        |0.0    |[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0] |\n",
      "|5  |2  |1  |UA     |704   |SFO|7.98  |102     |2    |885.0 |0    |0.0        |1.0    |[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]    |\n",
      "|7  |2  |6  |AA     |380   |ORD|10.83 |135     |54   |1180.0|1    |1.0        |0.0    |[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0]  |\n",
      "|1  |16 |6  |UA     |1477  |ORD|8.0   |232     |-7   |2317.0|0    |0.0        |0.0    |[1.0,16.0,6.0,0.0,0.0,2317.0,8.0,232.0]   |\n",
      "|1  |22 |5  |UA     |620   |SJC|7.98  |250     |-13  |2943.0|0    |0.0        |4.0    |[1.0,22.0,5.0,0.0,4.0,2943.0,7.98,250.0]  |\n",
      "|11 |8  |1  |OO     |5590  |SFO|7.77  |60      |88   |254.0 |1    |2.0        |1.0    |[11.0,8.0,1.0,2.0,1.0,254.0,7.77,60.0]    |\n",
      "|4  |26 |1  |AA     |1144  |SFO|13.25 |210     |-10  |2356.0|0    |1.0        |1.0    |[4.0,26.0,1.0,1.0,1.0,2356.0,13.25,210.0] |\n",
      "|4  |25 |0  |AA     |321   |ORD|13.75 |160     |31   |1574.0|1    |1.0        |0.0    |[4.0,25.0,0.0,1.0,0.0,1574.0,13.75,160.0] |\n",
      "|8  |30 |2  |UA     |646   |ORD|13.28 |151     |16   |1157.0|1    |0.0        |0.0    |[8.0,30.0,2.0,0.0,0.0,1157.0,13.28,151.0] |\n",
      "|3  |16 |3  |UA     |107   |ORD|9.0   |264     |3    |2808.0|0    |0.0        |0.0    |[3.0,16.0,3.0,0.0,0.0,2808.0,9.0,264.0]   |\n",
      "|0  |3  |4  |AA     |1559  |LGA|17.08 |190     |32   |1765.0|1    |1.0        |3.0    |[0.0,3.0,4.0,1.0,3.0,1765.0,17.08,190.0]  |\n",
      "|5  |9  |1  |UA     |770   |SFO|12.7  |158     |20   |1556.0|1    |0.0        |1.0    |[5.0,9.0,1.0,0.0,1.0,1556.0,12.7,158.0]   |\n",
      "|3  |10 |4  |B6     |937   |ORD|17.58 |265     |155  |2792.0|1    |4.0        |0.0    |[3.0,10.0,4.0,4.0,0.0,2792.0,17.58,265.0] |\n",
      "|11 |15 |1  |AA     |2303  |ORD|6.75  |160     |23   |1291.0|1    |1.0        |0.0    |[11.0,15.0,1.0,1.0,0.0,1291.0,6.75,160.0] |\n",
      "|8  |18 |4  |UA     |802   |SJC|6.33  |160     |17   |1526.0|1    |0.0        |4.0    |[8.0,18.0,4.0,0.0,4.0,1526.0,6.33,160.0]  |\n",
      "|2  |14 |5  |B6     |71    |JFK|6.17  |166     |0    |1519.0|0    |4.0        |2.0    |[2.0,14.0,5.0,4.0,2.0,1519.0,6.17,166.0]  |\n",
      "|7  |21 |4  |OO     |5806  |ORD|19.0  |110     |21   |977.0 |1    |2.0        |0.0    |[7.0,21.0,4.0,2.0,0.0,977.0,19.0,110.0]   |\n",
      "|11 |6  |6  |OO     |5524  |SFO|8.75  |82      |40   |509.0 |1    |2.0        |1.0    |[11.0,6.0,6.0,2.0,1.0,509.0,8.75,82.0]    |\n",
      "|0  |10 |4  |AA     |15    |JFK|11.0  |379     |100  |4162.0|1    |1.0        |2.0    |[0.0,10.0,4.0,1.0,2.0,4162.0,11.0,379.0]  |\n",
      "|11 |14 |0  |OO     |6500  |SJC|6.33  |90      |-28  |496.0 |0    |2.0        |4.0    |[11.0,14.0,0.0,2.0,4.0,496.0,6.33,90.0]   |\n",
      "|3  |1  |2  |OH     |5105  |ORD|13.83 |75      |-9   |425.0 |0    |5.0        |0.0    |[3.0,1.0,2.0,5.0,0.0,425.0,13.83,75.0]    |\n",
      "|1  |18 |1  |OO     |6761  |ORD|20.5  |102     |168  |750.0 |1    |2.0        |0.0    |[1.0,18.0,1.0,2.0,0.0,750.0,20.5,102.0]   |\n",
      "|4  |19 |1  |AA     |181   |JFK|16.92 |360     |15   |3983.0|1    |1.0        |2.0    |[4.0,19.0,1.0,1.0,2.0,3983.0,16.92,360.0] |\n",
      "|7  |24 |0  |AA     |1860  |ORD|17.42 |90      |7    |538.0 |0    |1.0        |0.0    |[7.0,24.0,0.0,1.0,0.0,538.0,17.42,90.0]   |\n",
      "|3  |21 |1  |WN     |342   |SJC|15.25 |60      |-8   |476.0 |0    |3.0        |4.0    |[3.0,21.0,1.0,3.0,4.0,476.0,15.25,60.0]   |\n",
      "|9  |26 |0  |B6     |1087  |JFK|21.67 |132     |-31  |871.0 |0    |4.0        |2.0    |[9.0,26.0,0.0,4.0,2.0,871.0,21.67,132.0]  |\n",
      "|5  |30 |1  |US     |58    |SMF|20.67 |83      |-7   |639.0 |0    |6.0        |5.0    |[5.0,30.0,1.0,6.0,5.0,639.0,20.67,83.0]   |\n",
      "|3  |30 |3  |AA     |755   |ORD|10.75 |165     |6    |1675.0|0    |1.0        |0.0    |[3.0,30.0,3.0,1.0,0.0,1675.0,10.75,165.0] |\n",
      "|6  |12 |6  |OH     |5557  |LGA|11.25 |147     |38   |1162.0|1    |5.0        |3.0    |[6.0,12.0,6.0,5.0,3.0,1162.0,11.25,147.0] |\n",
      "|11 |27 |6  |AA     |2331  |ORD|14.42 |160     |53   |1291.0|1    |1.0        |0.0    |[11.0,27.0,6.0,1.0,0.0,1291.0,14.42,160.0]|\n",
      "|9  |2  |4  |UA     |909   |ORD|15.42 |153     |-20  |1429.0|0    |0.0        |0.0    |[9.0,2.0,4.0,0.0,0.0,1429.0,15.42,153.0]  |\n",
      "|0  |7  |1  |OO     |5494  |SFO|21.0  |64      |5    |320.0 |0    |2.0        |1.0    |[0.0,7.0,1.0,2.0,1.0,320.0,21.0,64.0]     |\n",
      "|2  |21 |5  |AA     |496   |ORD|6.58  |105     |18   |985.0 |1    |1.0        |0.0    |[2.0,21.0,5.0,1.0,0.0,985.0,6.58,105.0]   |\n",
      "|4  |14 |3  |OO     |5889  |ORD|14.83 |166     |119  |1489.0|1    |2.0        |0.0    |[4.0,14.0,3.0,2.0,0.0,1489.0,14.83,166.0] |\n",
      "|7  |4  |1  |AA     |1058  |ORD|19.5  |155     |266  |1617.0|1    |1.0        |0.0    |[7.0,4.0,1.0,1.0,0.0,1617.0,19.5,155.0]   |\n",
      "|0  |22 |2  |AA     |678   |SFO|16.25 |80      |62   |542.0 |1    |1.0        |1.0    |[0.0,22.0,2.0,1.0,1.0,542.0,16.25,80.0]   |\n",
      "|2  |6  |4  |AA     |1438  |ORD|18.33 |85      |45   |658.0 |1    |1.0        |0.0    |[2.0,6.0,4.0,1.0,0.0,658.0,18.33,85.0]    |\n",
      "|9  |9  |4  |UA     |683   |LGA|14.07 |156     |-5   |1180.0|0    |0.0        |3.0    |[9.0,9.0,4.0,0.0,3.0,1180.0,14.07,156.0]  |\n",
      "|3  |11 |5  |UA     |498   |ORD|14.28 |143     |28   |1347.0|1    |0.0        |0.0    |[3.0,11.0,5.0,0.0,0.0,1347.0,14.28,143.0] |\n",
      "|3  |14 |1  |OO     |5912  |ORD|11.02 |70      |-1   |509.0 |0    |2.0        |0.0    |[3.0,14.0,1.0,2.0,0.0,509.0,11.02,70.0]   |\n",
      "|10 |3  |1  |B6     |345   |JFK|8.33  |192     |-1   |1675.0|0    |4.0        |2.0    |[10.0,3.0,1.0,4.0,2.0,1675.0,8.33,192.0]  |\n",
      "|2  |25 |2  |UA     |11    |JFK|11.17 |394     |36   |4162.0|1    |0.0        |2.0    |[2.0,25.0,2.0,0.0,2.0,4162.0,11.17,394.0] |\n",
      "|0  |5  |6  |AA     |1547  |ORD|15.25 |60      |16   |285.0 |1    |1.0        |0.0    |[0.0,5.0,6.0,1.0,0.0,285.0,15.25,60.0]    |\n",
      "|11 |25 |4  |OO     |6063  |ORD|13.23 |55      |27   |288.0 |1    |2.0        |0.0    |[11.0,25.0,4.0,2.0,0.0,288.0,13.23,55.0]  |\n",
      "|1  |1  |5  |OO     |6417  |ORD|17.55 |176     |92   |1630.0|1    |2.0        |0.0    |[1.0,1.0,5.0,2.0,0.0,1630.0,17.55,176.0]  |\n",
      "|0  |8  |2  |OO     |5449  |SMF|17.17 |45      |259  |138.0 |1    |2.0        |5.0    |[0.0,8.0,2.0,2.0,5.0,138.0,17.17,45.0]    |\n",
      "|8  |26 |5  |AA     |753   |LGA|14.75 |240     |125  |2235.0|1    |1.0        |3.0    |[8.0,26.0,5.0,1.0,3.0,2235.0,14.75,240.0] |\n",
      "|4  |6  |2  |UA     |1293  |SFO|13.17 |94      |-17  |726.0 |0    |0.0        |1.0    |[4.0,6.0,2.0,0.0,1.0,726.0,13.17,94.0]    |\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+-----+-----------+-------+------------------------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_assembled = flights_assembled.select('*').na.drop(subset=['label'])\n",
    "flights_assembled.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the necessary class\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # Create an assembler object\n",
    "# assembler = VectorAssembler(inputCols=[\n",
    "#     'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "# ], outputCol='features')\n",
    "\n",
    "# # Consolidate predictor columns\n",
    "# flights_assembled = assembler.transform(flights)\n",
    "\n",
    "# # Check the resulting column\n",
    "# flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796967376972481\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights_assembled.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80224\n"
     ]
    }
   ],
   "source": [
    "# # Split into training and testing sets in a 80:20 ratio\n",
    "# flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=43)\n",
    "\n",
    "# # Check that training set has around 80% of records\n",
    "# training_ratio = flights_train.count() / flights.count()\n",
    "# print(training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[mon: int, dom: int, dow: int, carrier: string, flight: int, org: string, mile: int, depart: double, duration: int, delay: int]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(flights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|0    |1.0       |[0.3537863838184509,0.646213616181549]  |\n",
      "|1    |1.0       |[0.44849115504682624,0.5515088449531738]|\n",
      "|1    |1.0       |[0.3537863838184509,0.646213616181549]  |\n",
      "|1    |0.0       |[0.5504121750158529,0.4495878249841471] |\n",
      "|1    |0.0       |[0.5504121750158529,0.4495878249841471] |\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1264|\n",
      "|    0|       0.0| 2501|\n",
      "|    1|       1.0| 3617|\n",
      "|    0|       1.0| 2165|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.6408295799727663\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1680|\n",
      "|    0|       0.0| 2575|\n",
      "|    1|       1.0| 3201|\n",
      "|    0|       1.0| 2091|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Selecting numeric columns\n",
    "flights_train_num = flights_train.select(\"mon\", 'depart', 'duration', 'features', 'label')\n",
    "flights_test_num = flights_test.select(\"mon\", \"depart\", \"duration\", 'features', 'label')\n",
    "\n",
    "# Create classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train_num)\n",
    "\n",
    "# Create a predictions for the test data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test_num)\n",
    "prediction.groupBy(\"label\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.60\n",
      "recall   = 0.66\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall   = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6050195127140048\n",
      "0.6424741246290961\n"
     ]
    }
   ],
   "source": [
    "print(weighted_precision)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.60\n",
      "recall    = 0.66\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('./sms.csv', sep=';', header=False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])|\n",
      "|[dont, worry, guess, busy]      |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377]) |\n",
      "|[call, freephone]               |(1024,[133,138],[5.367951058306837,2.273418200008753])                                              |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])    |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "sms = wrangled.select('id', 'words', 'label')\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms').transform(sms)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024).transform(wrangled)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features').fit(wrangled).transform(wrangled)\n",
    "\n",
    "tf_idf.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   39|\n",
      "|    0|       0.0|  932|\n",
      "|    1|       1.0|  121|\n",
      "|    0|       1.0|    4|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms = tf_idf.select('label', 'features')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.97\n",
      "recall   = 0.76\n",
      "0.9610271598998699\n",
      "0.9912793803418793\n"
     ]
    }
   ],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall   = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "print(weighted_precision)\n",
    "print(auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
