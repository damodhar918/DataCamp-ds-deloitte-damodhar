{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| NULL|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| NULL|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- mon: integer (nullable = true)\n",
      " |-- dom: integer (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- org: string (nullable = true)\n",
      " |-- mile: integer (nullable = true)\n",
      " |-- depart: double (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      "\n",
      "None\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('flights').getOrCreate()\n",
    "\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv('./flights.csv', sep=',', header=True, inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.printSchema())\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights = flights.withColumn('km', round(flights.mile * 1.60934, 0)).drop('mile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoder(\n",
    "    inputCols=['org_idx', 'dow'],\n",
    "    outputCols=['org_dummy', 'dow_dummy']\n",
    ")\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'dow_dummy'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline = pipeline.fit(flights_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = pipeline.transform(flights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  1|Sorry, I'll call ...|    0|\n",
      "|  2|Dont worry. I gue...|    0|\n",
      "|  3|Call FREEPHONE 08...|    1|\n",
      "|  4|Win a 1000 cash p...|    1|\n",
      "|  5|Go until jurong p...|    0|\n",
      "+---+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Read data from CSV file\n",
    "sms = spark.read.csv('./sms.csv', sep=';', header=False, schema=schema, nullValue='NA')\n",
    "\n",
    "sms.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Break text into tokens at non-word characters\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\n",
    "\n",
    "# Apply the hashing trick and transform to TF-IDF\n",
    "hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol='hash')\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol='features')\n",
    "\n",
    "# Create a logistic regression object and add everything to a pipeline\n",
    "logistic = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+------+--------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|    km|features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+--------+\n",
      "| 11| 20|  6|     US|    19|JFK|  9.48|     351| NULL|3465.0|[3465.0]|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 16.33|      82|   30| 509.0| [509.0]|\n",
      "|  2| 20|  4|     UA|   226|SFO|  6.17|      82|   -8| 542.0| [542.0]|\n",
      "|  9| 13|  1|     AA|   419|ORD| 10.33|     195|   -5|1989.0|[1989.0]|\n",
      "|  4|  2|  5|     AA|   325|ORD|  8.92|      65| NULL| 415.0| [415.0]|\n",
      "+---+---+---+-------+------+---+------+--------+-----+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['km'], outputCol='features')\n",
    "\n",
    "flights = assembler.transform(flights.drop('features'))\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidatorModel_58be9d4e89bb"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Create an empty parameter grid\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# Create objects for building and evaluating a regression model\n",
    "regression = LinearRegression(labelCol='duration')\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# Create a cross validator\n",
    "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Train and test model on multiple folds of the training data\n",
    "cv = cv.fit(flights_train)\n",
    "cv\n",
    "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty paramter grid\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# Create regression model\n",
    "regression = LinearRegression(labelCol='duration')\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# Create an indexer for the org field\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# Create an one-hot encoder for the indexed org field\n",
    "onehot = OneHotEncoder(inputCol='org_idx', outputCol='org_dummy')\n",
    "\n",
    "# Assemble the km and one-hot encoded fields\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# Create a pipeline and cross-validator\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer for the org field\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# Create an one-hot encoder for the indexed org field\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Assemble the km and one-hot encoded fields\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# Create a pipeline and cross-validator.\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  12\n"
     ]
    }
   ],
   "source": [
    "# Create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grids for two parameters\n",
    "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0])\\\n",
    "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "\n",
    "# Build the parameter grid\n",
    "params = params.build()\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, \n",
    "                    evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexerModel: uid=StringIndexer_57a57fa3f95a, handleInvalid=error, OneHotEncoderModel: uid=OneHotEncoder_9c94b567ac87, dropLast=true, handleInvalid=error, VectorAssembler_46e3f09e608a, LinearRegressionModel: uid=LinearRegression_5c48066dcda7, numFeatures=8]\n",
      "RMSE = 11.315301114936847\n"
     ]
    }
   ],
   "source": [
    "# Drop the existed feature column\n",
    "flights_train, flights_test = flights.drop('features').randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the data\n",
    "cvModel = cv.fit(flights_train)\n",
    "\n",
    "# Get the best model from cross validation\n",
    "best_model = cvModel.bestModel\n",
    "\n",
    "# Look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# Get the parameters for the LinearRegression object in the best model\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "# Generate predictions on test data using the best model then calculate RMSE\n",
    "predictions = best_model.transform(flights_test)\n",
    "print(\"RMSE =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the best model from cross validation\n",
    "# best_model = cv.bestModel\n",
    "\n",
    "# # Look at the stages in the best model\n",
    "# print(best_model.stages)\n",
    "\n",
    "# # Get the parameters for the LinearRegression object in the best model\n",
    "# best_model.stages[3].extractParamMap()\n",
    "\n",
    "# # Generate predictions on testing data using the best model then calculate RMSE\n",
    "# predictions = best_model.transform(flights_test)\n",
    "# print(\"RMSE =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  72\n"
     ]
    }
   ],
   "source": [
    "# Create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grid for hashing trick parameters\n",
    "params = params.addGrid(hasher.numFeatures, (1024, 4096, 16384))\\\n",
    "               .addGrid(hasher.binary, (True, False))\n",
    "\n",
    "# Add grid for logistic regression parameters\n",
    "params = params.addGrid(logistic.regParam, (0.01, 0.1, 1.0, 10.0))\\\n",
    "               .addGrid(logistic.elasticNetParam, (0.0, 0.5, 1.0))\n",
    "\n",
    "# Build parameter grid\n",
    "params = params.build()\n",
    "\n",
    "print('Number of models to be tested: ', len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----------------+-----+\n",
      "|mon|depart|duration|         features|label|\n",
      "+---+------+--------+-----------------+-----+\n",
      "|  0| 16.33|      82| [0.0,16.33,82.0]|    1|\n",
      "|  2|  6.17|      82|  [2.0,6.17,82.0]|    0|\n",
      "|  9| 10.33|     195|[9.0,10.33,195.0]|    0|\n",
      "|  5|  7.98|     102| [5.0,7.98,102.0]|    0|\n",
      "|  7| 10.83|     135|[7.0,10.83,135.0]|    1|\n",
      "+---+------+--------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['mon', 'depart', 'duration'], outputCol='features')\n",
    "flights = assembler.transform(flights.drop('features'))\n",
    "flights = flights.withColumn('label', (flights.delay >= 15).cast('integer'))\n",
    "flights = flights.select('mon', 'depart', 'duration', 'features', 'label')\n",
    "flights = flights.dropna()\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeRegressionModel: uid=dtr_46cc5d298f73, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_94e6bacd12f1, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_a229ec41c4a7, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_666f136ac103, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_8a867e6cd431, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_0e275e0b9a66, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_66fd33f9643d, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_b1859b1e2227, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_f0342ab5385d, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_fd925bf00459, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_b8429acbe13a, depth=5, numNodes=59, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_c6393c4c43e0, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_8f772a5b652d, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_ee1a74a5a7e6, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_a0d8edbc5b8b, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_2067c4051482, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_97aad2cc949b, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_27f2fd12d348, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_6c3130fa5591, depth=5, numNodes=63, numFeatures=3,\n",
      " DecisionTreeRegressionModel: uid=dtr_81cb9b489bb9, depth=5, numNodes=63, numFeatures=3]\n",
      "(3,[0,1,2],[0.3620215021193922,0.31479204980678727,0.32318644807382035])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pprint import pprint\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Create model objects and train on training data\n",
    "tree = DecisionTreeClassifier().fit(flights_train)\n",
    "gbt = GBTClassifier().fit(flights_train)\n",
    "\n",
    "# Compare AUC on test data\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(tree.transform(flights_test))\n",
    "evaluator.evaluate(gbt.transform(flights_test))\n",
    "\n",
    "# Find the number of trees and the relative importance of features\n",
    "pprint(gbt.trees)\n",
    "print(gbt.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6317135994213321\n",
      "0.6715623984909723\n",
      "20\n",
      "(3,[0,1,2],[0.3620215021193922,0.31479204980678727,0.32318644807382035])\n"
     ]
    }
   ],
   "source": [
    "# Import the classes required\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create model objects and train on training data\n",
    "tree = DecisionTreeClassifier().fit(flights_train)\n",
    "gbt = GBTClassifier().fit(flights_train)\n",
    "\n",
    "# Compare AUC on testing data\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(evaluator.evaluate(tree.transform(flights_test)))\n",
    "print(evaluator.evaluate(gbt.transform(flights_test)))\n",
    "\n",
    "# Find the number of trees and the relative importance of features\n",
    "print(gbt.getNumTrees)\n",
    "print(gbt.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Create a parameter grid\n",
    "params = ParamGridBuilder() \\\n",
    "        .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
    "        .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
    "        .build()\n",
    "\n",
    "# Create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Create a cross-validator\n",
    "cv = CrossValidator(estimator=forest, estimatorParamMaps=params, \n",
    "                    evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_12f77b69ac92"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6192442309867001, 0.660902695918036, 0.6707696192701326, 0.6406393858684399, 0.6648784750709386, 0.6756797541227204, 0.6419117412289781, 0.6637761737910489, 0.6734437206206086, 0.6419117412289781, 0.6637761737910489, 0.6734437206206086]\n",
      "0.6756797541227204\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto, current: onethird)\n",
      "0.6726534961105283\n"
     ]
    }
   ],
   "source": [
    "cv = cv.fit(flights_train)\n",
    "# Average AUC for each parameter combination in grid\n",
    "print(cv.avgMetrics)\n",
    "\n",
    "# Average AUC for the best model\n",
    "print(max(cv.avgMetrics))\n",
    "\n",
    "# What's the optimal parameter value for maxDepth?\n",
    "print(cv.bestModel.explainParam('maxDepth'))\n",
    "# What's the optimal parameter value for featureSubsetStrategy?\n",
    "print(cv.bestModel.explainParam('featureSubsetStrategy'))\n",
    "\n",
    "# AUC for best model on testing data\n",
    "print(evaluator.evaluate(cv.transform(flights_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6726534961105283\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(flights_train)\n",
    "\n",
    "# Average AUC for each parameter combination in grid\n",
    "avg_auc = cvModel.avgMetrics\n",
    "\n",
    "# Average AUC for the best model\n",
    "best_model_auc = max(avg_auc)\n",
    "\n",
    "# What's the optimal paramter value?\n",
    "opt_max_depth = cvModel.bestModel.explainParam('maxDepth')\n",
    "opt_feat_substrat = cvModel.bestModel.explainParam('featureSubsetStrategy')\n",
    "\n",
    "# AUC for best model on test data\n",
    "best_auc = evaluator.evaluate(cvModel.transform(flights_test))\n",
    "print(best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
