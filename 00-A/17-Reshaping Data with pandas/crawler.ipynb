{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JDAMOD~1\\AppData\\Local\\Temp/ipykernel_42576/1516467491.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtable_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tbody'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable_body\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpicture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data-src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Get basic players information for all players\n",
    "base_url = \"https://sofifa.com/players?offset=\"\n",
    "columns = ['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall', 'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special']\n",
    "data = pd.DataFrame(columns = columns)\n",
    "\n",
    "\n",
    "for offset in range(0, 335):\n",
    "    url = base_url + str(offset * 60)\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'html.parser')\n",
    "    table_body = soup.find('tbody')\n",
    "    print(table_body)\n",
    "    for row in table_body.find_all('tr'):\n",
    "        td = row.find_all('td')\n",
    "        picture = td[0].find('img').get('data-src')\n",
    "        pid = td[0].find('img').get('id')\n",
    "        nationality = td[1].find('img').get('title')\n",
    "        flag_img = td[1].find('img').get('data-src')\n",
    "        name = td[1].find(\"a\").get(\"data-tooltip\")\n",
    "        age = td[2].text\n",
    "        overall = td[3].text.strip()\n",
    "        potential = td[4].text.strip()\n",
    "        club = td[5].find('a').text\n",
    "        club_logo = td[5].find('img').get('data-src')\n",
    "        value = td[6].text.strip()\n",
    "        wage = td[7].text.strip()\n",
    "        special = td[8].text.strip()\n",
    "        player_data = pd.DataFrame([[pid, name, age, picture, nationality, flag_img, overall, potential, club, club_logo, value, wage, special]])\n",
    "        player_data.columns = columns\n",
    "        data = data.append(player_data, ignore_index=True)\n",
    "    print(\"done for \"+str(offset),end=\"\\r\")\n",
    "data = data.drop_duplicates()\n",
    "data.head()\n",
    "\n",
    "# Get detailed player information from player page\n",
    "detailed_columns = ['Preferred Foot','Weak Foot','Skill Moves','International Reputation','Work Rate','Body Type','Real Face','Release Clause','Position','Jersey Number','Joined','Contract Valid Until','Height','Weight','LS','ST','RS','LW','LF','CF','RF','RW','LAM','CAM','RAM','LM','LCM','CM','RCM','RM','LWB','LDM','CDM','RDM','RWB','LB','LCB','CB','RCB','RB','GK','Likes','Dislikes','Following','Crossing','Finishing','Heading Accuracy','Short Passing','Volleys','Dribbling','Curve','FK Accuracy','Long Passing','Ball Control','Acceleration','Sprint Speed','Agility','Reactions','Balance','Shot Power','Jumping','Stamina','Strength','Long Shots','Aggression','Interceptions','Positioning','Vision','Penalties','Composure','Defensive Awareness','Standing Tackle','Sliding Tackle','GK Diving','GK Handling','GK Kicking','GK Positioning','GK Reflexes']\n",
    "detailed_data = pd.DataFrame(index = range(0, data.count()[0]), columns = detailed_columns)\n",
    "detailed_data[\"ID\"] = data[\"ID\"].values\n",
    "\n",
    "player_data_url = 'https://sofifa.com/player/'\n",
    "count = 0\n",
    "for id in data[\"ID\"][:20]:\n",
    "    url = player_data_url + str(id)\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'html.parser')\n",
    "    skill_map = {}\n",
    "    columns = soup.find(\"div\", {\"class\":\"columns\"})\n",
    "    columns12 = columns.find_all(\"div\",{\"class\":\"column col-12\"})\n",
    "    for column in columns12:\n",
    "        skills = column.find_all('li')\n",
    "        for skill in skills:\n",
    "            if(skill.find('label') != None):\n",
    "                label = skill.find('label').text\n",
    "                value = skill.text.replace(label, '').strip()\n",
    "                skill_map[label] = value\n",
    "    meta_data = soup.find('div', {'class': 'meta'}).text.split(' ')\n",
    "    length = len(meta_data)\n",
    "    weight = meta_data[length - 1]\n",
    "    height = meta_data[length - 2].split('\\'')[0] + '\\'' + meta_data[length - 2].split('\\'')[1].split('\\\"')[0]\n",
    "    skill_map[\"Height\"] = height\n",
    "    skill_map['Weight'] = weight\n",
    "    if('Position' in skill_map.keys()):\n",
    "        if skill_map['Position'] in ('', 'RES', 'SUB'):\n",
    "            skill_map['Position'] = soup.find('div', {'class': 'meta bp3-text-overflow-ellipsis'}).find('span').text\n",
    "        if(skill_map['Position'] != 'GK'):\n",
    "            card_rows = soup.find(\"div\",{\"class\":\"lineup\"}).find_all(\"div\",{\"class\":\"column col-sm-2\"})\n",
    "            for attribute in card_rows:\n",
    "                if(attribute.find('div')):\n",
    "                    name = ''.join(re.find_all('[a-zA-Z]', attribute.text))\n",
    "                    value = attribute.text.replace(name, '').strip()\n",
    "                    skill_map[str(name)] = value\n",
    "    skill_map[\"Likes\"] = columns12[3].find(\"button\",{\"class\":\"bp3-button like-btn need-sign-in\"}).find(\"span\",{\"class\":\"count\"}).text\n",
    "    skill_map[\"Dislikes\"] = columns12[3].find(\"button\",{\"class\":\"bp3-button dislike-btn need-sign-in\"}).find(\"span\",{\"class\":\"count\"}).text\n",
    "    skill_map[\"Following\"] = columns12[3].find(\"button\",{\"rel\":\"nofollow\"}).find(\"span\",{\"class\":\"count\"}).text\n",
    "    name = []\n",
    "    value = []\n",
    "    columns3 = columns.find_all(\"ul\",{\"class\":\"pl\"})\n",
    "    switch = 0\n",
    "    for column in columns3[3:]:\n",
    "        for li in column.find_all(\"li\"):\n",
    "            text = li.text\n",
    "            name.append(text[2:].strip(\" \").rstrip())\n",
    "            value.append(text[:2].strip(\" \").rstrip())\n",
    "    for name, value in zip(name[:-2],value[:-2]):\n",
    "        skill_map[name] = value\n",
    "    count = count + 1\n",
    "    print(\"Loaded so far: \"+str(count)+\"/\"+str(data.shape[0]), end=\"\\r\")\n",
    "    for key, value in skill_map.items():\n",
    "        detailed_data.loc[detailed_data[\"ID\"] == id, key] = value\n",
    "\n",
    "full_data = pd.merge(data, detailed_data.iloc[:,:79], how = 'inner', on = 'ID')\n",
    "full_data.to_csv('data.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
