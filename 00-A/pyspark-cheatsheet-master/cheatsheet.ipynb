{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af8fc07",
   "metadata": {},
   "source": [
    "## Run This Code First! This creates the Spark session and loads data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c449825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install delta-spark --upgrade delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a365f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976aeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install delta-spark==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7cc3d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoCredentialsError",
     "evalue": "Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour/file/path.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./requirements.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_fileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\boto3\\s3\\inject.py:636\u001b[0m, in \u001b[0;36mupload_fileobj\u001b[1;34m(self, Fileobj, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m create_transfer_manager(\u001b[38;5;28mself\u001b[39m, config) \u001b[38;5;28;01mas\u001b[39;00m manager:\n\u001b[0;32m    629\u001b[0m     future \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mupload(\n\u001b[0;32m    630\u001b[0m         fileobj\u001b[38;5;241m=\u001b[39mFileobj,\n\u001b[0;32m    631\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mBucket,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m         subscribers\u001b[38;5;241m=\u001b[39msubscribers,\n\u001b[0;32m    635\u001b[0m     )\n\u001b[1;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\tasks.py:139\u001b[0m, in \u001b[0;36mTask.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# If the task is not done (really only if some other related\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# task to the TransferFuture had failed) then execute the task's\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# main() method.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\tasks.py:162\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Log what is about to be executed.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_final:\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\upload.py:764\u001b[0m, in \u001b[0;36mPutObjectTask._main\u001b[1;34m(self, client, fileobj, bucket, key, extra_args)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m:param client: The client to use when calling PutObject\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m:param fileobj: The file to upload.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;124;03m    used in the upload.\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fileobj \u001b[38;5;28;01mas\u001b[39;00m body:\n\u001b[1;32m--> 764\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\client.py:963\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    959\u001b[0m     maybe_compress_request(\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[0;32m    961\u001b[0m     )\n\u001b[0;32m    962\u001b[0m     apply_request_checksum(request_dict)\n\u001b[1;32m--> 963\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    969\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, operation_name\u001b[38;5;241m=\u001b[39moperation_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    974\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[0;32m    975\u001b[0m )\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\client.py:986\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 986\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    989\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    990\u001b[0m                 service_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[0;32m    995\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[1;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[0;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m         operation_model,\n\u001b[0;32m    117\u001b[0m         request_dict,\n\u001b[0;32m    118\u001b[0m     )\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\endpoint.py:198\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[1;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[0;32m    196\u001b[0m context \u001b[38;5;241m=\u001b[39m request_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m--> 198\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[0;32m    200\u001b[0m     request, operation_model, context\n\u001b[0;32m    201\u001b[0m )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[0;32m    203\u001b[0m     attempts,\n\u001b[0;32m    204\u001b[0m     operation_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m     exception,\n\u001b[0;32m    208\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\endpoint.py:134\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[1;34m(self, params, operation_model)\u001b[0m\n\u001b[0;32m    130\u001b[0m     service_id \u001b[38;5;241m=\u001b[39m operation_model\u001b[38;5;241m.\u001b[39mservice_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\n\u001b[0;32m    131\u001b[0m     event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest-created.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{op_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    132\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, op_name\u001b[38;5;241m=\u001b[39moperation_model\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    133\u001b[0m     )\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m prepared_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(request)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepared_request\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    411\u001b[0m     aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maliased_event_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m             handlers.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[1;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[0;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[1;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\signers.py:105\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[1;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# Don't call this method directly.\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\signers.py:189\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[1;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 189\u001b[0m \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\auth.py:418\u001b[0m, in \u001b[0;36mSigV4Auth.add_auth\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_auth\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoCredentialsError()\n\u001b[0;32m    419\u001b[0m     datetime_now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m    420\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datetime_now\u001b[38;5;241m.\u001b[39mstrftime(SIGV4_TIMESTAMP)\n",
      "\u001b[1;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'damodhar918'\n",
    "file_path = 'your/file/path.txt'\n",
    "\n",
    "with open('./requirements.txt', 'rb') as data:\n",
    "    s3.upload_fileobj(data, bucket_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29471a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17d2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://USBLRJDAMO9.us.deloitte.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>cheatsheet</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2780c77fb50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our Spark session and SQL Context.\n",
    "warehouse_path = \"file:///{}\\spark_warehouse\".format(os.getcwd())\n",
    "builder = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.executor.memory\", \"2G\")\n",
    "    .config(\"spark.driver.memory\", \"2G\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_path)\n",
    "    .appName(\"cheatsheet\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "# sqlContext = SparkSession.builder.getOrCreate().sparkContext\n",
    "sqlContext = SQLContext(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.conf import SparkConf\n",
    "# # Create our Spark session and SQL Context.\n",
    "# warehouse_path = \"file:///{}\\spark_warehouse\".format(os.getcwd())\n",
    "# conf = SparkConf()\\\n",
    "#     .setAppName(\"cheatsheet\")\\\n",
    "#     .setMaster(\"local[*]\")\\\n",
    "#     .set(\"spark.executor.memory\", \"2G\")\\\n",
    "#     .set(\"spark.driver.memory\", \"2G\")\\\n",
    "#     .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "#     .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "#     .set(\"spark.sql.warehouse.dir\", warehouse_path)\n",
    "\n",
    "# spark = SparkSession.builder\\\n",
    "#     .config(conf=conf)\\\n",
    "#     .getOrCreate()\n",
    "# sqlContext = SQLContext(spark)\n",
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3e046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///c:\\\\Users\\\\jdamodhar\\\\Desktop\\\\python_essential\\\\DataCamp-ds-deloitte-master\\\\00-A\\\\pyspark-cheatsheet-master\\\\spark_warehouse'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warehouse_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmodified Auto dataset.\n",
    "auto_df = spark.read.format(\"csv\").option(\"header\", True).load(\"data/auto-mpg.csv\")\n",
    "\n",
    "# Fixed Auto dataset.\n",
    "auto_df_fixed = spark.read.format(\"csv\").option(\"header\", True).load(\"data/auto-mpg-fixed.csv\")\n",
    "for (column_name) in (\"mpg cylinders displacement horsepower weight acceleration\".split()):\n",
    "    auto_df_fixed = auto_df_fixed.withColumn(column_name, col(column_name).cast(\"double\"))\n",
    "auto_df_fixed = auto_df_fixed.withColumn(\"modelyear\", col(\"modelyear\").cast(\"int\"))\n",
    "auto_df_fixed = auto_df_fixed.withColumn(\"origin\", col(\"origin\").cast(\"int\"))\n",
    "\n",
    "# Cover type dataset.\n",
    "covtype_df = spark.read.format(\"parquet\").load(\"data/covtype.parquet\")\n",
    "for column_name in covtype_df.columns:\n",
    "    covtype_df = covtype_df.withColumn(column_name, col(column_name).cast(\"int\"))\n",
    "\n",
    "# Customer spend dataset.\n",
    "spend_df = spark.read.format(\"csv\").option(\"header\", True).load(\"data/customer_spend.csv\")\n",
    "\n",
    "# Weblog.\n",
    "weblog_df = spark.read.format(\"csv\").option(\"header\", True).load(\"data/weblog.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9ed6d",
   "metadata": {},
   "source": [
    "## Loading data stored in filesystems or databases, and saving it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bf903",
   "metadata": {},
   "source": [
    "**Load a DataFrame from CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00494ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+----------------------------+\n",
      "|mpg |cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|carname                     |\n",
      "+----+---------+------------+----------+------+------------+---------+------+----------------------------+\n",
      "|18.0|8        |307.0       |130.0     |3504. |12.0        |70       |1     |chevrolet chevelle malibu   |\n",
      "|15.0|8        |350.0       |165.0     |3693. |11.5        |70       |1     |buick skylark 320           |\n",
      "|18.0|8        |318.0       |150.0     |3436. |11.0        |70       |1     |plymouth satellite          |\n",
      "|16.0|8        |304.0       |150.0     |3433. |12.0        |70       |1     |amc rebel sst               |\n",
      "|17.0|8        |302.0       |140.0     |3449. |10.5        |70       |1     |ford torino                 |\n",
      "|15.0|8        |429.0       |198.0     |4341. |10.0        |70       |1     |ford galaxie 500            |\n",
      "|14.0|8        |454.0       |220.0     |4354. |9.0         |70       |1     |chevrolet impala            |\n",
      "|14.0|8        |440.0       |215.0     |4312. |8.5         |70       |1     |plymouth fury iii           |\n",
      "|14.0|8        |455.0       |225.0     |4425. |10.0        |70       |1     |pontiac catalina            |\n",
      "|15.0|8        |390.0       |190.0     |3850. |8.5         |70       |1     |amc ambassador dpl          |\n",
      "|15.0|8        |383.0       |170.0     |3563. |10.0        |70       |1     |dodge challenger se         |\n",
      "|14.0|8        |340.0       |160.0     |3609. |8.0         |70       |1     |plymouth 'cuda 340          |\n",
      "|15.0|8        |400.0       |150.0     |3761. |9.5         |70       |1     |chevrolet monte carlo       |\n",
      "|14.0|8        |455.0       |225.0     |3086. |10.0        |70       |1     |buick estate wagon (sw)     |\n",
      "|24.0|4        |113.0       |95.00     |2372. |15.0        |70       |3     |toyota corona mark ii       |\n",
      "|22.0|6        |198.0       |95.00     |2833. |15.5        |70       |1     |plymouth duster             |\n",
      "|18.0|6        |199.0       |97.00     |2774. |15.5        |70       |1     |amc hornet                  |\n",
      "|21.0|6        |200.0       |85.00     |2587. |16.0        |70       |1     |ford maverick               |\n",
      "|27.0|4        |97.00       |88.00     |2130. |14.5        |70       |3     |datsun pl510                |\n",
      "|26.0|4        |97.00       |46.00     |1835. |20.5        |70       |2     |volkswagen 1131 deluxe sedan|\n",
      "+----+---------+------------+----------+------+------------+---------+------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"data/auto-mpg.csv\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ca6d4",
   "metadata": {},
   "source": [
    "**Load a DataFrame from a Tab Separated Value (TSV) file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e197530d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+----------------------------+\n",
      "|mpg |cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|carname                     |\n",
      "+----+---------+------------+----------+------+------------+---------+------+----------------------------+\n",
      "|18.0|8        |307.0       |130.0     |3504. |12.0        |70       |1     |chevrolet chevelle malibu   |\n",
      "|15.0|8        |350.0       |165.0     |3693. |11.5        |70       |1     |buick skylark 320           |\n",
      "|18.0|8        |318.0       |150.0     |3436. |11.0        |70       |1     |plymouth satellite          |\n",
      "|16.0|8        |304.0       |150.0     |3433. |12.0        |70       |1     |amc rebel sst               |\n",
      "|17.0|8        |302.0       |140.0     |3449. |10.5        |70       |1     |ford torino                 |\n",
      "|15.0|8        |429.0       |198.0     |4341. |10.0        |70       |1     |ford galaxie 500            |\n",
      "|14.0|8        |454.0       |220.0     |4354. |9.0         |70       |1     |chevrolet impala            |\n",
      "|14.0|8        |440.0       |215.0     |4312. |8.5         |70       |1     |plymouth fury iii           |\n",
      "|14.0|8        |455.0       |225.0     |4425. |10.0        |70       |1     |pontiac catalina            |\n",
      "|15.0|8        |390.0       |190.0     |3850. |8.5         |70       |1     |amc ambassador dpl          |\n",
      "|15.0|8        |383.0       |170.0     |3563. |10.0        |70       |1     |dodge challenger se         |\n",
      "|14.0|8        |340.0       |160.0     |3609. |8.0         |70       |1     |plymouth 'cuda 340          |\n",
      "|15.0|8        |400.0       |150.0     |3761. |9.5         |70       |1     |chevrolet monte carlo       |\n",
      "|14.0|8        |455.0       |225.0     |3086. |10.0        |70       |1     |buick estate wagon (sw)     |\n",
      "|24.0|4        |113.0       |95.00     |2372. |15.0        |70       |3     |toyota corona mark ii       |\n",
      "|22.0|6        |198.0       |95.00     |2833. |15.5        |70       |1     |plymouth duster             |\n",
      "|18.0|6        |199.0       |97.00     |2774. |15.5        |70       |1     |amc hornet                  |\n",
      "|21.0|6        |200.0       |85.00     |2587. |16.0        |70       |1     |ford maverick               |\n",
      "|27.0|4        |97.00       |88.00     |2130. |14.5        |70       |3     |datsun pl510                |\n",
      "|26.0|4        |97.00       |46.00     |1835. |20.5        |70       |2     |volkswagen 1131 deluxe sedan|\n",
      "+----+---------+------------+----------+------+------------+---------+------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .load(\"data/auto-mpg.tsv\")\n",
    ")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bd3e6",
   "metadata": {},
   "source": [
    "**Save a DataFrame in CSV format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ad41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.write.mode('overwrite').csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdef498",
   "metadata": {},
   "source": [
    "**Load a DataFrame from Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c63e8ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0| 3504.|        12.0|       70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|     165.0| 3693.|        11.5|       70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0|     150.0| 3436.|        11.0|       70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0|     150.0| 3433.|        12.0|       70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0|     140.0| 3449.|        10.5|       70|     1|         ford torino|\n",
      "|15.0|        8|       429.0|     198.0| 4341.|        10.0|       70|     1|    ford galaxie 500|\n",
      "|14.0|        8|       454.0|     220.0| 4354.|         9.0|       70|     1|    chevrolet impala|\n",
      "|14.0|        8|       440.0|     215.0| 4312.|         8.5|       70|     1|   plymouth fury iii|\n",
      "|14.0|        8|       455.0|     225.0| 4425.|        10.0|       70|     1|    pontiac catalina|\n",
      "|15.0|        8|       390.0|     190.0| 3850.|         8.5|       70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|       383.0|     170.0| 3563.|        10.0|       70|     1| dodge challenger se|\n",
      "|14.0|        8|       340.0|     160.0| 3609.|         8.0|       70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|       400.0|     150.0| 3761.|         9.5|       70|     1|chevrolet monte c...|\n",
      "|14.0|        8|       455.0|     225.0| 3086.|        10.0|       70|     1|buick estate wago...|\n",
      "|24.0|        4|       113.0|     95.00| 2372.|        15.0|       70|     3|toyota corona mar...|\n",
      "|22.0|        6|       198.0|     95.00| 2833.|        15.5|       70|     1|     plymouth duster|\n",
      "|18.0|        6|       199.0|     97.00| 2774.|        15.5|       70|     1|          amc hornet|\n",
      "|21.0|        6|       200.0|     85.00| 2587.|        16.0|       70|     1|       ford maverick|\n",
      "|27.0|        4|       97.00|     88.00| 2130.|        14.5|       70|     3|        datsun pl510|\n",
      "|26.0|        4|       97.00|     46.00| 1835.|        20.5|       70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"data/auto-mpg.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b351a78",
   "metadata": {},
   "source": [
    "**Save a DataFrame in Parquet format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf83fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.write.mode('overwrite').parquet(\"output.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1070509",
   "metadata": {},
   "source": [
    "**Load a DataFrame from JSON Lines (jsonl) Formatted Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d9342d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+--------------------------------+--------+----------+-------------------------------------------------------+------+\n",
      "|client                                                                                                                        |country                         |session |timestamp |uri                                                    |user  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+--------------------------------+--------+----------+-------------------------------------------------------+------+\n",
      "|{false, Mozilla/5.0 (X11; Linux i686) AppleWebKit/5310 (KHTML, like Gecko) Chrome/21.0.855.0 Safari/5310}                     |Bangladesh                      |55fa8213|869196249 |http://larson-harper.com/list/post/                    |dde312|\n",
      "|{true, Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/5331 (KHTML, like Gecko) Chrome/48.0.880.0 Safari/5331}    |Niue                            |2fcd4a83|1031238717|http://hahn.com/post.html                              |9d00b9|\n",
      "|{true, Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_12_9) AppleWebKit/5361 (KHTML, like Gecko) Chrome/42.0.868.0 Safari/5361}   |Rwanda                          |013b996e|628683372 |http://mann-cruz.biz/list/explore/home.html            |1339d4|\n",
      "|{false, Mozilla/5.0 (Windows CE) AppleWebKit/5320 (KHTML, like Gecko) Chrome/55.0.820.0 Safari/5320}                          |Austria                         |07e8a71a|1043628668|https://www.jones.com/tags/blog/privacy.html           |966312|\n",
      "|{false, Mozilla/5.0 (Windows NT 5.2) AppleWebKit/5310 (KHTML, like Gecko) Chrome/37.0.889.0 Safari/5310}                      |Belize                          |b23d05d8|192738669 |http://rose-coleman.com/app/blog/about.html            |2af1e1|\n",
      "|{false, Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_6) AppleWebKit/5342 (KHTML, like Gecko) Chrome/59.0.871.0 Safari/5342}    |Lao People's Democratic Republic|d83dfbae|1066490444|http://shepherd.com/categories/wp-content/list/main.php|844395|\n",
      "|{false, Mozilla/5.0 (Windows NT 5.01) AppleWebKit/5330 (KHTML, like Gecko) Chrome/58.0.813.0 Safari/5330}                     |French Guiana                   |e77dfaa2|1350920869|https://rose-wilson.org/                               |NULL  |\n",
      "|{false, Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_12_5) AppleWebKit/5362 (KHTML, like Gecko) Chrome/36.0.812.0 Safari/5362}  |Turks and Caicos Islands        |56664269|280986223 |http://owens-scott.info/                               |NULL  |\n",
      "|{false, Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5321 (KHTML, like Gecko) Chrome/58.0.892.0 Safari/5321}                   |Ethiopia                        |628d6059|881914195 |https://higgins.com/posts/wp-content/search/terms/     |8ab45a|\n",
      "|{false, Mozilla/5.0 (Macintosh; Intel Mac OS X 10_5_1) AppleWebKit/5342 (KHTML, like Gecko) Chrome/59.0.883.0 Safari/5342}    |Saint Kitts and Nevis           |85f9120c|1065114708|https://www.gordon.com/app/tag/search/index.php        |NULL  |\n",
      "|{true, Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5360 (KHTML, like Gecko) Chrome/42.0.845.0 Safari/5360}                    |Niue                            |b582eb2d|903885352 |http://taylor.com/post.htm                             |NULL  |\n",
      "|{true, Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5330 (KHTML, like Gecko) Chrome/54.0.839.0 Safari/5330}                    |Mozambique                      |3a3a076a|1396767240|https://www.walker-kelley.org/register/                |c9cfbc|\n",
      "|{false, Mozilla/5.0 (Macintosh; PPC Mac OS X 10_7_2) AppleWebKit/5360 (KHTML, like Gecko) Chrome/18.0.851.0 Safari/5360}      |Mauritania                      |f50b92bd|846552008 |https://floyd.com/                                     |4e612e|\n",
      "|{false, Mozilla/5.0 (X11; Linux i686) AppleWebKit/5341 (KHTML, like Gecko) Chrome/41.0.828.0 Safari/5341}                     |Bangladesh                      |b22d584c|1267210959|https://www.benson.info/                               |NULL  |\n",
      "|{true, Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_5_5) AppleWebKit/5352 (KHTML, like Gecko) Chrome/49.0.826.0 Safari/5352}  |Syrian Arab Republic            |b70a5779|389952112 |https://www.simon.com/faq/                             |NULL  |\n",
      "|{false, Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5342 (KHTML, like Gecko) Chrome/20.0.869.0 Safari/5342}                   |Nicaragua                       |11cf53ff|1535167846|http://www.davis.com/register/                         |185348|\n",
      "|{false, Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_11_1) AppleWebKit/5351 (KHTML, like Gecko) Chrome/37.0.848.0 Safari/5351}|Tokelau                         |8a020624|864088605 |http://www.gallegos-foley.com/tags/terms/              |7a1c8e|\n",
      "|{false, Mozilla/5.0 (X11; Linux i686) AppleWebKit/5332 (KHTML, like Gecko) Chrome/25.0.898.0 Safari/5332}                     |Andorra                         |bbe09fd3|315508192 |http://www.johnson.com/posts/faq.asp                   |38cbc0|\n",
      "|{false, Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_6) AppleWebKit/5342 (KHTML, like Gecko) Chrome/62.0.827.0 Safari/5342}   |Guatemala                       |0824188a|1989386   |https://www.keller-ward.biz/tag/category/posts/login/  |f6804a|\n",
      "|{true, Mozilla/5.0 (Windows CE) AppleWebKit/5312 (KHTML, like Gecko) Chrome/17.0.878.0 Safari/5312}                           |Guernsey                        |798f511a|458479835 |https://scott.com/tag/blog/category/search/            |b2f5b0|\n",
      "+------------------------------------------------------------------------------------------------------------------------------+--------------------------------+--------+----------+-------------------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"data/weblog.jsonl\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d143f2f",
   "metadata": {},
   "source": [
    "**Save a DataFrame into a Hive catalog table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e13b3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto_df.write.mode(\"overwrite\").saveAsTable(\"autompg\", path=\"file:///spark_warehouse/autompg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "158ff0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"c:/Users/jdamodhar/Desktop/python_essential/DataCamp-ds-deloitte-master/00-A/pyspark-cheatsheet-master/spark_warehouse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "151789cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.write.mode(\"overwrite\").saveAsTable(\"autompg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86ba1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
    "\n",
    "auto_df.write.mode(\"overwrite\").saveAsTable(\"my_database.autompg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535f6590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " createtab_stmt | CREATE TABLE spark_catalog.my_database.autompg (\\n  mpg STRING,\\n  cylinders STRING,\\n  displacement STRING,\\n  horsepower STRING,\\n  weight STRING,\\n  acceleration STRING,\\n  modelyear STRING,\\n  origin STRING,\\n  carname STRING)\\nUSING parquet\\n \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE my_database.autompg;\").show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246e195",
   "metadata": {},
   "source": [
    "**Load a Hive catalog table into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b11a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0| 3504.|        12.0|       70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|     165.0| 3693.|        11.5|       70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0|     150.0| 3436.|        11.0|       70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0|     150.0| 3433.|        12.0|       70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0|     140.0| 3449.|        10.5|       70|     1|         ford torino|\n",
      "|15.0|        8|       429.0|     198.0| 4341.|        10.0|       70|     1|    ford galaxie 500|\n",
      "|14.0|        8|       454.0|     220.0| 4354.|         9.0|       70|     1|    chevrolet impala|\n",
      "|14.0|        8|       440.0|     215.0| 4312.|         8.5|       70|     1|   plymouth fury iii|\n",
      "|14.0|        8|       455.0|     225.0| 4425.|        10.0|       70|     1|    pontiac catalina|\n",
      "|15.0|        8|       390.0|     190.0| 3850.|         8.5|       70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|       383.0|     170.0| 3563.|        10.0|       70|     1| dodge challenger se|\n",
      "|14.0|        8|       340.0|     160.0| 3609.|         8.0|       70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|       400.0|     150.0| 3761.|         9.5|       70|     1|chevrolet monte c...|\n",
      "|14.0|        8|       455.0|     225.0| 3086.|        10.0|       70|     1|buick estate wago...|\n",
      "|24.0|        4|       113.0|     95.00| 2372.|        15.0|       70|     3|toyota corona mar...|\n",
      "|22.0|        6|       198.0|     95.00| 2833.|        15.5|       70|     1|     plymouth duster|\n",
      "|18.0|        6|       199.0|     97.00| 2774.|        15.5|       70|     1|          amc hornet|\n",
      "|21.0|        6|       200.0|     85.00| 2587.|        16.0|       70|     1|       ford maverick|\n",
      "|27.0|        4|       97.00|     88.00| 2130.|        14.5|       70|     3|        datsun pl510|\n",
      "|26.0|        4|       97.00|     46.00| 1835.|        20.5|       70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"autompg\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553656c5",
   "metadata": {},
   "source": [
    "**Load a DataFrame from a SQL query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c1bf0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----------+\n",
      "|             carname| mpg|horsepower|\n",
      "+--------------------+----+----------+\n",
      "|            bmw 2002|26.0|     113.0|\n",
      "|  chevrolet citation|28.8|     115.0|\n",
      "|oldsmobile omega ...|26.8|     115.0|\n",
      "|          dodge colt|27.9|     105.0|\n",
      "|       datsun 280-zx|32.7|     132.0|\n",
      "|oldsmobile cutlas...|26.6|     105.0|\n",
      "+--------------------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.sql(\n",
    "    \"select carname, mpg, horsepower from autompg where horsepower > 100 and mpg > 25\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a996bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"delta\").save(warehouse_path+\"/autompg-delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355d231",
   "metadata": {},
   "source": [
    "**Load a CSV file from Amazon S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02358f4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSectionError",
     "evalue": "No section: 'default'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSectionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m configparser\u001b[38;5;241m.\u001b[39mConfigParser()\n\u001b[0;32m      5\u001b[0m config\u001b[38;5;241m.\u001b[39mread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/.aws/credentials\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m access_key \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maws_access_key_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m secret_key \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws_secret_access_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Requires compatible hadoop-aws and aws-java-sdk-bundle JARs.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\configparser.py:797\u001b[0m, in \u001b[0;36mRawConfigParser.get\u001b[1;34m(self, section, option, raw, vars, fallback)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get an option value for a given section.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \n\u001b[0;32m    784\u001b[0m \u001b[38;5;124;03mIf `vars` is provided, it must be a dictionary. The option is looked up\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;124;03mThe section DEFAULT is special.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 797\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unify_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSectionError:\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fallback \u001b[38;5;129;01mis\u001b[39;00m _UNSET:\n",
      "File \u001b[1;32mc:\\Users\\jdamodhar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\configparser.py:1168\u001b[0m, in \u001b[0;36mRawConfigParser._unify_values\u001b[1;34m(self, section, vars)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m section \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_section:\n\u001b[1;32m-> 1168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSectionError(section) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;66;03m# Update with the entry specific variables\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m vardict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNoSectionError\u001b[0m: No section: 'default'"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_key = config.get(\"default\", \"aws_access_key_id\").replace('\"', \"\")\n",
    "secret_key = config.get(\"default\", \"aws_secret_access_key\").replace('\"', \"\")\n",
    "\n",
    "# Requires compatible hadoop-aws and aws-java-sdk-bundle JARs.\n",
    "spark.conf.set(\n",
    "    \"fs.s3a.aws.credentials.provider\",\n",
    "    \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    ")\n",
    "spark.conf.set(\"fs.s3a.access.key\", access_key)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", secret_key)\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)    .load(\"s3a://cheatsheet111/auto-mpg.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84988e",
   "metadata": {},
   "source": [
    "**Load a CSV file from Oracle Cloud Infrastructure (OCI) Object Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "\n",
    "oci_config = oci.config.from_file()\n",
    "conf = spark.sparkContext.getConf()\n",
    "conf.set(\"fs.oci.client.auth.tenantId\", oci_config[\"tenancy\"])\n",
    "conf.set(\"fs.oci.client.auth.userId\", oci_config[\"user\"])\n",
    "conf.set(\"fs.oci.client.auth.fingerprint\", oci_config[\"fingerprint\"])\n",
    "conf.set(\"fs.oci.client.auth.pemfilepath\", oci_config[\"key_file\"])\n",
    "conf.set(\n",
    "    \"fs.oci.client.hostname\",\n",
    "    \"https://objectstorage.{0}.oraclecloud.com\".format(oci_config[\"region\"]),\n",
    ")\n",
    "PATH = \"oci://<your_bucket>@<your_namespace/<your_path>\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d5750",
   "metadata": {},
   "source": [
    "**Read an Oracle DB table into a DataFrame using a Wallet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "password = \"my_password\"\n",
    "table = \"source_table\"\n",
    "tnsname = \"my_tns_name\"\n",
    "user = \"ADMIN\"\n",
    "wallet_path = \"/path/to/your/wallet\"\n",
    "\n",
    "properties = {\n",
    "    \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
    "    \"oracle.net.tns_admin\": tnsname,\n",
    "    \"password\": password,\n",
    "    \"user\": user,\n",
    "}\n",
    "url = f\"jdbc:oracle:thin:@{tnsname}?TNS_ADMIN={wallet_path}\"\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50084e7d",
   "metadata": {},
   "source": [
    "**Write a DataFrame to an Oracle DB table using a Wallet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "password = \"my_password\"\n",
    "table = \"target_table\"\n",
    "tnsname = \"my_tns_name\"\n",
    "user = \"ADMIN\"\n",
    "wallet_path = \"/path/to/your/wallet\"\n",
    "\n",
    "properties = {\n",
    "    \"driver\": \"oracle.jdbc.driver.OracleDriver\",\n",
    "    \"oracle.net.tns_admin\": tnsname,\n",
    "    \"password\": password,\n",
    "    \"user\": user,\n",
    "}\n",
    "url = f\"jdbc:oracle:thin:@{tnsname}?TNS_ADMIN={wallet_path}\"\n",
    "\n",
    "# Possible modes are \"Append\", \"Overwrite\", \"Ignore\", \"Error\"\n",
    "df.write.jdbc(url=url, table=table, mode=\"Append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019dc83",
   "metadata": {},
   "source": [
    "**Write a DataFrame to a Postgres table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92917172",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_database = os.environ.get(\"PGDATABASE\") or \"postgres\"\n",
    "pg_host = os.environ.get(\"PGHOST\") or \"localhost\"\n",
    "pg_password = os.environ.get(\"PGPASSWORD\") or \"password\"\n",
    "pg_user = os.environ.get(\"PGUSER\") or \"postgres\"\n",
    "table = \"autompg\"\n",
    "\n",
    "properties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_password,\n",
    "}\n",
    "url = f\"jdbc:postgresql://{pg_host}:5432/{pg_database}\"\n",
    "auto_df.write.jdbc(url=url, table=table, mode=\"Append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473f712",
   "metadata": {},
   "source": [
    "**Read a Postgres table into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_database = os.environ.get(\"PGDATABASE\") or \"postgres\"\n",
    "pg_host = os.environ.get(\"PGHOST\") or \"localhost\"\n",
    "pg_password = os.environ.get(\"PGPASSWORD\") or \"password\"\n",
    "pg_user = os.environ.get(\"PGUSER\") or \"postgres\"\n",
    "table = \"autompg\"\n",
    "\n",
    "properties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_password,\n",
    "}\n",
    "url = f\"jdbc:postgresql://{pg_host}:5432/{pg_database}\"\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c487d0b",
   "metadata": {},
   "source": [
    "## Special data handling scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31fbf38",
   "metadata": {},
   "source": [
    "**Provide the schema when loading a DataFrame from CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0da90d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0|3504.0|        12.0|       70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|     165.0|3693.0|        11.5|       70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0|     150.0|3436.0|        11.0|       70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0|     150.0|3433.0|        12.0|       70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0|     140.0|3449.0|        10.5|       70|     1|         ford torino|\n",
      "|15.0|        8|       429.0|     198.0|4341.0|        10.0|       70|     1|    ford galaxie 500|\n",
      "|14.0|        8|       454.0|     220.0|4354.0|         9.0|       70|     1|    chevrolet impala|\n",
      "|14.0|        8|       440.0|     215.0|4312.0|         8.5|       70|     1|   plymouth fury iii|\n",
      "|14.0|        8|       455.0|     225.0|4425.0|        10.0|       70|     1|    pontiac catalina|\n",
      "|15.0|        8|       390.0|     190.0|3850.0|         8.5|       70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|       383.0|     170.0|3563.0|        10.0|       70|     1| dodge challenger se|\n",
      "|14.0|        8|       340.0|     160.0|3609.0|         8.0|       70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|       400.0|     150.0|3761.0|         9.5|       70|     1|chevrolet monte c...|\n",
      "|14.0|        8|       455.0|     225.0|3086.0|        10.0|       70|     1|buick estate wago...|\n",
      "|24.0|        4|       113.0|      95.0|2372.0|        15.0|       70|     3|toyota corona mar...|\n",
      "|22.0|        6|       198.0|      95.0|2833.0|        15.5|       70|     1|     plymouth duster|\n",
      "|18.0|        6|       199.0|      97.0|2774.0|        15.5|       70|     1|          amc hornet|\n",
      "|21.0|        6|       200.0|      85.0|2587.0|        16.0|       70|     1|       ford maverick|\n",
      "|27.0|        4|        97.0|      88.0|2130.0|        14.5|       70|     3|        datsun pl510|\n",
      "|26.0|        4|        97.0|      46.0|1835.0|        20.5|       70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"mpg\", DoubleType(), True),\n",
    "        StructField(\"cylinders\", IntegerType(), True),\n",
    "        StructField(\"displacement\", DoubleType(), True),\n",
    "        StructField(\"horsepower\", DoubleType(), True),\n",
    "        StructField(\"weight\", DoubleType(), True),\n",
    "        StructField(\"acceleration\", DoubleType(), True),\n",
    "        StructField(\"modelyear\", IntegerType(), True),\n",
    "        StructField(\"origin\", IntegerType(), True),\n",
    "        StructField(\"carname\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .load(\"data/auto-mpg.csv\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df5e40",
   "metadata": {},
   "source": [
    "**Save a DataFrame to CSV, overwriting existing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42922272",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.write.mode(\"overwrite\").csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54862ca",
   "metadata": {},
   "source": [
    "**Save a DataFrame to CSV with a header**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5b3c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.coalesce(1).write.mode('overwrite').csv(\"header.csv\", header=\"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a985da",
   "metadata": {},
   "source": [
    "**Save a DataFrame in a single CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "304017c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.coalesce(1).write.mode('overwrite').csv(\"single.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27fd7ad",
   "metadata": {},
   "source": [
    "**Save DataFrame as a dynamic partitioned table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9af5631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "auto_df.write.mode(\"append\").partitionBy(\"modelyear\").saveAsTable(\n",
    "    \"autompg_partitioned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc99b6",
   "metadata": {},
   "source": [
    "**Overwrite specific partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4f5142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0| 3504.|        12.0|       70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|     165.0| 3693.|        11.5|       70|     1|   buick skylark 320|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auto_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "your_dataframe.write.mode(\"overwrite\").insertInto(\"your_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting money-parser\n",
      "  Downloading money_parser-0.0.1-py3-none-any.whl (6.4 kB)\n",
      "Installing collected packages: money-parser\n",
      "Successfully installed money-parser-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install money-parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672aee1",
   "metadata": {},
   "source": [
    "**Load a CSV file with a money column into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f517050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+\n",
      "|      date|customer_id|spend_dollars|\n",
      "+----------+-----------+-------------+\n",
      "|2020-01-31|          0|       0.0700|\n",
      "|2020-01-31|          1|       0.9800|\n",
      "|2020-01-31|          2|       0.0600|\n",
      "|2020-01-31|          3|       0.6500|\n",
      "+----------+-----------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DecimalType\n",
    "from decimal import Decimal\n",
    "\n",
    "# Load the text file.\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/customer_spend.csv\")\n",
    ")\n",
    "\n",
    "# Convert with a hardcoded custom UDF.\n",
    "money_udf = udf(lambda x: Decimal(x[1:].replace(\",\", \"\")), DecimalType(8, 4))\n",
    "money1 = df.withColumn(\"spend_dollars\", money_udf(df.spend_dollars))\n",
    "\n",
    "# Convert with the money_parser library (much safer).\n",
    "from money_parser import price_str\n",
    "\n",
    "money_convert = udf(\n",
    "    lambda x: Decimal(price_str(x)) if x is not None else None,\n",
    "    DecimalType(8, 4),\n",
    ")\n",
    "df = df.withColumn(\"spend_dollars\", money_convert(df.spend_dollars))\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc4046",
   "metadata": {},
   "source": [
    "## Adding, removing and modifying DataFrame columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e460b",
   "metadata": {},
   "source": [
    "**Add a new column to a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70876715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+--------------------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|               upper|               lower|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+--------------------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0| 3504.|        12.0|       70|     1|chevrolet chevell...|CHEVROLET CHEVELL...|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|     165.0| 3693.|        11.5|       70|     1|   buick skylark 320|   BUICK SKYLARK 320|   buick skylark 320|\n",
      "|18.0|        8|       318.0|     150.0| 3436.|        11.0|       70|     1|  plymouth satellite|  PLYMOUTH SATELLITE|  plymouth satellite|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper, lower\n",
    "\n",
    "df = auto_df.withColumn(\"upper\", upper(auto_df.carname)).withColumn(\n",
    "    \"lower\", lower(auto_df.carname)\n",
    ")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53caa355",
   "metadata": {},
   "source": [
    "**Modify a DataFrame column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93663a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0| 3504.|        12.0|     1970|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|     165.0| 3693.|        11.5|     1970|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0|     150.0| 3436.|        11.0|     1970|     1|  plymouth satellite|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "df = auto_df.withColumn(\"modelyear\", concat(lit(\"19\"), col(\"modelyear\")))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598af4a",
   "metadata": {},
   "source": [
    "**Add a column with multiple conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ff7f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " mpg          | 18.0                 \n",
      " cylinders    | 8                    \n",
      " displacement | 307.0                \n",
      " horsepower   | 130.0                \n",
      " weight       | 3504.                \n",
      " acceleration | 12.0                 \n",
      " modelyear    | 70                   \n",
      " origin       | 1                    \n",
      " carname      | chevrolet chevell... \n",
      " mpg_class    | low                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df = auto_df.withColumn(\n",
    "    \"mpg_class\",\n",
    "    when(col(\"mpg\") <= 20, \"low\")\n",
    "    .when(col(\"mpg\") <= 30, \"mid\")\n",
    "    .when(col(\"mpg\") <= 40, \"high\")\n",
    "    .otherwise(\"very high\"),\n",
    ")\n",
    "df.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f8117",
   "metadata": {},
   "source": [
    "**Add a constant column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5f37b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+---+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|one|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+---+\n",
      "|18.0|        8|       307.0|     130.0| 3504.|        12.0|       70|     1|chevrolet chevell...|  1|\n",
      "|15.0|        8|       350.0|     165.0| 3693.|        11.5|       70|     1|   buick skylark 320|  1|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = auto_df.withColumn(\"one\", lit(1))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020391a",
   "metadata": {},
   "source": [
    "**Concatenate columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c66f81d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|modelyear|origin|             carname|concatenated|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+------------+\n",
      "|18.0|        8|       307.0|     130.0| 3504.|        12.0|       70|     1|chevrolet chevell...|      8_18.0|\n",
      "|15.0|        8|       350.0|     165.0| 3693.|        11.5|       70|     1|   buick skylark 320|      8_15.0|\n",
      "+----+---------+------------+----------+------+------------+---------+------+--------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "df = auto_df.withColumn(\n",
    "    \"concatenated\", concat(col(\"cylinders\"), lit(\"_\"), col(\"mpg\"))\n",
    ")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44209a3a",
   "metadata": {},
   "source": [
    "**Drop a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f9717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+------+------------+---------+------+--------------------+\n",
      "| mpg|cylinders|displacement|weight|acceleration|modelyear|origin|             carname|\n",
      "+----+---------+------------+------+------------+---------+------+--------------------+\n",
      "|18.0|        8|       307.0| 3504.|        12.0|       70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0| 3693.|        11.5|       70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0| 3436.|        11.0|       70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0| 3433.|        12.0|       70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0| 3449.|        10.5|       70|     1|         ford torino|\n",
      "|15.0|        8|       429.0| 4341.|        10.0|       70|     1|    ford galaxie 500|\n",
      "|14.0|        8|       454.0| 4354.|         9.0|       70|     1|    chevrolet impala|\n",
      "|14.0|        8|       440.0| 4312.|         8.5|       70|     1|   plymouth fury iii|\n",
      "|14.0|        8|       455.0| 4425.|        10.0|       70|     1|    pontiac catalina|\n",
      "|15.0|        8|       390.0| 3850.|         8.5|       70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|       383.0| 3563.|        10.0|       70|     1| dodge challenger se|\n",
      "|14.0|        8|       340.0| 3609.|         8.0|       70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|       400.0| 3761.|         9.5|       70|     1|chevrolet monte c...|\n",
      "|14.0|        8|       455.0| 3086.|        10.0|       70|     1|buick estate wago...|\n",
      "|24.0|        4|       113.0| 2372.|        15.0|       70|     3|toyota corona mar...|\n",
      "|22.0|        6|       198.0| 2833.|        15.5|       70|     1|     plymouth duster|\n",
      "|18.0|        6|       199.0| 2774.|        15.5|       70|     1|          amc hornet|\n",
      "|21.0|        6|       200.0| 2587.|        16.0|       70|     1|       ford maverick|\n",
      "|27.0|        4|       97.00| 2130.|        14.5|       70|     3|        datsun pl510|\n",
      "|26.0|        4|       97.00| 1835.|        20.5|       70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+------+------------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = auto_df.drop(\"horsepower\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab977b19",
   "metadata": {},
   "source": [
    "**Change a column name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+------+------+------------+---------+------+--------------------+\n",
      "| mpg|cylinders|displacement|horses|weight|acceleration|modelyear|origin|             carname|\n",
      "+----+---------+------------+------+------+------------+---------+------+--------------------+\n",
      "|18.0|        8|       307.0| 130.0| 3504.|        12.0|       70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0| 165.0| 3693.|        11.5|       70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0| 150.0| 3436.|        11.0|       70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0| 150.0| 3433.|        12.0|       70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0| 140.0| 3449.|        10.5|       70|     1|         ford torino|\n",
      "|15.0|        8|       429.0| 198.0| 4341.|        10.0|       70|     1|    ford galaxie 500|\n",
      "|14.0|        8|       454.0| 220.0| 4354.|         9.0|       70|     1|    chevrolet impala|\n",
      "|14.0|        8|       440.0| 215.0| 4312.|         8.5|       70|     1|   plymouth fury iii|\n",
      "|14.0|        8|       455.0| 225.0| 4425.|        10.0|       70|     1|    pontiac catalina|\n",
      "|15.0|        8|       390.0| 190.0| 3850.|         8.5|       70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|       383.0| 170.0| 3563.|        10.0|       70|     1| dodge challenger se|\n",
      "|14.0|        8|       340.0| 160.0| 3609.|         8.0|       70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|       400.0| 150.0| 3761.|         9.5|       70|     1|chevrolet monte c...|\n",
      "|14.0|        8|       455.0| 225.0| 3086.|        10.0|       70|     1|buick estate wago...|\n",
      "|24.0|        4|       113.0| 95.00| 2372.|        15.0|       70|     3|toyota corona mar...|\n",
      "|22.0|        6|       198.0| 95.00| 2833.|        15.5|       70|     1|     plymouth duster|\n",
      "|18.0|        6|       199.0| 97.00| 2774.|        15.5|       70|     1|          amc hornet|\n",
      "|21.0|        6|       200.0| 85.00| 2587.|        16.0|       70|     1|       ford maverick|\n",
      "|27.0|        4|       97.00| 88.00| 2130.|        14.5|       70|     3|        datsun pl510|\n",
      "|26.0|        4|       97.00| 46.00| 1835.|        20.5|       70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+------+------+------------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = auto_df.withColumnRenamed(\"horsepower\", \"horses\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3770a",
   "metadata": {},
   "source": [
    "**Change multiple column names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bec5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+------+------+------------+----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horses|weight|acceleration|year|origin|             carname|\n",
      "+----+---------+------------+------+------+------------+----+------+--------------------+\n",
      "|18.0|        8|       307.0| 130.0| 3504.|        12.0|  70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0| 165.0| 3693.|        11.5|  70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0| 150.0| 3436.|        11.0|  70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0| 150.0| 3433.|        12.0|  70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0| 140.0| 3449.|        10.5|  70|     1|         ford torino|\n",
      "|15.0|        8|       429.0| 198.0| 4341.|        10.0|  70|     1|    ford galaxie 500|\n",
      "|14.0|        8|       454.0| 220.0| 4354.|         9.0|  70|     1|    chevrolet impala|\n",
      "|14.0|        8|       440.0| 215.0| 4312.|         8.5|  70|     1|   plymouth fury iii|\n",
      "|14.0|        8|       455.0| 225.0| 4425.|        10.0|  70|     1|    pontiac catalina|\n",
      "|15.0|        8|       390.0| 190.0| 3850.|         8.5|  70|     1|  amc ambassador dpl|\n",
      "|15.0|        8|       383.0| 170.0| 3563.|        10.0|  70|     1| dodge challenger se|\n",
      "|14.0|        8|       340.0| 160.0| 3609.|         8.0|  70|     1|  plymouth 'cuda 340|\n",
      "|15.0|        8|       400.0| 150.0| 3761.|         9.5|  70|     1|chevrolet monte c...|\n",
      "|14.0|        8|       455.0| 225.0| 3086.|        10.0|  70|     1|buick estate wago...|\n",
      "|24.0|        4|       113.0| 95.00| 2372.|        15.0|  70|     3|toyota corona mar...|\n",
      "|22.0|        6|       198.0| 95.00| 2833.|        15.5|  70|     1|     plymouth duster|\n",
      "|18.0|        6|       199.0| 97.00| 2774.|        15.5|  70|     1|          amc hornet|\n",
      "|21.0|        6|       200.0| 85.00| 2587.|        16.0|  70|     1|       ford maverick|\n",
      "|27.0|        4|       97.00| 88.00| 2130.|        14.5|  70|     3|        datsun pl510|\n",
      "|26.0|        4|       97.00| 46.00| 1835.|        20.5|  70|     2|volkswagen 1131 d...|\n",
      "+----+---------+------------+------+------+------------+----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = auto_df.withColumnRenamed(\"horsepower\", \"horses\").withColumnRenamed(\n",
    "    \"modelyear\", \"year\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e854cb6",
   "metadata": {},
   "source": [
    "**Change all column names at once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+-----------+-------+-------------+----------+-------+--------------------+\n",
      "|Xmpg|Xcylinders|Xdisplacement|Xhorsepower|Xweight|Xacceleration|Xmodelyear|Xorigin|            Xcarname|\n",
      "+----+----------+-------------+-----------+-------+-------------+----------+-------+--------------------+\n",
      "|18.0|         8|        307.0|      130.0|  3504.|         12.0|        70|      1|chevrolet chevell...|\n",
      "|15.0|         8|        350.0|      165.0|  3693.|         11.5|        70|      1|   buick skylark 320|\n",
      "|18.0|         8|        318.0|      150.0|  3436.|         11.0|        70|      1|  plymouth satellite|\n",
      "|16.0|         8|        304.0|      150.0|  3433.|         12.0|        70|      1|       amc rebel sst|\n",
      "|17.0|         8|        302.0|      140.0|  3449.|         10.5|        70|      1|         ford torino|\n",
      "|15.0|         8|        429.0|      198.0|  4341.|         10.0|        70|      1|    ford galaxie 500|\n",
      "|14.0|         8|        454.0|      220.0|  4354.|          9.0|        70|      1|    chevrolet impala|\n",
      "|14.0|         8|        440.0|      215.0|  4312.|          8.5|        70|      1|   plymouth fury iii|\n",
      "|14.0|         8|        455.0|      225.0|  4425.|         10.0|        70|      1|    pontiac catalina|\n",
      "|15.0|         8|        390.0|      190.0|  3850.|          8.5|        70|      1|  amc ambassador dpl|\n",
      "|15.0|         8|        383.0|      170.0|  3563.|         10.0|        70|      1| dodge challenger se|\n",
      "|14.0|         8|        340.0|      160.0|  3609.|          8.0|        70|      1|  plymouth 'cuda 340|\n",
      "|15.0|         8|        400.0|      150.0|  3761.|          9.5|        70|      1|chevrolet monte c...|\n",
      "|14.0|         8|        455.0|      225.0|  3086.|         10.0|        70|      1|buick estate wago...|\n",
      "|24.0|         4|        113.0|      95.00|  2372.|         15.0|        70|      3|toyota corona mar...|\n",
      "|22.0|         6|        198.0|      95.00|  2833.|         15.5|        70|      1|     plymouth duster|\n",
      "|18.0|         6|        199.0|      97.00|  2774.|         15.5|        70|      1|          amc hornet|\n",
      "|21.0|         6|        200.0|      85.00|  2587.|         16.0|        70|      1|       ford maverick|\n",
      "|27.0|         4|        97.00|      88.00|  2130.|         14.5|        70|      3|        datsun pl510|\n",
      "|26.0|         4|        97.00|      46.00|  1835.|         20.5|        70|      2|volkswagen 1131 d...|\n",
      "+----+----------+-------------+-----------+-------+-------------+----------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = auto_df.toDF(*[\"X\" + name for name in auto_df.columns])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3f83c",
   "metadata": {},
   "source": [
    "**Convert a DataFrame column to a Python list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chevrolet chevelle malibu', 'buick skylark 320', 'plymouth satellite', 'amc rebel sst', 'ford torino', 'ford galaxie 500', 'chevrolet impala', 'plymouth fury iii', 'pontiac catalina', 'amc ambassador dpl']\n"
     ]
    }
   ],
   "source": [
    "names = auto_df.select(\"carname\").rdd.flatMap(lambda x: x).collect()\n",
    "print(str(names[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5780ff",
   "metadata": {},
   "source": [
    "**Convert a scalar query to a Python value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.514572864321615\n"
     ]
    }
   ],
   "source": [
    "average = auto_df.agg(dict(mpg=\"avg\")).first()[0]\n",
    "print(str(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52351c",
   "metadata": {},
   "source": [
    "**Consume a DataFrame row-wise as Python dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6594c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mpg': '18.0', 'cylinders': '8', 'displacement': '307.0', 'horsepower': '130.0', 'weight': '3504.', 'acceleration': '12.0', 'modelyear': '70', 'origin': '1', 'carname': 'chevrolet chevelle malibu'}\n",
      "{'mpg': '15.0', 'cylinders': '8', 'displacement': '350.0', 'horsepower': '165.0', 'weight': '3693.', 'acceleration': '11.5', 'modelyear': '70', 'origin': '1', 'carname': 'buick skylark 320'}\n",
      "{'mpg': '18.0', 'cylinders': '8', 'displacement': '318.0', 'horsepower': '150.0', 'weight': '3436.', 'acceleration': '11.0', 'modelyear': '70', 'origin': '1', 'carname': 'plymouth satellite'}\n"
     ]
    }
   ],
   "source": [
    "first_three = auto_df.limit(3)\n",
    "for row in first_three.collect():\n",
    "    my_dict = row.asDict()\n",
    "    print(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b6853",
   "metadata": {},
   "source": [
    "**Select particular columns from a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05ef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+\n",
      "| mpg|cylinders|displacement|\n",
      "+----+---------+------------+\n",
      "|18.0|        8|       307.0|\n",
      "|15.0|        8|       350.0|\n",
      "|18.0|        8|       318.0|\n",
      "|16.0|        8|       304.0|\n",
      "|17.0|        8|       302.0|\n",
      "|15.0|        8|       429.0|\n",
      "|14.0|        8|       454.0|\n",
      "|14.0|        8|       440.0|\n",
      "|14.0|        8|       455.0|\n",
      "|15.0|        8|       390.0|\n",
      "|15.0|        8|       383.0|\n",
      "|14.0|        8|       340.0|\n",
      "|15.0|        8|       400.0|\n",
      "|14.0|        8|       455.0|\n",
      "|24.0|        4|       113.0|\n",
      "|22.0|        6|       198.0|\n",
      "|18.0|        6|       199.0|\n",
      "|21.0|        6|       200.0|\n",
      "|27.0|        4|       97.00|\n",
      "|26.0|        4|       97.00|\n",
      "+----+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = auto_df.select([\"mpg\", \"cylinders\", \"displacement\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24245c",
   "metadata": {},
   "source": [
    "**Create an empty dataframe with a specified schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|my_id|my_string|\n",
      "+-----+---------+\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, LongType, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"my_id\", LongType(), True),\n",
    "        StructField(\"my_string\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "df = spark.createDataFrame([], schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([],StructType([])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982254b",
   "metadata": {},
   "source": [
    "**Create a constant dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71b704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------------------+\n",
      "|my_id|my_string|       my_timestamp|\n",
      "+-----+---------+-------------------+\n",
      "|    1|      foo|2021-01-01 00:00:00|\n",
      "|    2|      bar|2021-01-02 00:00:00|\n",
      "+-----+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"my_id\", LongType(), True),\n",
    "        StructField(\"my_string\", StringType(), True),\n",
    "        StructField(\"my_timestamp\", TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"foo\", datetime.datetime.strptime(\"2021-01-01\", \"%Y-%m-%d\")),\n",
    "        (2, \"bar\", datetime.datetime.strptime(\"2021-01-02\", \"%Y-%m-%d\")),\n",
    "    ],\n",
    "    schema,\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358f747",
   "metadata": {},
   "source": [
    "**Convert String to Double**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71dc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.withColumn(\"horsepower\", col(\"horsepower\").cast(\"double\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102835a",
   "metadata": {},
   "source": [
    "**Convert String to Integer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38209b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.withColumn(\"horsepower\", col(\"horsepower\").cast(\"int\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da6aab",
   "metadata": {},
   "source": [
    "**Get the size of a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} rows\".format(auto_df.count()))\n",
    "print(\"{} columns\".format(len(auto_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595d2bc",
   "metadata": {},
   "source": [
    "**Get a DataFrame's number of partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} partition(s)\".format(auto_df.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcb80f",
   "metadata": {},
   "source": [
    "**Get data types of a DataFrame's columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5113ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auto_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354bd9de",
   "metadata": {},
   "source": [
    "**Convert an RDD to Data Frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6486bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# First, get the RDD from the DataFrame.\n",
    "rdd = auto_df.rdd\n",
    "\n",
    "# This converts it back to an RDD with no changes.\n",
    "df = rdd.map(lambda x: Row(**x.asDict())).toDF()\n",
    "\n",
    "# This changes the rows before creating the DataFrame.\n",
    "df = rdd.map(\n",
    "    lambda x: Row(**{k: v * 2 for (k, v) in x.asDict().items()})\n",
    ").toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404b00a",
   "metadata": {},
   "source": [
    "**Print the contents of an RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e77034",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = auto_df.rdd\n",
    "print(rdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbda1b",
   "metadata": {},
   "source": [
    "**Print the contents of a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08221c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.show(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f085452",
   "metadata": {},
   "source": [
    "**Process each row of a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad95bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def foreach_function(row):\n",
    "    if row.horsepower is not None:\n",
    "        os.system(\"echo \" + row.horsepower)\n",
    "\n",
    "auto_df.foreach(foreach_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40f01c",
   "metadata": {},
   "source": [
    "**DataFrame Map example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e89edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(row):\n",
    "    if row.horsepower is not None:\n",
    "        return [float(row.horsepower) * 10]\n",
    "    else:\n",
    "        return [None]\n",
    "\n",
    "df = auto_df.rdd.map(map_function).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf2e31",
   "metadata": {},
   "source": [
    "**DataFrame Flatmap example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac44402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "\n",
    "def flatmap_function(row):\n",
    "    if row.cylinders is not None:\n",
    "        return list(range(int(row.cylinders)))\n",
    "    else:\n",
    "        return [None]\n",
    "\n",
    "rdd = auto_df.rdd.flatMap(flatmap_function)\n",
    "row = Row(\"val\")\n",
    "df = rdd.map(row).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b6826",
   "metadata": {},
   "source": [
    "**Create a custom UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df.withColumn(\"manufacturer\", first_word_udf(col(\"carname\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b068fdf",
   "metadata": {},
   "source": [
    "## Data conversions and other modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d3a4f",
   "metadata": {},
   "source": [
    "**Run a SparkSQL Statement on a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "\n",
    "auto_df.registerTempTable(\"auto_df\")\n",
    "df = sqlContext.sql(\n",
    "    \"select modelyear, avg(mpg) from auto_df group by modelyear\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c75a3",
   "metadata": {},
   "source": [
    "**Extract data from a string using a regular expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "\n",
    "group = 0\n",
    "df = (\n",
    "    auto_df.withColumn(\n",
    "        \"identifier\", regexp_extract(col(\"carname\"), \"(\\S?\\d+)\", group)\n",
    "    )\n",
    "    .drop(\"acceleration\")\n",
    "    .drop(\"cylinders\")\n",
    "    .drop(\"displacement\")\n",
    "    .drop(\"modelyear\")\n",
    "    .drop(\"mpg\")\n",
    "    .drop(\"origin\")\n",
    "    .drop(\"horsepower\")\n",
    "    .drop(\"weight\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7979d0a",
   "metadata": {},
   "source": [
    "**Fill NULL values in specific columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de505160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_df.fillna({\"horsepower\": 0})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95098c",
   "metadata": {},
   "source": [
    "**Fill NULL values with column average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = auto_df.fillna({\"horsepower\": auto_df.agg(avg(\"horsepower\")).first()[0]})\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710c307",
   "metadata": {},
   "source": [
    "**Fill NULL values with group average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "unmodified_columns = auto_df.columns\n",
    "unmodified_columns.remove(\"horsepower\")\n",
    "manufacturer_avg = auto_df.groupBy(\"cylinders\").agg({\"horsepower\": \"avg\"})\n",
    "df = auto_df.join(manufacturer_avg, \"cylinders\").select(\n",
    "    *unmodified_columns,\n",
    "    coalesce(\"horsepower\", \"avg(horsepower)\").alias(\"horsepower\"),\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1741b7",
   "metadata": {},
   "source": [
    "**Unpack a DataFrame's JSON column to a new DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, json_tuple\n",
    "\n",
    "source = spark.sparkContext.parallelize(\n",
    "    [[\"1\", '{ \"a\" : 10, \"b\" : 11 }'], [\"2\", '{ \"a\" : 20, \"b\" : 21 }']]\n",
    ").toDF([\"id\", \"json\"])\n",
    "df = source.select(\"id\", json_tuple(col(\"json\"), \"a\", \"b\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c601c5",
   "metadata": {},
   "source": [
    "**Query a JSON column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44976758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, json_tuple\n",
    "\n",
    "source = spark.sparkContext.parallelize(\n",
    "    [[\"1\", '{ \"a\" : 10, \"b\" : 11 }'], [\"2\", '{ \"a\" : 20, \"b\" : 21 }']]\n",
    ").toDF([\"id\", \"json\"])\n",
    "df = (\n",
    "    source.select(\"id\", json_tuple(col(\"json\"), \"a\", \"b\"))\n",
    "    .withColumnRenamed(\"c0\", \"a\")\n",
    "    .withColumnRenamed(\"c1\", \"b\")\n",
    "    .where(col(\"b\") > 15)\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8719b8",
   "metadata": {},
   "source": [
    "## Filtering, sorting, removing duplicates and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1db94",
   "metadata": {},
   "source": [
    "**Filter a column using a condition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.filter(col(\"mpg\") > \"30\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991878e",
   "metadata": {},
   "source": [
    "**Filter based on a specific column value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.where(col(\"cylinders\") == \"8\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9852f",
   "metadata": {},
   "source": [
    "**Filter based on an IN list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16077fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.where(col(\"cylinders\").isin([\"4\", \"6\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579899e4",
   "metadata": {},
   "source": [
    "**Filter based on a NOT IN list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554db3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.where(~col(\"cylinders\").isin([\"4\", \"6\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9042e",
   "metadata": {},
   "source": [
    "**Filter values based on keys in another DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Our DataFrame of keys to exclude.\n",
    "exclude_keys = auto_df.select(\n",
    "    (col(\"modelyear\") + 1).alias(\"adjusted_year\")\n",
    ").distinct()\n",
    "\n",
    "# The anti join returns only keys with no matches.\n",
    "filtered = auto_df.join(\n",
    "    exclude_keys,\n",
    "    how=\"left_anti\",\n",
    "    on=auto_df.modelyear == exclude_keys.adjusted_year,\n",
    ")\n",
    "\n",
    "# Alternatively we can register a temporary table and use a SQL expression.\n",
    "exclude_keys.registerTempTable(\"exclude_keys\")\n",
    "df = auto_df.filter(\n",
    "    \"modelyear not in ( select adjusted_year from exclude_keys )\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdf568",
   "metadata": {},
   "source": [
    "**Get Dataframe rows that match a substring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_df.where(auto_df.carname.contains(\"custom\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ecd9f",
   "metadata": {},
   "source": [
    "**Filter a Dataframe based on a custom substring search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.where(col(\"carname\").like(\"%custom%\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69540a15",
   "metadata": {},
   "source": [
    "**Filter based on a column's length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "df = auto_df.where(length(col(\"carname\")) < 12)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c02822",
   "metadata": {},
   "source": [
    "**Multiple filter conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# OR\n",
    "df = auto_df.filter((col(\"mpg\") > \"30\") | (col(\"acceleration\") < \"10\"))\n",
    "# AND\n",
    "df = auto_df.filter((col(\"mpg\") > \"30\") & (col(\"acceleration\") < \"13\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370fb8e",
   "metadata": {},
   "source": [
    "**Sort DataFrame by a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.orderBy(\"carname\")\n",
    "df = auto_df.orderBy(col(\"carname\").desc())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e050fc",
   "metadata": {},
   "source": [
    "**Take the first N rows of a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "df = auto_df.limit(n)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e684b",
   "metadata": {},
   "source": [
    "**Get distinct values of a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71584e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_df.select(\"cylinders\").distinct()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bcea0",
   "metadata": {},
   "source": [
    "**Remove duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7876216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_df.dropDuplicates([\"carname\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e82ad",
   "metadata": {},
   "source": [
    "## Group DataFrame data by key to perform aggregates like counting, sums, averages, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710bd79d",
   "metadata": {},
   "source": [
    "**count(*) on a particular column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46216a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# No sorting.\n",
    "df = auto_df.groupBy(\"cylinders\").count()\n",
    "\n",
    "# With sorting.\n",
    "df = auto_df.groupBy(\"cylinders\").count().orderBy(desc(\"count\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759e27f",
   "metadata": {},
   "source": [
    "**Group and sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25296bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, desc\n",
    "\n",
    "df = (\n",
    "    auto_df.groupBy(\"cylinders\")\n",
    "    .agg(avg(\"horsepower\").alias(\"avg_horsepower\"))\n",
    "    .orderBy(desc(\"avg_horsepower\"))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fba7c5",
   "metadata": {},
   "source": [
    "**Filter groups based on an aggregate value, equivalent to SQL HAVING clause**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7601b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "df = (\n",
    "    auto_df.groupBy(\"cylinders\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .filter(col(\"count\") > 100)\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7feaad",
   "metadata": {},
   "source": [
    "**Group by multiple columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94861105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, desc\n",
    "\n",
    "df = (\n",
    "    auto_df.groupBy([\"modelyear\", \"cylinders\"])\n",
    "    .agg(avg(\"horsepower\").alias(\"avg_horsepower\"))\n",
    "    .orderBy(desc(\"avg_horsepower\"))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a6f8a",
   "metadata": {},
   "source": [
    "**Aggregate multiple columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions = dict(horsepower=\"avg\", weight=\"max\", displacement=\"max\")\n",
    "df = auto_df.groupBy(\"modelyear\").agg(expressions)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74555642",
   "metadata": {},
   "source": [
    "**Aggregate multiple columns with custom orderings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc_nulls_last\n",
    "\n",
    "expressions = dict(horsepower=\"avg\", weight=\"max\", displacement=\"max\")\n",
    "orderings = [\n",
    "    desc_nulls_last(\"max(displacement)\"),\n",
    "    desc_nulls_last(\"avg(horsepower)\"),\n",
    "    asc(\"max(weight)\"),\n",
    "]\n",
    "df = auto_df.groupBy(\"modelyear\").agg(expressions).orderBy(*orderings)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2955b569",
   "metadata": {},
   "source": [
    "**Get the maximum of a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390defaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "df = auto_df.select(max(col(\"horsepower\")).alias(\"max_horsepower\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa209709",
   "metadata": {},
   "source": [
    "**Sum a list of columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = {x: \"sum\" for x in (\"weight\", \"cylinders\", \"mpg\")}\n",
    "df = auto_df.agg(exprs)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d43fb9",
   "metadata": {},
   "source": [
    "**Sum a column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df = auto_df.groupBy(\"cylinders\").agg(sum(\"weight\").alias(\"total_weight\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fa319",
   "metadata": {},
   "source": [
    "**Aggregate all numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccf481",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = set([\"decimal\", \"double\", \"float\", \"integer\", \"long\", \"short\"])\n",
    "exprs = {x[0]: \"sum\" for x in auto_df_fixed.dtypes if x[1] in numerics}\n",
    "df = auto_df_fixed.agg(exprs)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7130827",
   "metadata": {},
   "source": [
    "**Count unique after grouping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98955cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df = auto_df.groupBy(\"cylinders\").agg(countDistinct(\"mpg\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d5672",
   "metadata": {},
   "source": [
    "**Count distinct values on all columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f02df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df = auto_df.agg(*(countDistinct(c) for c in auto_df.columns))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7b6f2",
   "metadata": {},
   "source": [
    "**Group by then filter on the count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.groupBy(\"cylinders\").count().where(col(\"count\") > 100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b589a",
   "metadata": {},
   "source": [
    "**Find the top N per row group (use N=1 for maximum)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d21719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# To get the maximum per group, set n=1.\n",
    "n = 5\n",
    "w = Window().partitionBy(\"cylinders\").orderBy(col(\"horsepower\").desc())\n",
    "df = (\n",
    "    auto_df.withColumn(\"horsepower\", col(\"horsepower\").cast(\"double\"))\n",
    "    .withColumn(\"rn\", row_number().over(w))\n",
    "    .where(col(\"rn\") <= n)\n",
    "    .select(\"*\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c45c1",
   "metadata": {},
   "source": [
    "**Group key/values into a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7859641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list\n",
    "\n",
    "df = auto_df.groupBy(\"cylinders\").agg(\n",
    "    collect_list(col(\"carname\")).alias(\"models\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11093356",
   "metadata": {},
   "source": [
    "**Compute a histogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55996d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Target column must be numeric.\n",
    "df = auto_df.withColumn(\"horsepower\", col(\"horsepower\").cast(\"double\"))\n",
    "\n",
    "# N is the number of bins.\n",
    "N = 11\n",
    "histogram = df.select(\"horsepower\").rdd.flatMap(lambda x: x).histogram(N)\n",
    "print(histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bbb49",
   "metadata": {},
   "source": [
    "**Compute global percentiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f104d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, ntile\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().orderBy(col(\"mpg\").desc())\n",
    "df = auto_df.withColumn(\"ntile4\", ntile(4).over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb48a7",
   "metadata": {},
   "source": [
    "**Compute percentiles within a partition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff99179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, ntile\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy(\"cylinders\").orderBy(col(\"mpg\").desc())\n",
    "df = auto_df.withColumn(\"ntile4\", ntile(4).over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955acfc",
   "metadata": {},
   "source": [
    "**Compute percentiles after aggregating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f13508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, ntile\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "grouped = auto_df.groupBy(\"modelyear\").count()\n",
    "w = Window().orderBy(col(\"count\").desc())\n",
    "df = grouped.withColumn(\"ntile4\", ntile(4).over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da72f7",
   "metadata": {},
   "source": [
    "**Filter rows with values below a target percentile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "target_percentile = auto_df.agg(\n",
    "    F.expr(\"percentile(mpg, 0.9)\").alias(\"target_percentile\")\n",
    ").first()[0]\n",
    "df = auto_df.filter(col(\"mpg\") > lit(target_percentile))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322fa05",
   "metadata": {},
   "source": [
    "**Aggregate and rollup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893365bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, count, desc\n",
    "\n",
    "subset = auto_df.filter(col(\"modelyear\") > 79)\n",
    "df = (\n",
    "    subset.rollup(\"modelyear\", \"cylinders\")\n",
    "    .agg(\n",
    "        avg(\"horsepower\").alias(\"avg_horsepower\"),\n",
    "        count(\"modelyear\").alias(\"count\"),\n",
    "    )\n",
    "    .orderBy(desc(\"modelyear\"), desc(\"cylinders\"))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55992054",
   "metadata": {},
   "source": [
    "**Aggregate and cube**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, count, desc\n",
    "\n",
    "subset = auto_df.filter(col(\"modelyear\") > 79)\n",
    "df = (\n",
    "    subset.cube(\"modelyear\", \"cylinders\")\n",
    "    .agg(\n",
    "        avg(\"horsepower\").alias(\"avg_horsepower\"),\n",
    "        count(\"modelyear\").alias(\"count\"),\n",
    "    )\n",
    "    .orderBy(desc(\"modelyear\"), desc(\"cylinders\"))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d97dd",
   "metadata": {},
   "source": [
    "## Spark allows DataFrames to be joined similarly to how tables are joined in an RDBMS. The diagram below shows join types available in Spark.\n",
    "\n",
    "![Spark Join Types](images/jointypes.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea4916",
   "metadata": {},
   "source": [
    "**Join two DataFrames by column name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Load a list of manufacturer / country pairs.\n",
    "countries = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/manufacturers.csv\")\n",
    ")\n",
    "\n",
    "# Add a manufacturers column, to join with the manufacturers list.\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df.withColumn(\"manufacturer\", first_word_udf(auto_df.carname))\n",
    "\n",
    "# The actual join.\n",
    "df = df.join(countries, \"manufacturer\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291ccd3",
   "metadata": {},
   "source": [
    "**Join two DataFrames with an expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Load a list of manufacturer / country pairs.\n",
    "countries = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/manufacturers.csv\")\n",
    ")\n",
    "\n",
    "# Add a manufacturers column, to join with the manufacturers list.\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df.withColumn(\"manufacturer\", first_word_udf(auto_df.carname))\n",
    "\n",
    "# The actual join.\n",
    "df = df.join(countries, df.manufacturer == countries.manufacturer)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f10ed",
   "metadata": {},
   "source": [
    "**Multiple join conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Load a list of manufacturer / country pairs.\n",
    "countries = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/manufacturers.csv\")\n",
    ")\n",
    "\n",
    "# Add a manufacturers column, to join with the manufacturers list.\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df.withColumn(\"manufacturer\", first_word_udf(auto_df.carname))\n",
    "\n",
    "# The actual join.\n",
    "df = df.join(\n",
    "    countries,\n",
    "    (df.manufacturer == countries.manufacturer)\n",
    "    | (df.mpg == countries.manufacturer),\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f916a",
   "metadata": {},
   "source": [
    "**Various Spark join types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9254dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join on one column.\n",
    "joined = auto_df.join(auto_df, \"carname\")\n",
    "\n",
    "# Left (outer) join.\n",
    "joined = auto_df.join(auto_df, \"carname\", \"left\")\n",
    "\n",
    "# Left anti (not in) join.\n",
    "joined = auto_df.join(auto_df, \"carname\", \"left_anti\")\n",
    "\n",
    "# Right (outer) join.\n",
    "joined = auto_df.join(auto_df, \"carname\", \"right\")\n",
    "\n",
    "# Full join.\n",
    "joined = auto_df.join(auto_df, \"carname\", \"full\")\n",
    "\n",
    "# Cross join.\n",
    "df = auto_df.crossJoin(auto_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e7ef9",
   "metadata": {},
   "source": [
    "**Concatenate two DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", True).load(\"data/part1.csv\")\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", True).load(\"data/part2.csv\")\n",
    "df = df1.union(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8769f",
   "metadata": {},
   "source": [
    "**Load multiple files into a single DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Use a list.\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load([\"data/part1.csv\", \"data/part2.csv\"])\n",
    ")\n",
    "\n",
    "# Approach 2: Use a wildcard.\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"data/part*.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec90102",
   "metadata": {},
   "source": [
    "**Subtract DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a16368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.subtract(auto_df.where(col(\"mpg\") < \"25\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d54e1c",
   "metadata": {},
   "source": [
    "## Loading File Metadata and Processing Files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f7633",
   "metadata": {},
   "source": [
    "**Load Local File Details into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Simple: Use glob and only file names.\n",
    "files = [[x] for x in glob.glob(\"/etc/*\")]\n",
    "df = spark.createDataFrame(files)\n",
    "\n",
    "# Advanced: Use os.walk and extended attributes.\n",
    "target_path = \"/etc\"\n",
    "entries = []\n",
    "walker = os.walk(target_path)\n",
    "for root, dirs, files in walker:\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        try:\n",
    "            stat_info = os.stat(full_path)\n",
    "            entries.append(\n",
    "                [\n",
    "                    file,\n",
    "                    full_path,\n",
    "                    stat_info.st_size,\n",
    "                    datetime.datetime.fromtimestamp(stat_info.st_mtime),\n",
    "                ]\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"file\", StringType(), False),\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"size\", LongType(), False),\n",
    "        StructField(\"mtime\", TimestampType(), False),\n",
    "    ]\n",
    ")\n",
    "df = spark.createDataFrame(entries, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18ef12",
   "metadata": {},
   "source": [
    "**Load Files from Oracle Cloud Infrastructure into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fe18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "def get_authenticated_client(client):\n",
    "    config = oci.config.from_file()\n",
    "    authenticated_client = client(config)\n",
    "    return authenticated_client\n",
    "\n",
    "object_store_client = get_authenticated_client(\n",
    "    oci.object_storage.ObjectStorageClient\n",
    ")\n",
    "\n",
    "# Requires an object_store_client object.\n",
    "# See https://oracle-cloud-infrastructure-python-sdk.readthedocs.io/en/latest/api/object_storage/client/oci.object_storage.ObjectStorageClient.html\n",
    "input_bucket = \"oow_2019_dataflow_lab\"\n",
    "raw_inputs = object_store_client.list_objects(\n",
    "    object_store_client.get_namespace().data,\n",
    "    input_bucket,\n",
    "    fields=\"size,md5,timeModified\",\n",
    ")\n",
    "files = [\n",
    "    [x.name, x.size, x.time_modified, x.md5] for x in raw_inputs.data.objects\n",
    "]\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"size\", LongType(), True),\n",
    "        StructField(\"modified\", TimestampType(), True),\n",
    "        StructField(\"md5\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "df = spark.createDataFrame(files, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64f198",
   "metadata": {},
   "source": [
    "**Transform Many Images using Pillow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def resize_an_image(row):\n",
    "    width, height = 128, 128\n",
    "    file_name = row._1\n",
    "    new_name = file_name.replace(\".png\", \".resized.png\")\n",
    "    img = Image.open(file_name)\n",
    "    img = img.resize((width, height), Image.ANTIALIAS)\n",
    "    img.save(new_name)\n",
    "\n",
    "files = [[x] for x in glob.glob(\"data/resize_image?.png\")]\n",
    "df = spark.createDataFrame(files)\n",
    "df.foreach(resize_an_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ed111",
   "metadata": {},
   "source": [
    "## Dealing with NULLs and NaNs in DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac839ccf",
   "metadata": {},
   "source": [
    "**Filter rows with None or Null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e318f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.where(col(\"horsepower\").isNull())\n",
    "df = auto_df.where(col(\"horsepower\").isNotNull())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83fc83",
   "metadata": {},
   "source": [
    "**Drop rows with Null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e38a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_df.na.drop(thresh=1, subset=(\"horsepower\",))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259caa41",
   "metadata": {},
   "source": [
    "**Count all Null or NaN values in a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6620316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "\n",
    "df = auto_df.select(\n",
    "    [count(when(isnan(c), c)).alias(c) for c in auto_df.columns]\n",
    ")\n",
    "df = auto_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in auto_df.columns]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4744c44f",
   "metadata": {},
   "source": [
    "## Parsing and processing dates and times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c23ef",
   "metadata": {},
   "source": [
    "**Convert an ISO 8601 formatted date string to date type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4aaa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.sparkContext.parallelize([[\"2021-01-01\"], [\"2022-01-01\"]]).toDF(\n",
    "    [\"date_col\"]\n",
    ")\n",
    "df = df.withColumn(\"date_col\", col(\"date_col\").cast(\"date\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32e1d1",
   "metadata": {},
   "source": [
    "**Convert a custom formatted date string to date type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafe3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "df = spark.sparkContext.parallelize([[\"20210101\"], [\"20220101\"]]).toDF(\n",
    "    [\"date_col\"]\n",
    ")\n",
    "df = df.withColumn(\"date_col\", to_date(col(\"date_col\"), \"yyyyddMM\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80badd09",
   "metadata": {},
   "source": [
    "**Get the last day of the current month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7eb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, last_day\n",
    "\n",
    "df = spark.sparkContext.parallelize([[\"2020-01-01\"], [\"1712-02-10\"]]).toDF(\n",
    "    [\"date_col\"]\n",
    ")\n",
    "df = df.withColumn(\"date_col\", col(\"date_col\").cast(\"date\")).withColumn(\n",
    "    \"last_day\", last_day(col(\"date_col\"))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4a479",
   "metadata": {},
   "source": [
    "**Convert UNIX (seconds since epoch) timestamp to date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1081992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "df = spark.sparkContext.parallelize([[\"1590183026\"], [\"2000000000\"]]).toDF(\n",
    "    [\"ts_col\"]\n",
    ")\n",
    "df = df.withColumn(\"date_col\", from_unixtime(col(\"ts_col\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078588b4",
   "metadata": {},
   "source": [
    "**Load a CSV file with complex dates into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "import dateparser\n",
    "\n",
    "# Use the dateparser module to convert many formats into timestamps.\n",
    "date_convert = udf(\n",
    "    lambda x: dateparser.parse(x) if x is not None else None, TimestampType()\n",
    ")\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/date_examples.csv\")\n",
    ")\n",
    "df = df.withColumn(\"parsed\", date_convert(df.date))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e0cc9",
   "metadata": {},
   "source": [
    "## Analyzing unstructured data like [JSON](https://spark.apache.org/docs/latest/sql-data-sources-json.html), XML, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66533e3a",
   "metadata": {},
   "source": [
    "**Flatten top level text fields in a JSONl document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load JSONl into a DataFrame. Schema is inferred automatically.\n",
    "base = spark.read.json(\"data/financial.jsonl\")\n",
    "\n",
    "# Extract interesting fields. Alias keeps columns readable.\n",
    "target_json_fields = [\n",
    "    col(\"symbol\").alias(\"symbol\"),\n",
    "    col(\"quoteType.longName\").alias(\"longName\"),\n",
    "    col(\"price.marketCap.raw\").alias(\"marketCap\"),\n",
    "    col(\"summaryDetail.previousClose.raw\").alias(\"previousClose\"),\n",
    "    col(\"summaryDetail.fiftyTwoWeekHigh.raw\").alias(\"fiftyTwoWeekHigh\"),\n",
    "    col(\"summaryDetail.fiftyTwoWeekLow.raw\").alias(\"fiftyTwoWeekLow\"),\n",
    "    col(\"summaryDetail.trailingPE.raw\").alias(\"trailingPE\"),\n",
    "]\n",
    "df = base.select(target_json_fields)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f7a14",
   "metadata": {},
   "source": [
    "**Flatten top level text fields from a JSON column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30142914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, schema_of_json\n",
    "\n",
    "# quote/escape options needed when loading CSV containing JSON.\n",
    "base = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .load(\"data/financial.csv\")\n",
    ")\n",
    "\n",
    "# Infer JSON schema from one entry in the DataFrame.\n",
    "sample_json_document = base.select(\"financial_data\").first()[0]\n",
    "schema = schema_of_json(sample_json_document)\n",
    "\n",
    "# Parse using this schema.\n",
    "parsed = base.withColumn(\"parsed\", from_json(\"financial_data\", schema))\n",
    "\n",
    "# Extract interesting fields.\n",
    "target_json_fields = [\n",
    "    col(\"parsed.symbol\").alias(\"symbol\"),\n",
    "    col(\"parsed.quoteType.longName\").alias(\"longName\"),\n",
    "    col(\"parsed.price.marketCap.raw\").alias(\"marketCap\"),\n",
    "    col(\"parsed.summaryDetail.previousClose.raw\").alias(\"previousClose\"),\n",
    "    col(\"parsed.summaryDetail.fiftyTwoWeekHigh.raw\").alias(\"fiftyTwoWeekHigh\"),\n",
    "    col(\"parsed.summaryDetail.fiftyTwoWeekLow.raw\").alias(\"fiftyTwoWeekLow\"),\n",
    "    col(\"parsed.summaryDetail.trailingPE.raw\").alias(\"trailingPE\"),\n",
    "]\n",
    "df = parsed.select(target_json_fields)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b805343",
   "metadata": {},
   "source": [
    "**Unnest an array of complex structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b74b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "base = spark.read.json(\"data/financial.jsonl\")\n",
    "\n",
    "# Analyze balance sheet data, which is held in an array of complex types.\n",
    "target_json_fields = [\n",
    "    col(\"symbol\").alias(\"symbol\"),\n",
    "    col(\"balanceSheetHistoryQuarterly.balanceSheetStatements\").alias(\n",
    "        \"balanceSheetStatements\"\n",
    "    ),\n",
    "]\n",
    "selected = base.select(target_json_fields)\n",
    "\n",
    "# Select a few fields from the balance sheet statement data.\n",
    "target_json_fields = [\n",
    "    col(\"symbol\").alias(\"symbol\"),\n",
    "    col(\"col.endDate.fmt\").alias(\"endDate\"),\n",
    "    col(\"col.cash.raw\").alias(\"cash\"),\n",
    "    col(\"col.totalAssets.raw\").alias(\"totalAssets\"),\n",
    "    col(\"col.totalLiab.raw\").alias(\"totalLiab\"),\n",
    "]\n",
    "\n",
    "# Balance sheet data is in an array, use explode to generate one row per entry.\n",
    "df = selected.select(\"symbol\", explode(\"balanceSheetStatements\")).select(\n",
    "    target_json_fields\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec3253",
   "metadata": {},
   "source": [
    "## Using Python's Pandas library to augment Spark. Some operations require the pyarrow library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9473b6e",
   "metadata": {},
   "source": [
    "**Convert Spark DataFrame to Pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e50e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = auto_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4cbdb7",
   "metadata": {},
   "source": [
    "**Convert Pandas DataFrame to Spark DataFrame with Schema Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0178b",
   "metadata": {},
   "source": [
    "**Convert Pandas DataFrame to Spark DataFrame using a Custom Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code converts everything to strings.\n",
    "# If you want to preserve types, see https://gist.github.com/tonyfraser/79a255aa8a9d765bd5cf8bd13597171e\n",
    "from pyspark.sql.types import StructField, StructType, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(name, StringType(), True) for name in pandas_df.columns]\n",
    ")\n",
    "df = spark.createDataFrame(pandas_df, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb0766",
   "metadata": {},
   "source": [
    "**Convert N rows from a DataFrame to a Pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "pdf = auto_df.limit(N).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccccf97",
   "metadata": {},
   "source": [
    "**Grouped Aggregation with Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7792da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pandas import DataFrame\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udaf(pdf: DataFrame) -> float:\n",
    "    return pdf.mean()\n",
    "\n",
    "df = auto_df.groupby(\"cylinders\").agg(mean_udaf(auto_df[\"mpg\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de5ce0",
   "metadata": {},
   "source": [
    "**Use a Pandas Grouped Map Function via applyInPandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8374234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(pdf):\n",
    "    minv = pdf.horsepower.min()\n",
    "    maxv = pdf.horsepower.max() - minv\n",
    "    return pdf.assign(horsepower=(pdf.horsepower - minv) / maxv * 100)\n",
    "\n",
    "df = auto_df.groupby(\"cylinders\").applyInPandas(rescale, auto_df.schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6ca18",
   "metadata": {},
   "source": [
    "## Extracting key statistics out of a body of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35b502",
   "metadata": {},
   "source": [
    "**Compute the number of NULLs across all columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a856f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "df = auto_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in auto_df.columns]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a7f02",
   "metadata": {},
   "source": [
    "**Compute average values of all numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb32806",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = set([\"decimal\", \"double\", \"float\", \"integer\", \"long\", \"short\"])\n",
    "exprs = {x[0]: \"avg\" for x in auto_df_fixed.dtypes if x[1] in numerics}\n",
    "df = auto_df_fixed.agg(exprs)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092c12f",
   "metadata": {},
   "source": [
    "**Compute minimum values of all numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = set([\"decimal\", \"double\", \"float\", \"integer\", \"long\", \"short\"])\n",
    "exprs = {x[0]: \"min\" for x in auto_df_fixed.dtypes if x[1] in numerics}\n",
    "df = auto_df_fixed.agg(exprs)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d39935",
   "metadata": {},
   "source": [
    "**Compute maximum values of all numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ac9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = set([\"decimal\", \"double\", \"float\", \"integer\", \"long\", \"short\"])\n",
    "exprs = {x[0]: \"max\" for x in auto_df_fixed.dtypes if x[1] in numerics}\n",
    "df = auto_df_fixed.agg(exprs)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bada1",
   "metadata": {},
   "source": [
    "**Compute median values of all numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487048ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "numerics = set([\"decimal\", \"double\", \"float\", \"integer\", \"long\", \"short\"])\n",
    "aggregates = []\n",
    "for name, dtype in auto_df_fixed.dtypes:\n",
    "    if dtype not in numerics:\n",
    "        continue\n",
    "    aggregates.append(\n",
    "        F.expr(\"percentile({}, 0.5)\".format(name)).alias(\n",
    "            \"{}_median\".format(name)\n",
    "        )\n",
    "    )\n",
    "df = auto_df_fixed.agg(*aggregates)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181aa55",
   "metadata": {},
   "source": [
    "**Identify Outliers in a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0806c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sqrt\n",
    "\n",
    "target_column = \"mpg\"\n",
    "z_score_threshold = 2\n",
    "\n",
    "# Compute the median of the target column.\n",
    "target_df = auto_df.select(target_column)\n",
    "target_df.registerTempTable(\"target_column\")\n",
    "profiled = sqlContext.sql(\n",
    "    f\"select percentile({target_column}, 0.5) as median from target_column\"\n",
    ")\n",
    "\n",
    "# Compute deviations.\n",
    "deviations = target_df.crossJoin(profiled).withColumn(\n",
    "    \"deviation\", sqrt((target_df[target_column] - profiled[\"median\"]) ** 2)\n",
    ")\n",
    "deviations.registerTempTable(\"deviations\")\n",
    "\n",
    "# The Median Absolute Deviation\n",
    "mad = sqlContext.sql(\"select percentile(deviation, 0.5) as mad from deviations\")\n",
    "\n",
    "# Add a modified z score to the original DataFrame.\n",
    "df = (\n",
    "    auto_df.crossJoin(mad)\n",
    "    .crossJoin(profiled)\n",
    "    .withColumn(\n",
    "        \"zscore\",\n",
    "        0.6745\n",
    "        * sqrt((auto_df[target_column] - profiled[\"median\"]) ** 2)\n",
    "        / mad[\"mad\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.where(col(\"zscore\") > z_score_threshold)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411cb9c",
   "metadata": {},
   "source": [
    "## Upserts, updates and deletes on data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a45c8",
   "metadata": {},
   "source": [
    "**Save to a Delta Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"delta_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd2a30",
   "metadata": {},
   "source": [
    "**Update records in a DataFrame using Delta Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5fe280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "output_path = \"delta_tests\"\n",
    "\n",
    "# Currently you have to save/reload to convert from table to DataFrame.\n",
    "auto_df.write.mode(\"overwrite\").format(\"delta\").save(output_path)\n",
    "dt = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "# Run a SQL update operation.\n",
    "dt.update(\n",
    "    condition=expr(\"carname like 'Volks%'\"), set={\"carname\": expr(\"carname\")}\n",
    ")\n",
    "\n",
    "# Convert back to a DataFrame.\n",
    "df = dt.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f260d",
   "metadata": {},
   "source": [
    "**Merge into a Delta table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Save the original data.\n",
    "output_path = \"delta_tests\"\n",
    "auto_df.write.mode(\"overwrite\").format(\"delta\").save(output_path)\n",
    "\n",
    "# Load data that corrects some car names.\n",
    "corrected_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/auto-mpg-fixed.csv\")\n",
    ")\n",
    "\n",
    "# Merge the corrected data in.\n",
    "dt = DeltaTable.forPath(spark, output_path)\n",
    "ret = (\n",
    "    dt.alias(\"original\")\n",
    "    .merge(\n",
    "        corrected_df.alias(\"corrected\"),\n",
    "        \"original.modelyear = corrected.modelyear and original.weight = corrected.weight and original.acceleration = corrected.acceleration\",\n",
    "    )\n",
    "    .whenMatchedUpdate(\n",
    "        condition=expr(\"original.carname <> corrected.carname\"),\n",
    "        set={\"carname\": col(\"corrected.carname\")},\n",
    "    )\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "# Show select table history.\n",
    "df = dt.history().select(\"version operation operationMetrics\".split())\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18a446",
   "metadata": {},
   "source": [
    "**Show Table Version History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our table.\n",
    "output_path = \"delta_tests\"\n",
    "dt = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "# Show select table history.\n",
    "df = dt.history().select(\"version timestamp operation\".split())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea11f6",
   "metadata": {},
   "source": [
    "**Load a Delta Table by Version ID (Time Travel Query)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Get versions.\n",
    "output_path = \"delta_tests\"\n",
    "dt = DeltaTable.forPath(spark, output_path)\n",
    "versions = (\n",
    "    dt.history().select(\"version timestamp\".split()).orderBy(desc(\"version\"))\n",
    ")\n",
    "most_recent_version = versions.first()[0]\n",
    "print(\"Most recent version is\", most_recent_version)\n",
    "\n",
    "# Load the most recent data.\n",
    "df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", most_recent_version)\n",
    "    .load(output_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a96ee2",
   "metadata": {},
   "source": [
    "**Load a Delta Table by Timestamp (Time Travel Query)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bcf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Get versions.\n",
    "output_path = \"delta_tests\"\n",
    "dt = DeltaTable.forPath(spark, output_path)\n",
    "versions = dt.history().select(\"version timestamp\".split()).orderBy(\"timestamp\")\n",
    "most_recent_timestamp = versions.first()[1]\n",
    "print(\"Most recent timestamp is\", most_recent_timestamp)\n",
    "\n",
    "# Load the oldest version by timestamp.\n",
    "df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"timestampAsOf\", most_recent_timestamp)\n",
    "    .load(output_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6ae32",
   "metadata": {},
   "source": [
    "**Compact a Delta Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbe406",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"delta_tests\"\n",
    "\n",
    "# Load table.\n",
    "dt = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "# Clean up data older than the given window.\n",
    "retention_window_hours = 168\n",
    "dt.vacuum(retention_window_hours)\n",
    "\n",
    "# Show the new versions.\n",
    "df = dt.history().select(\"version timestamp\".split()).orderBy(\"version\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1fd3a",
   "metadata": {},
   "source": [
    "**Add custom metadata to a Delta table write**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "extra_properties = dict(\n",
    "    user=os.environ.get(\"USER\"),\n",
    "    write_timestamp=time.time(),\n",
    ")\n",
    "auto_df.write.mode(\"append\").option(\"userMetadata\", extra_properties).format(\n",
    "    \"delta\"\n",
    ").save(\"delta_table_metadata\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec22fb3",
   "metadata": {},
   "source": [
    "**Read custom Delta table metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d508b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DeltaTable.forPath(spark, \"delta_table_metadata\")\n",
    "df = dt.history().select(\"version timestamp userMetadata\".split())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8820f84",
   "metadata": {},
   "source": [
    "## Spark Streaming (Focuses on Structured Streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112a808",
   "metadata": {},
   "source": [
    "**Connect to Kafka using SASL PLAIN authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9070f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"USERNAME\" password=\"PASSWORD\";',\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.bootstrap.servers\": \"server:9092\",\n",
    "    \"group.id\": \"my_group\",\n",
    "    \"subscribe\": \"my_topic\",\n",
    "}\n",
    "df = spark.readStream.format(\"kafka\").options(**options).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7174c13",
   "metadata": {},
   "source": [
    "**Create a windowed Structured Stream over input CSV files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89263c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, current_timestamp, window\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    ")\n",
    "\n",
    "input_location = \"streaming/input\"\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"mpg\", DoubleType(), True),\n",
    "        StructField(\"cylinders\", IntegerType(), True),\n",
    "        StructField(\"displacement\", DoubleType(), True),\n",
    "        StructField(\"horsepower\", DoubleType(), True),\n",
    "        StructField(\"weight\", DoubleType(), True),\n",
    "        StructField(\"acceleration\", DoubleType(), True),\n",
    "        StructField(\"modelyear\", IntegerType(), True),\n",
    "        StructField(\"origin\", IntegerType(), True),\n",
    "        StructField(\"carname\", StringType(), True),\n",
    "        StructField(\"manufacturer\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "df = spark.readStream.csv(path=input_location, schema=schema).withColumn(\n",
    "    \"timestamp\", current_timestamp()\n",
    ")\n",
    "\n",
    "aggregated = (\n",
    "    df.groupBy(window(df.timestamp, \"1 minute\"), \"manufacturer\")\n",
    "    .agg(\n",
    "        avg(\"horsepower\").alias(\"avg_horsepower\"),\n",
    "        avg(\"timestamp\").alias(\"avg_timestamp\"),\n",
    "        count(\"modelyear\").alias(\"count\"),\n",
    "    )\n",
    "    .coalesce(10)\n",
    ")\n",
    "summary = aggregated.orderBy(\"window\", \"manufacturer\")\n",
    "query = (\n",
    "    summary.writeStream.outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa86145",
   "metadata": {},
   "source": [
    "**Create an unwindowed Structured Stream over input CSV files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, desc\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    ")\n",
    "\n",
    "input_location = \"streaming/input\"\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"mpg\", DoubleType(), True),\n",
    "        StructField(\"cylinders\", IntegerType(), True),\n",
    "        StructField(\"displacement\", DoubleType(), True),\n",
    "        StructField(\"horsepower\", DoubleType(), True),\n",
    "        StructField(\"weight\", DoubleType(), True),\n",
    "        StructField(\"acceleration\", DoubleType(), True),\n",
    "        StructField(\"modelyear\", IntegerType(), True),\n",
    "        StructField(\"origin\", IntegerType(), True),\n",
    "        StructField(\"carname\", StringType(), True),\n",
    "        StructField(\"manufacturer\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = spark.readStream.csv(path=input_location, schema=schema)\n",
    "summary = (\n",
    "    df.groupBy(\"modelyear\")\n",
    "    .agg(\n",
    "        avg(\"horsepower\").alias(\"avg_horsepower\"),\n",
    "        count(\"modelyear\").alias(\"count\"),\n",
    "    )\n",
    "    .orderBy(desc(\"modelyear\"))\n",
    "    .coalesce(10)\n",
    ")\n",
    "query = summary.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1aebe",
   "metadata": {},
   "source": [
    "**Add the current timestamp to a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df = auto_df.withColumn(\"timestamp\", current_timestamp())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48565009",
   "metadata": {},
   "source": [
    "**Session analytics on a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07059d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hash, session_window\n",
    "\n",
    "hits_per_session = (\n",
    "    weblog_df.groupBy(\"ip\", session_window(\"time\", \"5 minutes\"))\n",
    "    .count()\n",
    "    .withColumn(\"session\", hash(\"ip\", \"session_window\"))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a3f0d",
   "metadata": {},
   "source": [
    "**Call a UDF only when a threshold is reached**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def myudf(short_circuit, state, value):\n",
    "    if short_circuit == True:\n",
    "        return True\n",
    "\n",
    "    # Log, send an alert, etc.\n",
    "    return False\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", \"9090\")\n",
    "    .load()\n",
    ")\n",
    "parsed = (\n",
    "    df.selectExpr(\n",
    "        \"split(value,',')[0] as state\",\n",
    "        \"split(value,',')[1] as zipcode\",\n",
    "        \"split(value,',')[2] as spend\",\n",
    "    )\n",
    "    .groupBy(\"state\")\n",
    "    .agg({\"spend\": \"avg\"})\n",
    "    .orderBy(desc(\"avg(spend)\"))\n",
    ")\n",
    "tagged = parsed.withColumn(\n",
    "    \"below\", myudf(col(\"avg(spend)\") < 100, col(\"state\"), col(\"avg(spend)\"))\n",
    ")\n",
    "\n",
    "tagged.writeStream.outputMode(\"complete\").format(\n",
    "    \"console\"\n",
    ").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ef79e",
   "metadata": {},
   "source": [
    "**Streaming Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df91adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "pipeline_model = PipelineModel.load(\"path/to/pipeline\")\n",
    "df = pipeline.transform(input_df)\n",
    "df.writeStream.format(\"console\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19d0d9",
   "metadata": {},
   "source": [
    "**Control stream processing frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c04e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream.outputMode(\"complete\").format(\"console\").trigger(\n",
    "    processingTime=\"10 seconds\"\n",
    ").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0939cc28",
   "metadata": {},
   "source": [
    "**Write a streaming DataFrame to a database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8227c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "pg_database = os.environ.get(\"PGDATABASE\") or \"postgres\"\n",
    "pg_host = os.environ.get(\"PGHOST\") or \"localhost\"\n",
    "pg_password = os.environ.get(\"PGPASSWORD\") or \"password\"\n",
    "pg_user = os.environ.get(\"PGUSER\") or \"postgres\"\n",
    "url = f\"jdbc:postgresql://{pg_host}:5432/{pg_database}\"\n",
    "table = \"streaming\"\n",
    "properties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_password,\n",
    "}\n",
    "\n",
    "def foreach_batch_function(my_df, epoch_id):\n",
    "    my_df.write.jdbc(url=url, table=table, mode=\"Append\", properties=properties)\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"rate\")\n",
    "    .option(\"rowPerSecond\", 100)\n",
    "    .option(\"numPartitions\", 2)\n",
    "    .load()\n",
    ")\n",
    "query = df.writeStream.foreachBatch(foreach_batch_function).start()\n",
    "\n",
    "# Wait for some data to be processed and exit.\n",
    "for i in range(10):\n",
    "    time.sleep(5)\n",
    "    if len(query.recentProgress) > 0:\n",
    "        query.stop()\n",
    "        break\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "result = \"{} rows written to database\".format(df.count())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e89be",
   "metadata": {},
   "source": [
    "## Techniques for dealing with time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56271e7",
   "metadata": {},
   "source": [
    "**Zero fill missing values in a timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "# Use distinct values of customer and date from the dataset itself.\n",
    "# In general it's safer to use known reference tables for IDs and dates.\n",
    "df = spend_df.join(\n",
    "    spend_df.select(\"customer_id\")\n",
    "    .distinct()\n",
    "    .crossJoin(spend_df.select(\"date\").distinct()),\n",
    "    [\"date\", \"customer_id\"],\n",
    "    \"right\",\n",
    ").select(\"date\", \"customer_id\", coalesce(\"spend_dollars\", lit(0)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4fe8e",
   "metadata": {},
   "source": [
    "**First Time an ID is Seen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d261f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy(\"customer_id\").orderBy(\"date\")\n",
    "df = spend_df.withColumn(\"first_seen\", first(\"date\").over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe0508",
   "metadata": {},
   "source": [
    "**Cumulative Sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = (\n",
    "    Window()\n",
    "    .partitionBy(\"customer_id\")\n",
    "    .orderBy(\"date\")\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "df = spend_df.withColumn(\"running_sum\", sum(\"spend_dollars\").over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc743ae0",
   "metadata": {},
   "source": [
    "**Cumulative Sum in a Period**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, year\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Add an additional partition clause for the sub-period.\n",
    "w = (\n",
    "    Window()\n",
    "    .partitionBy([\"customer_id\", year(\"date\")])\n",
    "    .orderBy(\"date\")\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "df = spend_df.withColumn(\"running_sum\", sum(\"spend_dollars\").over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ccb73f",
   "metadata": {},
   "source": [
    "**Cumulative Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = (\n",
    "    Window()\n",
    "    .partitionBy(\"customer_id\")\n",
    "    .orderBy(\"date\")\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "df = spend_df.withColumn(\"running_avg\", avg(\"spend_dollars\").over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c0f81",
   "metadata": {},
   "source": [
    "**Cumulative Average in a Period**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, year\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Add an additional partition clause for the sub-period.\n",
    "w = (\n",
    "    Window()\n",
    "    .partitionBy([\"customer_id\", year(\"date\")])\n",
    "    .orderBy(\"date\")\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "df = spend_df.withColumn(\"running_avg\", avg(\"spend_dollars\").over(w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe315a",
   "metadata": {},
   "source": [
    "## Machine Learning is a deep subject, too much to cover in this cheatsheet which is intended for code you can easily paste into your apps. The examples below will show basics of ML in Spark. It is helpful to understand the terminology of ML like Features, Estimators and Models. If you want some background on these things consider courses like \"Google crash course in ML\" or Udemy's \"Machine Learning Course with Python\".\n",
    "\n",
    "A brief introduction to Spark ML terms:\n",
    "* Feature: A Feature is an individual measurement. For example if you want to predict height based on age and sex, a combination of age and sex is a Feature.\n",
    "* Vector: A Vector is a special Spark data type similar to an array of numbers. Spark ML algorithms require Features to be loaded into Vectors for training and predictions.\n",
    "* Vector Column: Model training requires considering many Features at the same time. Spark ML operates on `DataFrame`s. Before training can happen you need to construct a `DataFrame` column of type Vector. See examples below.\n",
    "* Label: Supervised ML algorithms like regression and classification require a label when training. In Spark you will put labels in a column in a `DataFrame` such that each row has both a Feature and its associated Label.\n",
    "* Model: A Model is an algorithm capable of turning Feature vectors into values, usually thought of as predictions.\n",
    "* Estimator: An Estimator builds a mathematical model that transforms input values into outputs. Estimators do double duty in Spark, some Estimators like regression and classification build statistical models. Some Estimators are purely for data preparation like the StringIndexer which builds a Model containing a dictionary that maps strings to numbers in a deterministic way.\n",
    "* Fitting: Fitting is the process of building a Model using an Estimator and an input DataFrame you provide.\n",
    "* Transformer: Transformers create new DataFrames using the `transform` API, which applies algorithms to the input DataFrame and outputs a DataFrame with additional columns. The nature of the `transform` could be statistical or it could be a simple algorithm, depending on the type of Estimator that created the Model.\n",
    "* Pipelines: Pipelines are a series of Estimators that apply a series of `transform`s to a `DataFrame` before calling `fit` on the final Estimator in the Pipeline. The Pipeline is itself an Estimator. When you `fit` a Pipeline, Spark `fit`s the first Estimator in the Pipeline using an input `DataFrame` you provide. This produces a Model. If there are additional Estimators in the Pipeline, the newly created Model's `transform` method is called against the input `DataFrame` to create a new `DataFrame`. The process then begins again with the newly created `DataFrame` being passed to the next Estimator's `fit` method. Fitting a Pipeline produces a `PipelineModel`.\n",
    "\n",
    "This image helps visualize the relationship between Spark ML classes.\n",
    "![Hierarchy of Spark ML Classes](images/mlhierarchy.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cb777",
   "metadata": {},
   "source": [
    "**Prepare data for training with a VectorAssembler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5babdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"acceleration\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "assembled = vectorAssembler.transform(auto_df_fixed)\n",
    "assembled = assembled.select(\n",
    "    [\"cylinders\", \"displacement\", \"acceleration\", \"features\"]\n",
    ")\n",
    "print(\"Data type for features column is:\", assembled.dtypes[-1][1])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91147546",
   "metadata": {},
   "source": [
    "**A basic Random Forest Regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e02249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "assembled = vectorAssembler.transform(auto_df_fixed)\n",
    "assembled = assembled.select([\"features\", \"mpg\", \"carname\"])\n",
    "\n",
    "# Define the estimator.\n",
    "rf = RandomForestRegressor(\n",
    "    numTrees=20,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"mpg\",\n",
    ")\n",
    "\n",
    "# Fit the model.\n",
    "rf_model = rf.fit(assembled)\n",
    "\n",
    "# Save the model.\n",
    "rf_model.write().overwrite().save(\"rf_regression_simple.model\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4654c8a",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c68903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Set up our main ML pipeline.\n",
    "columns_to_assemble = [\n",
    "    \"cylinders\",\n",
    "    \"displacement\",\n",
    "    \"acceleration\",\n",
    "]\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=columns_to_assemble,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "\n",
    "# Define the model.\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"mpg\",\n",
    ")\n",
    "\n",
    "# Run the pipeline.\n",
    "pipeline = Pipeline(stages=[vector_assembler, rf])\n",
    "\n",
    "# Hyperparameter search.\n",
    "target_metric = \"rmse\"\n",
    "paramGrid = (\n",
    "    ParamGridBuilder().addGrid(rf.numTrees, list(range(20, 100, 10))).build()\n",
    ")\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(\n",
    "        labelCol=\"mpg\", predictionCol=\"prediction\", metricName=target_metric\n",
    "    ),\n",
    "    numFolds=2,\n",
    "    parallelism=4,\n",
    ")\n",
    "\n",
    "# Run cross-validation to get the best parameters.\n",
    "fit_cross_validator = cross_validator.fit(auto_df_fixed)\n",
    "best_pipeline_model = fit_cross_validator.bestModel\n",
    "best_regressor = best_pipeline_model.stages[1]\n",
    "print(\"Best model has {} trees.\".format(best_regressor.getNumTrees))\n",
    "\n",
    "# Save the Cross Validator, to capture everything including stats.\n",
    "fit_cross_validator.write().overwrite().save(\"rf_regression_optimized.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e992f37",
   "metadata": {},
   "source": [
    "**Encode string variables as numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c87117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Add manufacturer name we will use as a string column.\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df_fixed.withColumn(\n",
    "    \"manufacturer\", first_word_udf(auto_df_fixed.carname)\n",
    ")\n",
    "\n",
    "# Encode the manufactor name into numbers.\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"manufacturer\", outputCol=\"manufacturer_encoded\"\n",
    ")\n",
    "encoded = (\n",
    "    indexer.fit(df)\n",
    "    .transform(df)\n",
    "    .select([\"manufacturer\", \"manufacturer_encoded\"])\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fae768",
   "metadata": {},
   "source": [
    "**One-hot encode a categorical variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Turn the model year into categories.\n",
    "year_encoder = OneHotEncoder(\n",
    "    inputCol=\"modelyear\", outputCol=\"modelyear_encoded\"\n",
    ")\n",
    "encoded = (\n",
    "    year_encoder.fit(auto_df_fixed)\n",
    "    .transform(auto_df_fixed)\n",
    "    .select([\"modelyear\", \"modelyear_encoded\"])\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae016e",
   "metadata": {},
   "source": [
    "**Optimize a model after a data preparation pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52615f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "### Phase 1.\n",
    "# Add manufacturer name we will use as a string column.\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df_fixed.withColumn(\n",
    "    \"manufacturer\", first_word_udf(auto_df_fixed.carname)\n",
    ")\n",
    "\n",
    "# Encode the manufactor name into numbers.\n",
    "manufacturer_encoder = StringIndexer(\n",
    "    inputCol=\"manufacturer\", outputCol=\"manufacturer_encoded\"\n",
    ")\n",
    "# Turn the model year into categories.\n",
    "year_encoder = OneHotEncoder(\n",
    "    inputCol=\"modelyear\", outputCol=\"modelyear_encoded\"\n",
    ")\n",
    "\n",
    "# Run data preparation as a pipeline.\n",
    "data_prep_pipeline = Pipeline(stages=[manufacturer_encoder, year_encoder])\n",
    "prepared = data_prep_pipeline.fit(df).transform(df)\n",
    "\n",
    "### Phase 2.\n",
    "# Assemble vectors.\n",
    "columns_to_assemble = [\n",
    "    \"cylinders\",\n",
    "    \"displacement\",\n",
    "    \"horsepower\",\n",
    "    \"weight\",\n",
    "    \"acceleration\",\n",
    "    \"manufacturer_encoded\",\n",
    "    \"modelyear_encoded\",\n",
    "]\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=columns_to_assemble,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "\n",
    "# Define the model.\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"mpg\",\n",
    ")\n",
    "\n",
    "# Define the Pipeline.\n",
    "pipeline = Pipeline(stages=[vector_assembler, rf])\n",
    "\n",
    "# Run cross-validation to get the best parameters.\n",
    "paramGrid = (\n",
    "    ParamGridBuilder().addGrid(rf.numTrees, list(range(20, 100, 10))).build()\n",
    ")\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(\n",
    "        labelCol=\"mpg\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    "    ),\n",
    "    numFolds=2,\n",
    "    parallelism=4,\n",
    ")\n",
    "fit_cross_validator = cross_validator.fit(prepared)\n",
    "\n",
    "# Save the Cross Validator, to capture everything including stats.\n",
    "fit_cross_validator.write().overwrite().save(\"rf_regression_full.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0554fb2",
   "metadata": {},
   "source": [
    "**Evaluate Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c21629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Metrics supported by RegressionEvaluator.\n",
    "metrics = \"rmse mse r2 mae\".split()\n",
    "\n",
    "# Load the simple model and compute its predictions.\n",
    "simple_assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "simple_input = simple_assembler.transform(auto_df_fixed).select(\n",
    "    [\"features\", \"mpg\"]\n",
    ")\n",
    "rf_simple_model = RandomForestRegressionModel.load(\"rf_regression_simple.model\")\n",
    "simple_predictions = rf_simple_model.transform(simple_input)\n",
    "\n",
    "# Load the complex model and compute its predictions.\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df_fixed.withColumn(\n",
    "    \"manufacturer\", first_word_udf(auto_df_fixed.carname)\n",
    ")\n",
    "manufacturer_encoder = StringIndexer(\n",
    "    inputCol=\"manufacturer\", outputCol=\"manufacturer_encoded\"\n",
    ")\n",
    "year_encoder = OneHotEncoder(\n",
    "    inputCol=\"modelyear\", outputCol=\"modelyear_encoded\"\n",
    ")\n",
    "data_prep_pipeline = Pipeline(stages=[manufacturer_encoder, year_encoder])\n",
    "prepared = data_prep_pipeline.fit(df).transform(df)\n",
    "columns_to_assemble = [\n",
    "    \"cylinders\",\n",
    "    \"displacement\",\n",
    "    \"horsepower\",\n",
    "    \"weight\",\n",
    "    \"acceleration\",\n",
    "    \"manufacturer_encoded\",\n",
    "    \"modelyear_encoded\",\n",
    "]\n",
    "complex_assembler = VectorAssembler(\n",
    "    inputCols=columns_to_assemble,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "complex_input = complex_assembler.transform(prepared).select(\n",
    "    [\"features\", \"mpg\"]\n",
    ")\n",
    "cv_model = CrossValidatorModel.load(\"rf_regression_full.model\")\n",
    "best_pipeline = cv_model.bestModel\n",
    "rf_complex_model = best_pipeline.stages[-1]\n",
    "complex_predictions = rf_complex_model.transform(complex_input)\n",
    "\n",
    "# Evaluate performances.\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"mpg\")\n",
    "performances = [\n",
    "    [\"simple\", simple_predictions, dict()],\n",
    "    [\"complex\", complex_predictions, dict()],\n",
    "]\n",
    "for label, predictions, tracker in performances:\n",
    "    for metric in metrics:\n",
    "        tracker[metric] = evaluator.evaluate(\n",
    "            predictions, {evaluator.metricName: metric}\n",
    "        )\n",
    "print(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fd073",
   "metadata": {},
   "source": [
    "**Get feature importances of a trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63162ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "\n",
    "# Load the model we fit earlier.\n",
    "model = CrossValidatorModel.load(\"rf_regression_full.model\")\n",
    "\n",
    "# Get the best model.\n",
    "best_pipeline = model.bestModel\n",
    "\n",
    "# Get the names of assembled columns.\n",
    "assembler = best_pipeline.stages[0]\n",
    "original_columns = assembler.getInputCols()\n",
    "\n",
    "# Get feature importances.\n",
    "real_model = best_pipeline.stages[1]\n",
    "for feature, importance in zip(original_columns, real_model.featureImportances):\n",
    "    print(\"{} contributes {:0.3f}%\".format(feature, importance * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02b61b",
   "metadata": {},
   "source": [
    "**Plot Hyperparameter tuning metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdca5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Add manufacturer name we will use as a string column.\n",
    "first_word_udf = udf(lambda x: x.split()[0], StringType())\n",
    "df = auto_df_fixed.withColumn(\n",
    "    \"manufacturer\", first_word_udf(auto_df_fixed.carname)\n",
    ")\n",
    "manufacturer_encoded = StringIndexer(\n",
    "    inputCol=\"manufacturer\", outputCol=\"manufacturer_encoded\"\n",
    ")\n",
    "encoded_df = manufacturer_encoded.fit(df).transform(df)\n",
    "\n",
    "# Set up our main ML pipeline.\n",
    "columns_to_assemble = [\n",
    "    \"manufacturer_encoded\",\n",
    "    \"cylinders\",\n",
    "    \"displacement\",\n",
    "    \"horsepower\",\n",
    "    \"weight\",\n",
    "    \"acceleration\",\n",
    "]\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=columns_to_assemble,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "\n",
    "# Define the model.\n",
    "rf = RandomForestRegressor(\n",
    "    numTrees=20,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"mpg\",\n",
    ")\n",
    "\n",
    "# Run the pipeline.\n",
    "pipeline = Pipeline(stages=[vector_assembler, rf])\n",
    "\n",
    "# Hyperparameter search.\n",
    "target_metric = \"rmse\"\n",
    "paramGrid = (\n",
    "    ParamGridBuilder().addGrid(rf.numTrees, list(range(20, 100, 10))).build()\n",
    ")\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(\n",
    "        labelCol=\"mpg\", predictionCol=\"prediction\", metricName=target_metric\n",
    "    ),\n",
    "    numFolds=2,\n",
    "    parallelism=4,\n",
    ")\n",
    "\n",
    "# Run cross-validation, get metrics for each parameter.\n",
    "model = crossval.fit(encoded_df)\n",
    "\n",
    "# Plot results using matplotlib.\n",
    "import pandas\n",
    "import matplotlib\n",
    "\n",
    "parameter_grid = [\n",
    "    {k.name: v for k, v in p.items()} for p in model.getEstimatorParamMaps()\n",
    "]\n",
    "pdf = pandas.DataFrame(\n",
    "    model.avgMetrics,\n",
    "    index=[x[\"numTrees\"] for x in parameter_grid],\n",
    "    columns=[target_metric],\n",
    ")\n",
    "ax = pdf.plot(style=\"*-\")\n",
    "ax.figure.suptitle(\"Hyperparameter Search: RMSE by Number of Trees\")\n",
    "ax.figure.savefig(\"hyperparameters.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d58bc",
   "metadata": {},
   "source": [
    "**Compute correlation matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76520b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Remove non-numeric columns.\n",
    "df = auto_df_fixed.drop(\"carname\")\n",
    "\n",
    "# Assemble all columns except mpg into a vector.\n",
    "feature_columns = list(df.columns)\n",
    "feature_columns.remove(\"mpg\")\n",
    "vector_col = \"features\"\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=vector_col,\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "df_vector = vector_assembler.transform(df).select(vector_col)\n",
    "\n",
    "# Compute the correlation matrix.\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "corr_array = matrix.collect()[0][\"pearson({})\".format(vector_col)].toArray()\n",
    "\n",
    "# This part is just for pretty-printing.\n",
    "pdf = pandas.DataFrame(\n",
    "    corr_array, index=feature_columns, columns=feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3c97d",
   "metadata": {},
   "source": [
    "**Save a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "assembled = vectorAssembler.transform(auto_df_fixed)\n",
    "\n",
    "# Random test/train split.\n",
    "train_df, test_df = assembled.randomSplit([0.7, 0.3])\n",
    "\n",
    "# A regression model.\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    numTrees=50,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"mpg\",\n",
    ")\n",
    "\n",
    "# A classification model.\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    numTrees=50,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"origin\",\n",
    ")\n",
    "\n",
    "# Fit the models.\n",
    "rf_regressor_model = rf_regressor.fit(train_df)\n",
    "rf_regressor_model.write().overwrite().save(\"rf_regressor_saveload.model\")\n",
    "rf_classifier_model = rf_classifier.fit(train_df)\n",
    "rf_classifier_model.write().overwrite().save(\"rf_classifier_saveload.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f57589",
   "metadata": {},
   "source": [
    "**Load a model and use it for transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# Model type and assembled features need to agree with the trained model.\n",
    "rf_model = RandomForestRegressionModel.load(\"rf_regressor_saveload.model\")\n",
    "\n",
    "# The input DataFrame needs the same structure we used when we fit.\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"cylinders\",\n",
    "        \"displacement\",\n",
    "        \"horsepower\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "assembled = vectorAssembler.transform(auto_df_fixed)\n",
    "predictions = rf_model.transform(assembled).select(\n",
    "    \"carname\", \"mpg\", \"prediction\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce7edf",
   "metadata": {},
   "source": [
    "**Load a model and use it for predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c781a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# Model type and assembled features need to agree with the trained model.\n",
    "rf_model = RandomForestRegressionModel.load(\"rf_regressor_saveload.model\")\n",
    "\n",
    "input_vector = Vectors.dense([8, 307.0, 130.0])\n",
    "prediction = rf_model.predict(input_vector)\n",
    "print(\"Prediction is\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06169a54",
   "metadata": {},
   "source": [
    "**Load a classification model and use it to compute confidences for output labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "# Model type and assembled features need to agree with the trained model.\n",
    "rf_model = RandomForestClassificationModel.load(\"rf_classifier_saveload.model\")\n",
    "\n",
    "input_vector = Vectors.dense([8, 307.0, 130.0])\n",
    "prediction = rf_model.predictProbability(input_vector)\n",
    "print(\"Predictions are\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc96ae",
   "metadata": {},
   "source": [
    "## A few performance tips and tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0e462",
   "metadata": {},
   "source": [
    "**Get the Spark version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb25113",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcae5a",
   "metadata": {},
   "source": [
    "**Log messages using Spark's Log4J**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = spark.sparkContext._jvm.org.apache.log4j.Logger.getRootLogger()\n",
    "logger.warn(\"WARNING LEVEL LOG MESSAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe2ea1",
   "metadata": {},
   "source": [
    "**Cache a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5003915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Make some copies of the DataFrame.\n",
    "df1 = auto_df.where(lit(1) > lit(0))\n",
    "df2 = auto_df.where(lit(2) > lit(0))\n",
    "df3 = auto_df.where(lit(3) > lit(0))\n",
    "\n",
    "print(\"Show the default storage level (NONE).\")\n",
    "print(auto_df.storageLevel)\n",
    "\n",
    "print(\"\\nChange storage level to Memory/Disk via the cache shortcut.\")\n",
    "df1.cache()\n",
    "print(df1.storageLevel)\n",
    "\n",
    "print(\n",
    "    \"\\nChange storage level to the equivalent of cache using an explicit StorageLevel.\"\n",
    ")\n",
    "df2.persist(storageLevel=StorageLevel(True, True, False, True, 1))\n",
    "print(df2.storageLevel)\n",
    "\n",
    "print(\"\\nSet storage level to NONE using an explicit StorageLevel.\")\n",
    "df3.persist(storageLevel=StorageLevel(False, False, False, False, 1))\n",
    "print(df3.storageLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416a4ef",
   "metadata": {},
   "source": [
    "**Show the execution plan, with costs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = auto_df.groupBy(\"cylinders\").count()\n",
    "execution_plan = str(df.explain(mode=\"cost\"))\n",
    "print(execution_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265383f",
   "metadata": {},
   "source": [
    "**Partition by a Column Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52745c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows is an iterable, e.g. itertools.chain\n",
    "def number_in_partition(rows):\n",
    "    try:\n",
    "        first_row = next(rows)\n",
    "        partition_size = sum(1 for x in rows) + 1\n",
    "        partition_value = first_row.modelyear\n",
    "        print(f\"Partition {partition_value} has {partition_size} records\")\n",
    "    except StopIteration:\n",
    "        print(\"Empty partition\")\n",
    "\n",
    "df = auto_df.repartition(20, \"modelyear\")\n",
    "df.foreachPartition(number_in_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bc098",
   "metadata": {},
   "source": [
    "**Range Partition a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079578b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# rows is an iterable, e.g. itertools.chain\n",
    "def count_in_partition(rows):\n",
    "    my_years = set()\n",
    "    number_in_partition = 0\n",
    "    for row in rows:\n",
    "        my_years.add(row.modelyear)\n",
    "        number_in_partition += 1\n",
    "    seen_years = sorted(list(my_years))\n",
    "    if len(seen_years) > 0:\n",
    "        seen_values = \",\".join(seen_years)\n",
    "        print(\n",
    "            f\"This partition has {number_in_partition} records with years {seen_values}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Empty partition\")\n",
    "\n",
    "number_of_partitions = 5\n",
    "df = auto_df.repartitionByRange(number_of_partitions, col(\"modelyear\"))\n",
    "df.foreachPartition(count_in_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ea8de",
   "metadata": {},
   "source": [
    "**Change Number of DataFrame Partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = auto_df.repartition(col(\"modelyear\"))\n",
    "number_of_partitions = 5\n",
    "df = auto_df.repartition(number_of_partitions, col(\"mpg\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b4f4f",
   "metadata": {},
   "source": [
    "**Coalesce DataFrame partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c83c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "target_partitions = math.ceil(auto_df.rdd.getNumPartitions() / 2)\n",
    "df = auto_df.coalesce(target_partitions)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5ef5d",
   "metadata": {},
   "source": [
    "**Set the number of shuffle partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c211529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default shuffle partitions is usually 200.\n",
    "grouped1 = auto_df.groupBy(\"cylinders\").count()\n",
    "print(\"{} partition(s)\".format(grouped1.rdd.getNumPartitions()))\n",
    "\n",
    "# Set the shuffle partitions to 20.\n",
    "# This can reduce the number of files generated when saving DataFrames.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 20)\n",
    "\n",
    "grouped2 = auto_df.groupBy(\"cylinders\").count()\n",
    "print(\"{} partition(s)\".format(grouped2.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341eba4",
   "metadata": {},
   "source": [
    "**Sample a subset of a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc723821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/auto-mpg.csv\")\n",
    "    .sample(0.1)\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2793529",
   "metadata": {},
   "source": [
    "**Run multiple concurrent jobs in different pools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c811dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "other_df = auto_df.toDF(*(\"_\" + c for c in auto_df.columns))\n",
    "target_frames = [\n",
    "    auto_df.crossJoin(other_df.limit((11 - i) * 20))\n",
    "    .groupBy(column_name)\n",
    "    .count()\n",
    "    for i, column_name in enumerate(auto_df.columns)\n",
    "]\n",
    "\n",
    "def launch(i, target):\n",
    "    spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", f\"pool{i}\")\n",
    "    print(\"Job\", i, \"returns\", target.first(), \"at\", datetime.datetime.now())\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i, target in enumerate(target_frames):\n",
    "        print(\"Starting job\", i, \"at\", datetime.datetime.now())\n",
    "        futures.append(executor.submit(launch, i, target))\n",
    "        time.sleep(0.2)\n",
    "    concurrent.futures.wait(futures)\n",
    "    for future in futures:\n",
    "        print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9396e6",
   "metadata": {},
   "source": [
    "**Print Spark configuration properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72106d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee3ba5",
   "metadata": {},
   "source": [
    "**Set Spark configuration properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\"\n",
    "value = 2\n",
    "\n",
    "# Wrong! Settings cannot be changed this way.\n",
    "# spark.sparkContext.getConf().set(key, value)\n",
    "\n",
    "# Correct.\n",
    "spark.conf.set(key, value)\n",
    "\n",
    "# Alternatively: Set at build time.\n",
    "# Some settings can only be made at build time.\n",
    "spark_builder = SparkSession.builder.appName(\"My App\")\n",
    "spark_builder.config(key, value)\n",
    "spark = spark_builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340cc6b",
   "metadata": {},
   "source": [
    "**Publish Metrics to Graphite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da49ec2",
   "metadata": {},
   "source": [
    "**Increase Spark driver/executor heap space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory configuration depends entirely on your runtime.\n",
    "# In OCI Data Flow you control memory by selecting a larger or smaller VM.\n",
    "# No other configuration is needed.\n",
    "#\n",
    "# For other environments see the Spark \"Cluster Mode Overview\" to get started.\n",
    "# https://spark.apache.org/docs/latest/cluster-overview.html\n",
    "# And good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
