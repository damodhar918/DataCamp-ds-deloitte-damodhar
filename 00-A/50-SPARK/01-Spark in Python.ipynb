{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/cheat-sheet/pyspark-cheat-sheet-spark-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Spark \n",
    "SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext(master = 'local[2]')\n",
    "# sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Print SparkContext information\n",
    "# print(\"SparkContext version: \", sc.version)\n",
    "# print(\"Python version: \", sc.pythonVer)\n",
    "# print(\"Master URL: \", sc.master)\n",
    "# print(\"Path where Spark is installed: \", sc.sparkHome)\n",
    "# print(\"Spark User: \", sc.sparkUser())\n",
    "# print(\"Application name: \", sc.appName)\n",
    "# print(\"Application ID: \", sc.applicationId)\n",
    "# print(\"Default level of parallelism: \", sc.defaultParallelism)\n",
    "# print(\"Default minimum number of partitions for RDDs: \", sc.defaultMinPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My app\").set(\"spark.executor.memory\", \"1g\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Shell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ ./bin/spark-shell --master local[2]\n",
    "# $ ./bin/pyspark --master local[s] --py-files code.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spark-shell --master local[2]\n",
    "# !pyspark --master local[s] --py-files code.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "rdd4 = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]), (\"b\", [\"p\",\"r\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n",
      "defaultdict(<class 'int'>, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})\n",
      "{'a': 2, 'b': 2}\n",
      "4950\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# List the number of partitions\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "# Count RDD instances\n",
    "print(rdd.count())\n",
    "\n",
    "# Count RDD instances by key\n",
    "print(rdd.countByKey()) # Returns a defaultdict(<type 'int'>,{'a':2,'b':1})\n",
    "\n",
    "# Count RDD instances by value\n",
    "print(rdd.countByValue()) # Returns a defaultdict(<type 'int'>,{('b',2):1,('a',2):1,('a',7):1})\n",
    "\n",
    "# Return (key,value) pairs as a dictionary\n",
    "print(rdd.collectAsMap()) # Returns {'a': 2, 'b': 2}\n",
    "\n",
    "# Sum of RDD elements\n",
    "print(rdd3.sum()) # Returns 4950\n",
    "\n",
    "# Check whether RDD is empty\n",
    "print(sc.parallelize([]).isEmpty()) # Returns True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an RDD from a text file\n",
    "# file_path = \"/my/directory/file.txt\" # Replace \"â€¢\" with an actual file name\n",
    "# text_rdd = sc.textFile(file_path)\n",
    "\n",
    "# # Create an RDD that contains the contents of multiple text files\n",
    "# directory_path = \"/my/directory\"\n",
    "# text_rdd2 = sc.wholeTextFiles(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: 99\n",
      "Minimum value: 0\n",
      "Mean value: 49.5\n",
      "Standard deviation: 28.86607004772212\n",
      "Variance: 833.25\n",
      "Histogram: ([0, 33, 66, 99], [33, 33, 34])\n",
      "Summary statistics: (count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute statistics\n",
    "max_val = rdd3.max()\n",
    "min_val = rdd3.min()\n",
    "mean_val = rdd3.mean()\n",
    "stdev_val = rdd3.stdev()\n",
    "variance_val = rdd3.variance()\n",
    "histogram_val = rdd3.histogram(3)\n",
    "stats_val = rdd3.stats()\n",
    "\n",
    "# Print results\n",
    "print(\"Maximum value: {}\".format(max_val))\n",
    "print(\"Minimum value: {}\".format(min_val))\n",
    "print(\"Mean value: {}\".format(mean_val))\n",
    "print(\"Standard deviation: {}\".format(stdev_val))\n",
    "print(\"Variance: {}\".format(variance_val))\n",
    "print(\"Histogram: {}\".format(histogram_val))\n",
    "print(\"Summary statistics: {}\".format(stats_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]\n",
      "['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']\n",
      "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n"
     ]
    }
   ],
   "source": [
    "#Apply a function to each RFD element>>> \n",
    "print(rdd.map(lambda x: x+(x[1],x[0])).collect() )#[('a' ,7,7, 'a'),('a' ,2,2, 'a'), ('b' ,2,2, 'b')]\n",
    "#Apply a function to each RDD element and flatten the result>>> \n",
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0]))\n",
    "print(rdd5.collect()) #['a',7 , 7 ,  'a' , 'a' , 2,  2,  'a', 'b', 2 , 2, 'b']\n",
    "#Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys>>> \n",
    "print(rdd4.flatMapValues(lambda x: x).collect()) #[('a', 'x'), ('a', 'y'), ('a', 'z'),('b', 'p'),('b', 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting Data\n",
    "- Getting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2), ('b', 2)]\n",
      "[('a', 7), ('a', 2)]\n",
      "('a', 7)\n",
      "[('b', 2), ('a', 7)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collect all RDD elements and return them as a list\n",
    "print(rdd.collect())\n",
    "\n",
    "# Take the first 2 elements of the RDD\n",
    "print(rdd.take(2))\n",
    "\n",
    "# Take the first element of the RDD\n",
    "print(rdd.first())\n",
    "\n",
    "# Take the top 2 elements of the RDD based on their values\n",
    "print(rdd.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.sample(False, 0.15, 81).collect() #Return sampled subset of rdd3     [3,4,27,31,40,41,42,43,60,76,79,80,86,97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2)]\n",
      "['a', 7, 2, 'b']\n",
      "['a', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "print(rdd.filter(lambda x: \"a\" in x).collect()) #Filter the RDD[('a',7),('a',2)]>>> \n",
    "print(rdd5.distinct().collect()) #Return distinct RDD values['a' ,2, 'b',7]>>> \n",
    "print(rdd.keys().collect()) #Return (key,value) RDD's keys['a',  'a',  'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def g(x): print(x)\n",
    "\n",
    "rdd.foreach(g) # Apply a function to all RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(lambda x: g(x)) # apply g(x) using a lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 9), ('b', 2)]\n",
      "('a', 7, 'a', 2, 'b', 2)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Reducing\n",
    "print(rdd.reduceByKey(lambda x, y: x + y).collect()) # Merge the rdd values for each key [('a', 9), ('b', 2)]\n",
    "print(rdd.reduce(lambda a, b: a+ b)) #Merge the rdd values('a', 7, 'a' , 2 , 'b' , 2)\n",
    "print(rdd.values().sum())  # Sum all the rdd values: 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]), (1, [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', [7, 2]), ('b', [2])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping\n",
    "# Group RDD values by even and odd numbers, and return values as a list for each group\n",
    "print(rdd3.groupBy(lambda x: x % 2).mapValues(list).collect())\n",
    "\n",
    "# Group RDD values by key and return values as a list for each key\n",
    "rdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (9, 2)), ('b', (2, 1))]\n",
      "[('a', 9), ('b', 2)]\n",
      "[(0, 0), (2, 1), (4, 2), (6, 3), (8, 4), (10, 5), (12, 6), (14, 7), (16, 8), (18, 9), (20, 10), (22, 11), (24, 12), (26, 13), (28, 14), (30, 15), (32, 16), (34, 17), (36, 18), (38, 19), (40, 20), (42, 21), (44, 22), (46, 23), (48, 24), (50, 25), (52, 26), (54, 27), (56, 28), (58, 29), (60, 30), (62, 31), (64, 32), (66, 33), (68, 34), (70, 35), (72, 36), (74, 37), (76, 38), (78, 39), (80, 40), (82, 41), (84, 42), (86, 43), (88, 44), (90, 45), (92, 46), (94, 47), (96, 48), (98, 49), (100, 50), (102, 51), (104, 52), (106, 53), (108, 54), (110, 55), (112, 56), (114, 57), (116, 58), (118, 59), (120, 60), (122, 61), (124, 62), (126, 63), (128, 64), (130, 65), (132, 66), (134, 67), (136, 68), (138, 69), (140, 70), (142, 71), (144, 72), (146, 73), (148, 74), (150, 75), (152, 76), (154, 77), (156, 78), (158, 79), (160, 80), (162, 81), (164, 82), (166, 83), (168, 84), (170, 85), (172, 86), (174, 87), (176, 88), (178, 89), (180, 90), (182, 91), (184, 92), (186, 93), (188, 94), (190, 95), (192, 96), (194, 97), (196, 98), (198, 99)]\n"
     ]
    }
   ],
   "source": [
    "# Aggregating\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "# Define seqOp and combOp functions for aggregation\n",
    "seqOp = lambda x, y: (x[0] + y, x[1] + 1)\n",
    "combOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "# Aggregate RDD elements of each partition and then combine the results\n",
    "rdd3.aggregate((0, 0), seqOp, combOp)\n",
    "\n",
    "# Aggregate values of each RDD key\n",
    "result = rdd.aggregateByKey((0, 0), seqOp, combOp).collect()\n",
    "print(result)\n",
    "\n",
    "# Aggregate the elements of each partition, and then combine the results\n",
    "rdd3.fold(0, add)\n",
    "\n",
    "# Merge the values for each key\n",
    "result = rdd.foldByKey(0, add).collect()\n",
    "print(result)\n",
    "\n",
    "# Create tuples of RDD elements by applying a function\n",
    "result = rdd3.keyBy(lambda x: x + x).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('b', 2)]\n",
      "[('d', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('a', 7), ('a', 2)),\n",
       " (('a', 7), ('d', 1)),\n",
       " (('a', 7), ('b', 1)),\n",
       " (('a', 2), ('a', 2)),\n",
       " (('a', 2), ('d', 1)),\n",
       " (('a', 2), ('b', 1)),\n",
       " (('b', 2), ('a', 2)),\n",
       " (('b', 2), ('d', 1)),\n",
       " (('b', 2), ('b', 1))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return each rdd value not contained in rdd2\n",
    "print(rdd.subtract(rdd2).collect())  # [('b' ,2), ('a' ,7)]\n",
    "\n",
    "# Return each (key,value) pair of rdd2 with no matching key in rdd\n",
    "print(rdd2.subtractByKey(rdd).collect())  # [('d', 1)]\n",
    "\n",
    "# Return the Cartesian product of rdd and rdd2\n",
    "rdd.cartesian(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 1), ('b', 1), ('a', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1), ('d', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort \n",
    "# RDD by value ascendingly\n",
    "print(rdd2.sortBy(lambda x: x[1]).collect())\n",
    "\n",
    "#Sort (key, value) RDD by key ascendingly \n",
    "rdd2.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition\n",
    "rdd = rdd.repartition(4) # Creating new RDD with 4 partitions\n",
    "rdd = rdd.coalesce(1) # Decreasing the number of partitions to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RDD as a text file\n",
    "rdd.saveAsTextFile(\"tmp_rdd.txt\")\n",
    "\n",
    "# Save RDD as a Hadoop file\n",
    "output_format = 'org.apache.hadoop.mapred.TextOutputFormat'\n",
    "# file_path = 'hdfs://namenodehost/parent/child'\n",
    "file_path = './tmp_child'\n",
    "rdd.saveAsHadoopFile(file_path, output_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping SparkContext \n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution \n",
    "\n",
    "$ ./bin/spark-submit examples/src/main/python/pi.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
